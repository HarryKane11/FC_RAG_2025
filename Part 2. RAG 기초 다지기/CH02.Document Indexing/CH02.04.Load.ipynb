{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "## 📚 강의 개요 (Overview)\n",
    "\n",
    "이 강의에서는 텍스트 데이터를 임베딩하고, 벡터 데이터베이스에 로드(Load)하여 효율적으로 저장하고 검색하는 방법을 다룹니다. RAG (Retrieval-Augmented Generation) 시스템에서 어떤 임베딩 모델을 사용할지, 그리고 어떤 벡터 저장소를 활용할지가 검색 및 응답 성능에 중요한 영향을 미칩니다.\n",
    "\n",
    "이 강의를 통해 다양한 임베딩 모델을 활용하여 텍스트를 벡터로 변환하고, 이를 벡터 데이터베이스에 저장하여 빠르게 검색하는 방법을 배웁니다.\n",
    "\n",
    "## 목차: \n",
    "* [OpenAI 임베딩 모델 활용하기](#openai-임베딩-모델-활용하기)\n",
    "* [Ollama Embedding 모델 활용하기](#ollama-embedding-모델-활용하기)\n",
    "* [FAISS로 임베딩 벡터 저장하기](#faiss로-임베딩-벡터-저장하기)\n",
    "* [Chroma 벡터 DB](#chroma-벡터-db)\n",
    "* [Qdrant 벡터 DB](#qdrant-벡터-db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 설정하기 (.env 파일을 사용하지 않을 경우 여기에 입력해주세요!)\n",
    "import os\n",
    "\n",
    "# 환경변수 설정\n",
    "os.environ[\"API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 임베딩 모델 활용하기\n",
    "\n",
    "`text-embedding-3-small`: OpenAI에서 제공하는 최신 임베딩 모델 중 하나로, 빠르고 가벼운 임베딩을 제공합니다.\n",
    "\n",
    "문서 임베딩 vs 질의 임베딩\n",
    "* `embed_documents()` → 여러 개의 문장을 한 번에 임베딩 (문서 검색 등에 활용).\n",
    "* `embed_query()` → 질의(Query)를 임베딩 (질문-응답 시스템에서 검색할 때 활용)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서(문장) 리스트를 벡터로 변환 (임베딩)\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 임베딩된 문서 개수와 개별 임베딩 벡터의 차원 출력\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.010634176433086395,\n",
       " -0.01016946416348219,\n",
       " -0.0020040736999362707,\n",
       " 0.023065242916345596,\n",
       " -0.026829415932297707]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질의(Query) 문장을 임베딩 벡터로 변환\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5] #처음 5개의 값만 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])# 개별 문장의 임베딩 차원 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Embedding 모델 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 문서를 로드하여 페이지별로 저장하는 과정\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"data/arxiv_paper.pdf\"\n",
    ")\n",
    "\n",
    "# PDF 로더 객체 생성\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDF의 각 페이지를 저장할 리스트 초기화\n",
    "pages = []\n",
    "\n",
    "# PDF를 비동기 방식으로 로드하여 페이지별로 저장\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "ronment, we show that structured reasoning ap-\n",
      "proaches, such as Chain-of-Thoughts, improve\n",
      "confidence calibration. However, our findings\n",
      "also reveal persistent challenges in distinguishing\n",
      "uncertainty, particularly under abductive settings,\n",
      "underscoring the need for more sophisticated em-\n",
      "bodied confidence elicitation methods.\n",
      "1. Introduction\n",
      "In complex embodied environments, success depends not\n",
      "only on what an agent knows but also on how well it un-\n",
      "derstands and communicates uncertainty. Whether navi-\n",
      "gating a cluttered space, interacting with objects, or plan-\n",
      "ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter를 사용하여 텍스트 분할 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # 하나의 청크 크기를 1000자로 설정\n",
    "    chunk_overlap=200,     # 청크 간 200자 겹치게 설정 (문맥 유지 목적)\n",
    "    length_function=len,   # 텍스트 길이를 측정하는 함수 (len 사용)\n",
    "    is_separator_regex=False  # separator를 정규식이 아닌 단순 문자열로 처리\n",
    ")\n",
    "\n",
    "# PDF에서 로드한 데이터를 텍스트 청크로 분할\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\")# 첫 번째 청크의 메타데이터\n",
    "print(texts[0].page_content)# 첫 번째 청크의 내용 \n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\")# 두 번째 청크의 메타데이터\n",
    "print(texts[1].page_content)# 두 번째 청크의 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# \"bge-m3\" 모델을 사용하여 텍스트 임베딩 생성\n",
    "embeddings_model=OllamaEmbeddings(model=\"bge-m3\")\n",
    "# 청크된 문서 리스트를 벡터화하여 임베딩 생성\n",
    "embeddings = embeddings_model.embed_documents([i.page_content for i in texts])\n",
    "\n",
    "len(embeddings[0])# 생성된 임베딩 벡터의 차원 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS로 임베딩 벡터 저장하기\n",
    "\n",
    "FAISS(Facebook AI Similarity Search)\n",
    "*  대량의 벡터 데이터를 효율적으로 검색하는 라이브러리\n",
    "* 문서 검색, 추천 시스템, 이미지 검색 등에서 활용됨.\n",
    "* 벡터의 유사도를 계산하여 가장 가까운 문서를 빠르게 찾을 수 있음.\n",
    "\n",
    "FAISS 벡터 저장소 구축\n",
    "* `FAISS.from_documents()` → 텍스트 청크를 벡터로 변환하여 FAISS에 저장.\n",
    "* `embedding=embeddings_model` → Ollama의 `bge-m3` 모델을 사용하여 임베딩을 생성.\n",
    "\n",
    "FAISS 검색 방식\n",
    "* `similarity_search(query, k=1)` → 가장 유사한 k개의 문서를 검색.\n",
    "* `similarity_search(query, k=10, filter={\"page\": 0})` → 특정 조건(예: page=0)에서 유사한 문서 검색.\n",
    "* `similarity_search_with_score(query, k=10)` → 검색된 문서와 함께 유사도 점수(거리) 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ocling (c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ocling (c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ocling (c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "embedchain 0.1.125 requires pypdf<6.0.0,>=5.0.0, but you have pypdf 4.3.1 which is incompatible.\n",
      "embedchain 0.1.125 requires rich<14.0.0,>=13.7.0, but you have rich 13.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크의 수: 99\n",
      "벡터 저장소에 저장된 문서 수: 99\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#  FAISS 벡터 저장소 생성 (OllamaEmbeddings을 활용)\n",
    "# 앞서 생성한 청크(`texts`)와 임베딩 모델(`embeddings_model`)을 이용하여 벡터 저장소를 구축\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings_model # Ollama 임베딩 모델\n",
    ")\n",
    "\n",
    "# 벡터 저장소 크기 확인\n",
    "print(f\"청크의 수: {len(texts)}\") # 총 청크된 문서 개수\n",
    "print(f\"벡터 저장소에 저장된 문서 수: {vector_store.index.ntotal}\")# 총 청크된 문서 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n"
     ]
    }
   ],
   "source": [
    "# 1개의 유사한 문서를 검색 (기본 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agent가 뭐야?\",k=1)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n",
      "tion and decision-making processes (Ren et al., 2023; Liang\n",
      "et al., 2024). For humans, this instinctive ability to express\n",
      "and calibrate uncertainty is fundamental to decision-making\n",
      "and social interaction. As AI systems are increasingly de-\n",
      "ployed in high-stakes contexts such as autonomous driving\n",
      "or healthcare, they must also acquire this crucial skill.\n",
      "*Preprint. Work in progress.\n",
      "Embodied Environment\n",
      "Elicitation Module\n",
      "Are you sure about your\n",
      "next action ?\n",
      "Elicitation Module\n",
      "Are you sure about\n",
      "what you  see?Elicitation\n",
      "Policies\n",
      "Execution\n",
      "PoliciesElicitation\n",
      "Policies\n",
      "Execution\n",
      "Policies\n",
      "Perception StageAction StageFigure 1. Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies andExecution Policies , which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n"
     ]
    }
   ],
   "source": [
    "# 특정 페이지에 대한 필터링 검색 (page=0인 문서에서 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agent가 뭐야?\",k=10,filter={\"page\": 0})\n",
    "\n",
    "# 필터링된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\n",
      "\n",
      "* [유사도=0.848845] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir  [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.876677] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr [{'source': 'data/arxiv_paper.pdf', 'page': 8}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.892435] tend to yield improved confidence calibration. For instance,\n",
      "MineLLM’s ECE achieves 0.32 and 0.30 pa [{'source': 'data/arxiv_paper.pdf', 'page': 5}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.892554] Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes [{'source': 'data/arxiv_paper.pdf', 'page': 4}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.897213] and other types of language-based guidance. For a given\n",
      "taskT, the agent operates under a policy π:I [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.897255] Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., “I am 70%\n",
      " [{'source': 'data/arxiv_paper.pdf', 'page': 1}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.906592] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore [{'source': 'data/arxiv_paper.pdf', 'page': 10}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.930120] Confidence Elicitation in Embodied Agents\n",
      "language understanding. Trained on a vast dataset of 500,0 [{'source': 'data/arxiv_paper.pdf', 'page': 16}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.934772] ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact wit [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.938182] (§3.3) refine and expand confidence assessment through scenario reinterpretation, action sampling, a [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 유사도 점수를 포함한 검색 (Query와 문서 간 거리 계산)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied Agent가 뭐야?\",k=10)\n",
    "\n",
    "print(\"여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\\n\")\n",
    "\n",
    "# 검색된 문서와 점수 출력\n",
    "for doc, score in results:\n",
    "    print(f\"* [유사도={score:3f}] {doc.page_content[:100]} [{doc.metadata}]\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma 벡터 DB\n",
    "\n",
    "\n",
    "* ChromaDB는 벡터 데이터베이스로, 텍스트 검색과 추천 시스템 등에 활용됨.\n",
    "* FAISS는 메모리 내(in-memory)에서 작동하지만, Chroma는 영구 저장(Persistent Storage) 가능.\n",
    "* Chroma는 쿼리 시 더 다양한 필터링과 조합이 가능함.\n",
    "\n",
    "**Chroma 벡터 저장소 구축**\n",
    "* `Chroma(collection_name=\"test_01\", embedding_function=embeddings_model)`→ \"test_01\"이라는 컬렉션을 생성하고, Ollama 임베딩 모델을 사용하여 벡터 저장.\n",
    "* `add_documents(documents=texts, ids=ids)`→ 임베딩을 생성한 후 ChromaDB에 저장.\n",
    "\n",
    "**유사도 검색 방식**\n",
    "* `similarity_search(query, k=1)` → 가장 유사한 k개의 문서를 검색.\n",
    "* `similarity_search_with_score(query, k=5)` → 문서와 쿼리 간의 유사도 점수를 함께 반환.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -qU chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e36a3340-2299-435e-bbe0-980b9f425a41',\n",
       " '927df71a-4ec1-4fc1-ad35-591db7f6b35e',\n",
       " 'f7762f29-5604-4668-b6b8-1d2cbd262922',\n",
       " '1a0a1b08-0bc2-4a5d-be57-75e960e59062',\n",
       " '64c232a2-3f9e-4f33-91c6-5df21c6b82c6',\n",
       " '0479c143-2250-4a96-ae1f-05ae58d47566',\n",
       " 'b838def6-ea76-4e22-a5ab-e53b82d778db',\n",
       " '02de8bab-82b0-476c-adcd-afba5eabd2b2',\n",
       " '64ded0df-cd27-4e41-9b89-615ab300fc41',\n",
       " '57f86230-afd3-43e1-ac63-7b84db5ad8ce',\n",
       " '468a59c9-aae2-492e-a1e8-727cb31d2d9e',\n",
       " '47aa8a17-3448-4f3b-abfe-7cb52a7dd9f7',\n",
       " 'faba007d-fafd-4706-b66f-aae4b4ebcb66',\n",
       " '1034752d-086d-4301-809b-948df36c3421',\n",
       " 'ee3a30d2-0c50-4404-b8b4-f0b58899de1f',\n",
       " 'ce8162d5-2885-412d-ac62-1d2604d4f382',\n",
       " '621177aa-c822-4243-a2cf-f6e52f7216e8',\n",
       " '5f1e09fe-aa11-4487-9e77-6aacbbdcdf66',\n",
       " 'cf823ad0-ee7d-42f5-85db-4e668a308d7e',\n",
       " 'c9b8da22-1f75-4f66-b400-4e2afe2af8fd',\n",
       " 'e71d1737-1dc2-47ac-8815-88cb88148ba7',\n",
       " '62ee0eca-8619-4d1f-884c-4b8624039a90',\n",
       " 'e0993fa8-eda7-47ee-8a03-c477848c51a1',\n",
       " 'df6774bf-85ed-467f-a684-af67f2cc8ce2',\n",
       " '818c8370-869c-4232-a5c3-1e3b4ca42e77',\n",
       " '9786afa2-fad9-4b00-9260-8137ea2a8601',\n",
       " '020302a0-3f53-4db5-a735-fdff6b37e0d9',\n",
       " '2693a543-01e3-40f5-a490-f97cf7da79ae',\n",
       " '4fe3a9ec-6b64-4739-8cb8-90055335f9c8',\n",
       " 'b4e8d249-6f92-4fd3-bc2b-7e9f4ddc2c93',\n",
       " '753cc5d7-8db1-44b0-954d-e7f3622b95b6',\n",
       " 'b6084850-8da0-4d82-b7e7-36b83fdfd87e',\n",
       " 'e636516f-4074-4537-b19a-add255cbad57',\n",
       " '74d85891-3847-4657-a1ca-050e7df72157',\n",
       " 'f6c3afa2-79b7-4dd3-8d25-8a0e3d62c5fd',\n",
       " 'd8bd663e-6d58-4109-9cf4-b2764b46c4d3',\n",
       " '31ddfb40-2253-4829-bc28-f997474d7afe',\n",
       " '1cc3d3b2-8345-425a-8af4-b370c047657f',\n",
       " 'd0199790-ff97-474f-847c-df6f3c5a7cd8',\n",
       " '69c0317f-3046-4511-b29d-8c5de9844671',\n",
       " '97facb1b-adc3-4561-8a99-d42fca665800',\n",
       " '64af178c-de08-45a5-b085-20c7b2eac667',\n",
       " '88ca3244-881f-4900-a25f-960a02cfb504',\n",
       " 'bdc62767-5e8b-4b89-b423-dc0ff343d627',\n",
       " '022327b2-956e-45fc-962e-f46cc2cc2a77',\n",
       " '0792c32d-0387-43a5-9d93-74e2a51e9966',\n",
       " '8a6c3fb1-de99-4930-8416-acbba536f640',\n",
       " 'a4f44d2f-ed72-42e5-a78b-43000e602a0a',\n",
       " '715c4537-9759-4384-8831-3d640c6b8bb7',\n",
       " '3e649ccc-5a72-4bb9-a899-1002fc631779',\n",
       " '081ca574-723a-463f-98bc-8b2064fd48c7',\n",
       " '0cb052bb-a4fe-47e3-93f6-b0f7447e47c0',\n",
       " '424af106-5559-4bc9-8727-9e122e440cdb',\n",
       " '727b455b-b407-4f43-b006-7e22707d1b51',\n",
       " 'bec09eac-bfc1-422d-bbb6-fcc86df4b357',\n",
       " '62a19535-735f-427e-9d01-ad61f5786d72',\n",
       " '9e9d739d-ccd4-460c-906e-604c6b1d2e73',\n",
       " 'b9bab06d-4045-4ae1-99f4-59c043b51578',\n",
       " 'b860d153-7071-40c6-9b35-cb39923a2dd2',\n",
       " 'e097c521-7d50-49be-85b5-c853b59401b3',\n",
       " 'a2522779-3800-4637-942f-d2859ccf7e93',\n",
       " '9162f66f-4be7-4ebe-b30c-4ae0545c0e79',\n",
       " 'd7c14ac6-514c-456c-81ee-76498a288b4b',\n",
       " 'd2c3c35b-aefa-4d81-b54e-761f61242209',\n",
       " 'a81e49bd-876f-409b-974c-11f516128e8a',\n",
       " '7407865f-21a7-4b45-8b0a-c77a5357c92a',\n",
       " '78343d1c-c115-4204-be6d-c0ee78b9b162',\n",
       " '4997813b-bc23-4c47-9a3f-bc595ecc0044',\n",
       " '8b31cbbf-6026-4ad5-9c78-79378c5b0f60',\n",
       " '5a9747dc-ce13-4ed3-aabe-9ca99c77982d',\n",
       " 'b97cf381-5bcf-4c36-815a-eb2b78d841d2',\n",
       " '7fc15e79-3ed8-480a-8a92-678235152abe',\n",
       " '64208e8c-d8a7-417e-9dea-3e129153db02',\n",
       " '4e6ce1fe-42b5-4617-be7b-edbd367ff24d',\n",
       " '06b004df-7e4c-4ff2-b554-375acdd5dec6',\n",
       " '3c03a694-4312-41f7-8e4f-a93b75b2afd5',\n",
       " '8425dbdd-391d-44e9-a06e-021a83855f22',\n",
       " '4196964c-a23b-4af9-aa65-7682c0e5f669',\n",
       " 'ef747e8d-d667-4f68-94d0-a2f309e45388',\n",
       " '2b6b4b63-5099-47f3-a018-2a909f379610',\n",
       " '7bead8c8-d2d7-498e-ab1c-9d3f84e5bf23',\n",
       " '051a3c4e-9306-401c-bf03-afdb21fc37c5',\n",
       " '5f51e253-7468-4ffe-b4ab-869588341ee7',\n",
       " '0e30a0e6-39f7-4a7e-a3af-9829413f51df',\n",
       " '21ba1464-f8ef-4072-aee6-7096b61f7c59',\n",
       " '1d90cbb7-2030-4b30-bd34-0b6d2baed8b4',\n",
       " 'e49b227c-a3cd-4fcc-a920-cdce1a01f415',\n",
       " 'ad499c0d-aaf3-4527-847c-7c433f408c6d',\n",
       " '49274f09-26dc-4702-84bb-f2916728e46d',\n",
       " 'c01744d2-ac6b-4c27-b09e-bb5b959725ba',\n",
       " '46fc571a-7e46-4aec-8188-7ff5ef16b26f',\n",
       " 'c98e2b06-0efa-4a47-988a-165efd15b06c',\n",
       " '13afd048-d8f2-4fb0-a278-d665ddae7055',\n",
       " '5659eb91-14b9-417d-9051-441f58ca7e83',\n",
       " 'a9983cd7-6e03-4add-ae61-a9bafc74c536',\n",
       " '21d40e2c-a977-46ed-bd56-c768c9e0a5c6',\n",
       " 'f8012018-3d12-4753-92f4-c8c06b7ac409',\n",
       " 'da4f2b28-a1d0-45ce-9af0-0c2e1fcda0b1',\n",
       " '9e716822-5aae-4aa2-86be-b54c2504c1fb',\n",
       " 'a48d62fe-eb76-4a72-addf-e442f7d0ec06']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from uuid import uuid4\n",
    "\n",
    "#  Chroma 벡터 저장소 생성\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_01\",  # 데이터가 저장될 컬렉션 이름\n",
    "    embedding_function=embeddings_model,  # Ollama 임베딩 모델 사용\n",
    ")\n",
    "\n",
    "#  UUID를 활용하여 문서별 고유 ID 생성\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# 벡터 저장소에 문서 추가 (임베딩 자동 생성)\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# 1개의 유사한 문서를 검색 (기본 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agent가 뭐야?\", k=1)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\n",
      "\n",
      "* [유사도=0.830] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir  [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.855] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore [{'page': 10, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.857] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr [{'page': 8, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.859] tend to yield improved confidence calibration. For instance,\n",
      "MineLLM’s ECE achieves 0.32 and 0.30 pa [{'page': 5, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.859] sents visual observations and It represents task instructions\n",
      "and other types of language-based guid [{'page': 2, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 유사도 점수를 포함한 검색 (Query와 문서 간 거리 계산)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agent가 뭐야?\", k=5)\n",
    "\n",
    "print(\"여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\\n\")\n",
    "\n",
    "# 검색된 문서와 점수 출력\n",
    "for doc, score in results:\n",
    "    print(f\"* [유사도={score:.3f}] {doc.page_content[:100]} [{doc.metadata}]\")  # 상위 100자만 출력\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant 벡터 DB\n",
    "\n",
    "* Qdrant는 벡터 검색(Vector Search) 및 필터링을 지원하는 벡터 데이터베이스.\n",
    "* 메모리 내(in-memory) 실행 가능하며, 영구 저장(persistent storage)도 지원.\n",
    "* FAISS나 Chroma와 다르게, 메타데이터 기반 필터링 기능이 강력함.\n",
    "\n",
    "**Qdrant 벡터 저장소 구축**\n",
    "* `QdrantClient(\":memory:\")` → 인메모리 벡터 저장소 생성.\n",
    "* `create_collection()` → 1024차원의 벡터를 저장할 수 있는 컬렉션 생성.\n",
    "* `distance=Distance.COSINE` → 문서 간 코사인 유사도를 계산하여 가장 유사한 문서를 찾음.\n",
    "  \n",
    "**Qdrant 검색 기능**\n",
    "* `similarity_search(query, k=1)` → 가장 유사한 k개의 문서를 검색.\n",
    "* `similarity_search_with_score(query, k=5)` → 문서와 쿼리 간의 유사도 점수를 함께 반환.\n",
    "* `similarity_search(query, k=1, filter=...)` → 특정 조건을 만족하는 문서만 검색."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -qU langchain_qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain에서 Qdrant 벡터 저장소 불러오기\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Qdrant 클라이언트 라이브러리 불러오기\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Qdrant 클라이언트 생성 (메모리 기반)\n",
    "# \":memory:\" 옵션을 사용하면 휘발성(In-Memory) 데이터베이스로 실행됨.\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "#  Qdrant 컬렉션 생성\n",
    "client.create_collection(\n",
    "    collection_name=\"test\",  # 저장소 이름\n",
    "    vectors_config=VectorParams(\n",
    "        size=1024,  # 벡터 차원 수 (사용하는 임베딩 모델에 맞춰야 함)\n",
    "        distance=Distance.COSINE  # 벡터 간 유사도 측정 방식 (코사인 거리 사용)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Qdrant 벡터 저장소 객체 생성\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,  # Qdrant 클라이언트\n",
    "    collection_name=\"test\",  # 컬렉션 이름\n",
    "    embedding=embeddings_model  # Ollama 임베딩 모델 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b840b5af-51c4-4073-b436-ed7e049b0bef',\n",
       " 'e11116e7-534f-4b44-8895-d3cac16c0cc3',\n",
       " 'bb0c0ad4-aa56-4859-a340-b1ba64601af4',\n",
       " '8df32e68-9a66-41c6-9083-ab60fa855898',\n",
       " '8605f401-379a-4099-bfb1-332b497b9a5a',\n",
       " 'f13a3fd5-e5e4-468f-bab1-f4be3bb35cc9',\n",
       " '83088019-cc7c-492b-81c0-60bf2d27180f',\n",
       " 'e41d6a63-a2b6-4e0a-9827-5a9d0f13f1a8',\n",
       " '83710cbf-5a27-42d3-9d5f-aed608eac089',\n",
       " '1be9b921-aeb4-41dc-a59a-e551171b756e',\n",
       " '207e508e-a8b8-495e-bbf4-45aaeac333c3',\n",
       " '018eef8c-7555-4c32-b8ba-32c06d1adcd4',\n",
       " '1f7f02d1-93ba-433f-a8d4-ea7f1df22c52',\n",
       " 'ec5cda78-489f-4939-b945-80495f9e94fc',\n",
       " '6cd32a15-6c56-4e43-b892-df19da068eb0',\n",
       " '8a07c46d-c59f-4358-a1d7-50411fa5c927',\n",
       " 'af88d6e9-da5e-4580-a647-eb2c37ca3cd7',\n",
       " 'c48a8721-12dc-4051-8d0a-f00426c592e5',\n",
       " '68cbc027-852c-4f49-859e-233261e103e4',\n",
       " '644b20fe-1e86-4738-86f6-1696de289a82',\n",
       " '0a9b490f-b184-45a5-8733-297f1e1718c9',\n",
       " 'de7da756-6634-4512-af78-6ec08e9b7b00',\n",
       " 'e15fa4c4-d95d-4fea-ab5a-c022786482b7',\n",
       " 'd95c8f44-a5cd-456a-b408-f8ed3fa9e961',\n",
       " '0434c7bf-fe92-492d-9194-4efbb5cf0d17',\n",
       " '4f480437-58b7-45c9-b895-7098f69a42fd',\n",
       " '9d9897f8-4bac-4881-b41a-60e9e0634f58',\n",
       " '8de7bc5d-7cc7-42e3-b41f-21d2a7cc9ea4',\n",
       " '45ea2c5d-b1d3-485d-82eb-c0f83069bd13',\n",
       " 'a30ff1ac-c925-4571-86af-969e070acb34',\n",
       " '89399203-8556-47f8-9a1a-99bf1a2ca5b4',\n",
       " '49a44f47-db39-4392-b09b-8efae05e9b31',\n",
       " 'a99ba258-88bd-4c3e-91f8-658b9f85c53c',\n",
       " '95454e12-9831-47cf-ac1d-26b295f231b6',\n",
       " 'fc4d2217-222f-4c8d-bb5a-ce4c0f5a1c66',\n",
       " '29a0c4d8-a194-461a-ad99-b04b48ee87a3',\n",
       " '08f65390-f695-4824-b186-86654c203b5b',\n",
       " '8afbc200-034c-434d-8cc5-be2802991b27',\n",
       " '47fb0a27-d039-43f7-9458-e5715b3d9245',\n",
       " '3ac2b721-90ce-4940-8973-e1a2b853e0e1',\n",
       " '12fd32a2-f395-452c-b583-dfd694ca45f3',\n",
       " 'c2d1ac81-7331-4d8f-b0f2-f778db5f8082',\n",
       " 'f4cead39-2c68-445b-ba8a-bf4ad848ba27',\n",
       " 'a3000421-77ee-41dc-ae67-84a806c014e1',\n",
       " '63bb6213-f3bc-484d-aa63-b01e914b1edc',\n",
       " '86fe75e9-2b00-4bdb-bcca-6ddad140f85e',\n",
       " '99664e11-4c12-4d7f-8d28-be62a9c3e82d',\n",
       " 'c978bb34-3c0f-4216-9c29-be9f17551cc2',\n",
       " '39cda4b3-bcbd-4a2a-8b66-088e8db1c887',\n",
       " '889a83d3-3794-4a67-8352-7799555c07f0',\n",
       " '590477e6-6ae7-4f20-8612-652545283a1d',\n",
       " '7b52856d-4468-40f5-8438-96218c46e18c',\n",
       " 'a4f5fb23-72ea-46c1-82fe-a879969e1992',\n",
       " '92137943-00e7-4a5b-85fd-1e27d9bbe6a8',\n",
       " '9081d4b0-35b5-472e-9db3-2e9478d09c19',\n",
       " '44ebf576-c708-4eb3-9fea-b3a1527320d0',\n",
       " '4e48e514-164e-4ada-8024-34c5f58de71f',\n",
       " 'df12dc67-1235-4efa-867d-12b9e52554c8',\n",
       " '9c4d3483-9d3b-4461-a483-bae618bb93ee',\n",
       " '343e1e81-ee1b-438a-b8e4-cc01d22b4c28',\n",
       " '1eca483c-38f0-4c45-9510-6f6a18ef838d',\n",
       " 'e07223a8-5cb9-4492-884a-83f3704d01c0',\n",
       " 'd20a8c4b-b43b-46dc-a70a-d2e49ac31af0',\n",
       " '9c0eb7d0-44ee-4fc0-a092-c62c61026d69',\n",
       " '282af929-0229-41f3-8f86-9d197af41a61',\n",
       " '9bf2b3d5-a61f-4c2f-9160-e2103aade803',\n",
       " 'c06f2738-09b6-4b0f-a102-613206e81916',\n",
       " '9b0bebb0-4eb9-4d8f-ae4e-33c4c540e83b',\n",
       " '5189f3a5-7ad6-4141-accd-5c8358122458',\n",
       " '90a8fa41-31a3-454f-961a-467315692d10',\n",
       " '85c2bd34-4741-4ab9-9bf5-a17c4fd57af4',\n",
       " 'd51abdf4-0344-4370-a7df-3898e14dd146',\n",
       " '13ba530b-12ce-4e20-986b-b0efb80c65b8',\n",
       " '5e41bd69-5daa-4c01-8224-eef4731d7c5f',\n",
       " '521e8ec9-6bd0-446f-bebb-8e3f05dc33c3',\n",
       " '89ddc64a-c0cf-4260-adbe-6359ce37a9c8',\n",
       " 'a728aa1a-8e3a-4b2b-b14e-da027c5d1a23',\n",
       " '7bf78941-be61-4a52-bc7b-4b12ef7e9222',\n",
       " '54511f8f-3eba-47f9-90a7-41eef0c6d249',\n",
       " 'cf914374-1a37-4fc4-9f14-e581789fb86c',\n",
       " 'acb9e0e5-3a48-468a-ba2c-85f1e0d355a8',\n",
       " '91df61ec-aaba-4a7d-9f05-e6998f2d0560',\n",
       " '2d0e4f92-b71d-47f6-ae62-4b711d2f339b',\n",
       " 'c7addf68-fbcd-4359-bbef-fd888456fc10',\n",
       " '8c7cf3bb-82e8-40c7-b612-4d34fd471e33',\n",
       " 'f5c7bdaa-52d2-42f2-9180-563458c283aa',\n",
       " 'f9ea3500-6d39-450f-935d-db5347ac1578',\n",
       " 'fecd04be-59d5-45f9-851d-cce597e260df',\n",
       " '723e906f-c4b4-419e-8e23-150d9fb355ca',\n",
       " '04e9ca63-3aff-475a-a27f-821fb95874c9',\n",
       " 'd8028934-9c68-43b9-b680-3ed900b3e269',\n",
       " '3fe28d7c-31e4-42e6-b48a-586117ed42c8',\n",
       " '3a5c65ff-d9d1-4a0a-a305-c6bee3f9028e',\n",
       " 'fbc10849-9f3a-4453-b5de-d4c99a93f3d5',\n",
       " 'd99ac434-6c9b-472e-8976-9e76a16b50ae',\n",
       " 'e30a734b-a0c8-4ce1-b025-a1c10b363311',\n",
       " 'a94fff2e-1bd8-4a9c-9185-aefa6956e439',\n",
       " 'cc724d3f-6622-411b-9cb0-a85eb856b059',\n",
       " '6a8ee6cf-2d9a-4c9f-9c3f-bdab683d63b4',\n",
       " '3eae0af6-6b38-4982-a5ab-e4959b6fab62']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 ID를 생성하기 위해 UUID 사용\n",
    "from uuid import uuid4\n",
    "\n",
    "# 각 문서에 대해 고유한 ID 생성\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# 벡터 저장소에 문서 추가\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': '67fa1f86-e931-43fb-866d-9f273e685a3f', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "# 1개의 유사한 문서를 검색 (기본 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agent가 뭐야?\", k=1)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* which are increasingly prevalent in real-world applications\n",
      "(Achiam et al., 2023; Touvron et al., 2023a). Additionally,\n",
      "their free-form nature of outputs further complicates the\n",
      "application of traditional methods. As a result, alternative\n",
      "approaches have been proposed, including estimating un-\n",
      "certainty by directly querying models for confidence scores\n",
      "after generating responses (Xiong et al., 2024; Kadavath\n",
      "et al., 2022; Lin et al., 2022a; Mielke et al., 2022; Chen &\n",
      "Mueller, 2024). Despite these advancements, existing meth-\n",
      "ods are not designed for embodied tasks, where confidence\n",
      "elicitation must address the challenges of multimodal per-\n",
      "ception, hierarchical reasoning and planning across various\n",
      "open-ended tasks, as well as non-deterministic interactions.\n",
      "LLM-based Embodied Agents. With the advent of lan-\n",
      "guage models, leveraging their reasoning and planning abili-\n",
      "ties to empower embodied agents has become quintessential\n",
      "(Huang et al., 2023; Yao et al., 2023; Chen et al., 2023; [{'source': 'data/arxiv_paper.pdf', 'page': 1, '_id': '207e508e-a8b8-495e-bbf4-45aaeac333c3', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "#  특정 필터링 조건을 적용한 검색\n",
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"thud\",\n",
    "    k=1,\n",
    "    filter=models.Filter(must=[models.FieldCondition(\n",
    "        key=\"metadata.page\",  # 특정 필드(예: page) 기반 필터링\n",
    "        match=models.MatchValue(value=1),  # page=1인 문서만 검색\n",
    "    )])\n",
    ")\n",
    "\n",
    "# 필터링된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\n",
      "\n",
      "* [유사도=0.585] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': 'b840b5af-51c4-4073-b436-ed7e049b0bef', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.573] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agents’ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using bayesian binning. In\n",
      "AAAI Conference on Artificial Intelligence, 2015.\n",
      "Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y ., Ha-\n",
      "jishirzi, H., Singh, S., and Fox, R. Do embodied agents\n",
      "dream of pixelated sheep: Embodied decision making\n",
      "using language guided world modelling. In International\n",
      "Conference on Machine Learning, 2023.\n",
      "Okoli, C. Inductive, abductive and deductive theorising.\n",
      "International Journal of Management Concepts and Phi-\n",
      "losophy, 2023. [{'source': 'data/arxiv_paper.pdf', 'page': 10, '_id': '9bf2b3d5-a61f-4c2f-9160-e2103aade803', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.571] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment is critical. However, the\n",
      "reliance on large pre-trained models raises concerns about\n",
      "energy efficiency and ethical considerations in high-stakes\n",
      "applications, which warrant further exploration.\n",
      "References\n",
      "Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,\n",
      "Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-\n",
      "ravi, A., Acharya, U. R., et al. A review of uncertainty\n",
      "quantification in deep learning: Techniques, applications\n",
      "and challenges. Information Fusion, 2021. [{'source': 'data/arxiv_paper.pdf', 'page': 8, '_id': '92137943-00e7-4a5b-85fd-1e27d9bbe6a8', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.571] tend to yield improved confidence calibration. For instance,\n",
      "MineLLM’s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVE’s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence judgments.\n",
      "So, How Effectively Can Embodied Agents Express Con-\n",
      "fidence in Dynamic Embodied Tasks? While embodied\n",
      "agents can convey confidence to some extent, their effec-\n",
      "tiveness depends on how well they integrate reasoning, un-\n",
      "certainty assessment, and environmental interactions. The\n",
      "findings reveal that embodied confidence elicitation remains\n",
      "a challenging problem, requiring a careful balance between\n",
      "general-purpose reasoning and task-specific specialization. [{'source': 'data/arxiv_paper.pdf', 'page': 5, '_id': '47fb0a27-d039-43f7-9458-e5715b3d9245', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.570] sents visual observations and It represents task instructions\n",
      "and other types of language-based guidance. For a given\n",
      "task T , the agent operates under a policy π : I → Athat\n",
      "maps input I to actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score c ∈ [0, 1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.\n",
      "The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks. This requires not only\n",
      "detecting uncertain aspects of the agent’s perception, reason-\n",
      "ing, or actions but also ensuring that confidence estimates\n",
      "are refined and reliable under dynamic multimodal condi-\n",
      "tions. To address this, we propose an embodied confidence\n",
      "estimation framework centered around Elicitation Modules\n",
      "that facilitates confidence elicitation at two critical points [{'source': 'data/arxiv_paper.pdf', 'page': 2, '_id': 'af88d6e9-da5e-4580-a647-eb2c37ca3cd7', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 유사도 점수를 포함한 검색 (Query와 문서 간 거리 계산)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agent가 뭐야?\", k=5)\n",
    "\n",
    "print(\"여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\\n\")\n",
    "\n",
    "# 검색된 문서와 점수 출력\n",
    "for doc, score in results:\n",
    "    print(f\"* [유사도={score:.3f}] {doc.page_content[:1000]} [{doc.metadata}]\")  # 첫 1000자 출력\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
