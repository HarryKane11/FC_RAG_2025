{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama pull gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬모델 실행 위한 GPU 사용 환경 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama로 Gemma3 4B 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='gemma3:4b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': '안녕하세요, 당신은 누구인가요?',\n",
    "  },\n",
    "],\n",
    "    options={\n",
    "        'temperature': 0.7,        # 창의성 조절 (0.0 ~ 1.0)\n",
    "        'top_p': 0.9,             # 샘플링 확률 임계값\n",
    "        'top_k': 40,              # 다음 토큰 선택시 고려할 상위 토큰 수\n",
    "        'num_predict': 100,       # 생성할 최대 토큰 수\n",
    "        'repeat_penalty': 1.1,    # 반복 패널티\n",
    "        'presence_penalty': 0,    # 새로운 주제 등장 확률 조절\n",
    "        'frequency_penalty': 0,   # 단어 반복 감소 조절\n",
    "        'stop': ['\\n', 'User:']   # 생성 중단 토큰 설정\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='gemma3:4b',\n",
    "    messages=[{'role': 'user', 'content': '안녕하세요, 당신은 누구인가요?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 사용 예시\n",
    "import ollama\n",
    "import base64\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "# 이미지 변환 함수들\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def pil_to_base64(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "def display_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return img\n",
    "\n",
    "# 기본 이미지 추론\n",
    "def analyze_image(image_path, prompt=\"이 이미지에 대해 설명해주세요.\"):\n",
    "    \"\"\"\n",
    "    이미지를 분석하고 결과를 반환하는 함수\n",
    "    \"\"\"\n",
    "    # 이미지 표시\n",
    "    display_image(image_path)\n",
    "    \n",
    "    model_name = \"gemma3:4b\"\n",
    "    \n",
    "    # 모델에 이미지와 함께 쿼리 보내기\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "                \"images\": [image_to_base64(image_path)]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 응답 출력\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "# 1. 단일 이미지 분석\n",
    "result = analyze_image(\"./data/docling_processing.png\", \"이 이미지에서 무엇이 보이나요?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface로 Gemma3 4B 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"./data/docling_processing.png\"},\n",
    "            {\"type\": \"text\", \"text\": \"이 이미지를 설명해주세요\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=200)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Together AI API로 Llama 4 추론하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together AI API Playground URL\n",
    "\n",
    "https://api.together.ai/playground/v2/chat/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"안녕하세요, 당신은 누구인가요?\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "import base64\n",
    "\n",
    "imagePath= \"./data/docling_processing.png\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "base64_image = encode_image(imagePath)\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"이 이미지를 한글로 설명해주세요\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\" if chunk.choices else \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq AI로 Llama 4 추론하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groq Cloud API Playground\n",
    "\n",
    "https://console.groq.com/playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"첫번째로 개발된 LLM은 무엇인가요?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath= \"./data/docling_processing.png\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "base64_image = encode_image(imagePath)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What's in this image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cerebras로 Llama 4 추론하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerebras Cloud API Playground\n",
    "\n",
    "https://cloud.cerebras.ai/platform/org_8ryjmjdft9v69jyh4fdn5hn9/playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install cerebras_cloud_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "client = Cerebras()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"첫번째로 개발된 LLM은 무엇인가요?\",\n",
    "        }\n",
    "    ],\n",
    "  model=\"llama-4-scout-17b-16e-instruct\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain으로 여러 오픈소스 플랫폼의 LLM 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-3-1b-it\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10, \"torch_dtype\": torch.bfloat16},\n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"안녕하세요, 당신은 누구인가요?\"),\n",
    "]\n",
    "\n",
    "print(hf.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0,\n",
    "    repeat_penalty=1.1,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant\",\n",
    "    ),\n",
    "    (\"human\", \"안녕하세요, 당신은 누구인가요?\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 이미지 데이터를 base64로 인코딩하는 함수\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# 이미지와 텍스트가 함께 있는 메시지 예시\n",
    "image_path = \"./data/docling_processing.png\"  # 실제 이미지 경로로 변경 필요\n",
    "image_b64 = encode_image_to_base64(image_path)\n",
    "\n",
    "# Ollama 멀티모달 모델 설정 (gemma3:4b 대신 멀티모달 지원 모델 사용)\n",
    "# 예: llava, bakllava, llava-llama3 등\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",  # 또는 다른 멀티모달 지원 모델\n",
    "    temperature=0,\n",
    "    repeat_penalty=1.1,\n",
    ")\n",
    "\n",
    "# 이미지와 텍스트가 함께 있는 메시지 구성 방법\n",
    "multimodal_message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": f\"data:image/jpeg;base64,{image_b64}\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\", \n",
    "            \"text\": \"이 이미지에 대해 설명해주세요.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 멀티모달 메시지 호출 예시\n",
    "multimodal_response = llm.invoke([SystemMessage(content=\"You are a helpful assistant.\"), multimodal_message])\n",
    "print(multimodal_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import base64\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"안녕하세요, 당신은 누구인가요?\"),\n",
    "]\n",
    "\n",
    "# 텍스트 메시지 스트리밍\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import base64\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "# 이미지 경로\n",
    "imagePath = \"./data/docling_processing.png\"\n",
    "\n",
    "# 이미지 인코딩 함수\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# 이미지 인코딩\n",
    "base64_image = encode_image(imagePath)\n",
    "\n",
    "# 텍스트 메시지 예시\n",
    "\n",
    "\n",
    "# 이미지와 함께 메시지 보내기 예시\n",
    "multimodal_messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What's in this image?\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "]\n",
    "\n",
    "for chunk in llm.stream(multimodal_messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init_chat_model로 동일한 형식으로 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "ollama=init_chat_model(\n",
    "    \"gemma3:4b\", model_provider=\"ollama\",temperature=0\n",
    ")\n",
    "\n",
    "groq=init_chat_model(\n",
    "    \"meta-llama/llama-4-scout-17b-16e-instruct\", model_provider=\"groq\",temperature=0\n",
    ")  \n",
    "\n",
    "\n",
    "\n",
    "llm_list = [\n",
    "    {\"name\": \"Ollama (Gemma 3 4B)\", \"model\": ollama},\n",
    "    {\"name\": \"Groq (Llama-4-Scout-17B)\", \"model\": groq},\n",
    "]\n",
    "\n",
    "for llm_info in llm_list:\n",
    "    print(f\"모델: {llm_info['name']}\")\n",
    "    print(\"=\"*50)\n",
    "    response = llm_info[\"model\"].invoke(\"안녕하세요, 당신은 누구인가요?\")\n",
    "    print(response.content)\n",
    "    print(\"-\"*50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
