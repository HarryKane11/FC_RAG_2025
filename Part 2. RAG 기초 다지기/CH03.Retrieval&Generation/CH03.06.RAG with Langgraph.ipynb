{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCELë¡œ RAG êµ¬ì¶•í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_qdrant import qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFë¡œ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\n",
    "    file_path=\"data/arxiv_paper.pdf\",\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes.\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., â€œI am 70%\n",
      "confident I craft a wooden tableâ€). 3) Interactive depen-\n",
      "dencies, where the agentâ€™s actions directly influence the\n",
      "environment, which in turn affects subsequent decisions,\n",
      "requiring ongoing adjustments to confidence estimates as\n",
      "tasks progress. 4) Finally, while state-of-the-art embodied\n",
      "agents leverage proprietary Large Language Models (LLMs)\n",
      "and Vision-Language Models (VLMs) for their strong mu\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "ElicitationPolicies What do you see?How confident are you? \n",
      "I see a pig next to\n",
      "a tree to the left [85% confident]\n",
      "ExecutionPolicies\n",
      "What do you see? Especially\n",
      "around the pig. How\n",
      "confident are you?\n",
      "What do you see? Especially\n",
      "around the trees. Howconfident are you?\n",
      "What do you see? Especially\n",
      "to your right. How confident\n",
      "are you?\n",
      "I see a pig with two trees \n",
      "[80% confident]\n",
      "I see sheeps and cows \n",
      "[90% confident]\n",
      "I see a mountain to the left\n",
      "[70% confide\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Method Prompt\n",
      "Vanilla\n",
      "Read the task (e.g., collect wood, build a shelter), provide your answer, and explain how confident\n",
      "you are in perceiving the environment accurately to complete the task (e.g., recognizing resources,\n",
      "locating structures, identifying threats).\n",
      "Read the task given, provide your answer, and explain how confident you are in planning and executing\n",
      "the actions needed to achieve the goal (e.g., gathering materials, crafting tools, building\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes.\n",
      "To address this, we introduce a set of policies that gener-\n",
      "ate additional observations and diverse action trajectories,\n",
      "promoting robust confidence assessment:\n",
      "âŸ³ Action Sampling: The agent can generate multiple pos-\n",
      "sible actions by sampling from a learned policy distribution\n",
      "over the action space, conditioned on the current state and\n",
      "task objectives. By doing so, the agent can explore multiple\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Metric Model Vanilla Self-Intervention CoT (Inductive) P&S (Deductive) Top-K (Abductive)\n",
      "ECE â†“\n",
      "GPT-4V 0.27 0.21 0.16 0.15 0.17\n",
      "MineLLM 0.49 0.41 0.34 0.39 0.43\n",
      "STEVE 0.43 0.32 0.26 0.26 0.35\n",
      "AUROC â†‘\n",
      "GPT-4V 0.69 0.76 0.83 0.82 0.73\n",
      "MineLLM 0.53 0.59 0.64 0.61 0.58\n",
      "STEVE 0.58 0.69 0.72 0.67 0.68\n",
      "PR-P â†‘\n",
      "GPT-4V 0.66 0.76 0.81 0.79 0.70\n",
      "MineLLM 0.51 0.59 0.63 0.60 0.57\n",
      "STEVE 0.56 0.67 0.69 0.66 0.64\n",
      "PR-N â†‘\n",
      "GPT-4V 0.52 0.53 0.58 0.55 0.53\n",
      "MineLLM 0.39 0.42 0.4\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Vanilla Self-Intervention CoT P&S Top-K\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0.24\n",
      "0.22\n",
      "0.14\n",
      "0.10\n",
      "0.12\n",
      "0.11\n",
      "0.11\n",
      "0.12\n",
      "0.11\n",
      "0.11\n",
      "0.13\n",
      "0.18\n",
      "0.12\n",
      "0.14\n",
      "0.15\n",
      "ECE â†“\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0.50\n",
      "0.53\n",
      "0.52\n",
      "0.35\n",
      "0.42\n",
      "0.50\n",
      "0.32\n",
      "0.44\n",
      "0.33\n",
      "0.30\n",
      "0.45\n",
      "0.38\n",
      "0.42\n",
      "0.47\n",
      "0.43\n",
      "ECE â†“\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0.40\n",
      "0.42\n",
      "0.41\n",
      "0.28\n",
      "0.27\n",
      "0.39\n",
      "0.24\n",
      "0.24\n",
      "0.33\n",
      "0.27\n",
      "0.28\n",
      "0.35\n",
      "0.30\n",
      "0.31\n",
      "0.34\n",
      "ECE â†“\n",
      "Action\n",
      "Sampling\n",
      "Scenario\n",
      "Reinterpretation\n",
      "Hypothetical\n",
      "Reasoning\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0.76\n",
      "0.75\n",
      "0.78\n",
      "0.80\n",
      "0.79\n",
      "0.81\n",
      "\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Execution Strategies GPT-4V MineLLM STEVE\n",
      "ECE â†“ AUROC â†‘ ECE â†“ AUROC â†‘ ECE â†“ AUROC â†‘\n",
      "No Execution Strategy 0.27 0.69 0.49 0.53 0.43 0.58\n",
      "AS + SR 0.18 0.82 0.32 0.59 0.39 0.69\n",
      "AS + HR 0.20 0.79 0.34 0.57 0.37 0.66\n",
      "SR + HR 0.22 0.80 0.37 0.54 0.44 0.58\n",
      "AS + SR + HR 0.17 0.83 0.32 0.62 0.38 0.69\n",
      "Table 3.Performance of Vanilla Elicitation with Combined Execution Strategies. AS = Action Sampling, SR = Scenario Reinterpreta-\n",
      "tion, HR = Hypothetical Reasoning. E\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Goel, V . Anatomy of deductive reasoning.Trends in cogni-\n",
      "tive sciences, 2007.\n",
      "Groot, T. and Valdenegro Toro, M. Overconfidence is key:\n",
      "Verbalized uncertainty evaluation in large language and\n",
      "vision-language models. In Trustworthy Natural Lan-\n",
      "guage Processing, 2024.\n",
      "Guo, X., Huang, K., Liu, J., Fan, W., V Â´elez, N., Wu, Q.,\n",
      "Wang, H., Griffiths, T. L., and Wang, M. Embodied\n",
      "LLM agents learn to cooperate in organized teams. In\n",
      "Language Gamification - Neur\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agentsâ€™ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using baye\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Interactive planning with large language models enables\n",
      "open-world multi-task agents. In Advances in Neural\n",
      "Information Processing Systems, 2023c.\n",
      "Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\n",
      "Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\n",
      "zler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\n",
      "P., Dean, J., and Fedus, W. Emergent abilities of large\n",
      "language models. Transactions on Machine Learning\n",
      "Research, 2022a.\n",
      "Wei, J., Wang, X., Sc\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "A. Definitions of Uncertainty Types\n",
      "The three fundamental forms of logical reasoning, inductive, deductive, and abductive, have long been recognized and\n",
      "studied (Peirce, 1934; Walton, 2014; Wei et al., 2022b; Levine et al., 2022; Okoli, 2023). As language models demonstrated\n",
      "extraordinary abilities, designing better reasoning mechanisms has become a popular research trend (Cheng et al., 2024; Liu\n",
      "et al., 2024). These reasoning paradigms serve as fundamen\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "uncertainty occurs within a structured decision-making process when the available information is insufficient to determine\n",
      "a definitive outcome. Consider an agent tasked with crafting a wooden tool in a survival environment. It knows the rule:\n",
      "â€If wood is available, then a wooden tool can be crafted.â€ However, if the agent is uncertain whether wood is currently\n",
      "accessible, it cannot confidently conclude whether crafting is possible. This scenario exempli\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Mine Wood Log\n",
      "Craft Wooden\n",
      "Planks\n",
      "Craft Sticks\n",
      "Craft Wooden\n",
      "Pickaxe\n",
      "Craft Stone\n",
      "Pickaxe\n",
      "Mine\n",
      "Cobblestone\n",
      "Craft\n",
      "CraftingTable\n",
      "Craft Iron\n",
      "Pickaxe\n",
      "Smelt Raw\n",
      "Iron\n",
      "Mine Iron Ore\n",
      "Mine Coal Ore\n",
      "Craft Furnace\n",
      "Obtain Diamond\n",
      "Figure 6.An illustrative diagram of the Obtain Diamond task, featuring five distinct colors to represent the source materials re-\n",
      "quiredâ€”wood, stone, iron, coal, and diamondâ€”aligned with the Minecraft tech tree.\n",
      "ability to balance perception,\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "Difficulty Task ID Task Description\n",
      "Easy\n",
      "1 Find a pig\n",
      "2 Find a cow\n",
      "3 Find a tree\n",
      "4 Mine log\n",
      "5 Mine sand\n",
      "6 Craft a plank\n",
      "7 Craft a stick\n",
      "8 Craft a chest\n",
      "9 Craft a wooden door\n",
      "10 Craft a wooden boat\n",
      "Medium\n",
      "11 Find a tree\n",
      " in the forest\n",
      "12 Find a pig\n",
      " on grass\n",
      "13 Find a cow\n",
      " in the desert\n",
      "14 Craft a wooden sword\n",
      "15 Craft a wooden pickaxe\n",
      "16 Craft a stone pickaxe\n",
      "17 Smelt an iron ingot\n",
      "18 Smelt glass\n",
      "19 Cook beef\n",
      "20 Cook mutton\n",
      "Hard\n",
      "21 Find a pig\n",
      " near a gra\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "language understanding. Trained on a vast dataset of 500,000 Minecraft-specific image-text instruction pairs, MineLLM\n",
      "can generate detailed insights about the game environment, answer complex queries, and provide contextual guidance for\n",
      "planning and execution. Its integration into MP5 enables the framework to address context- and process-dependent tasks\n",
      "with remarkable success rates, achieving a 91% success rate on context-dependent tasks and demonstrati\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "These results confirm our hypothesis that as task difficulty increases, confidence calibration significantly deteriorates, with\n",
      "the ECE gap increasing as high as 0.20 (Easy CoT vs. Hard CoT). However, the results also demonstrate that structured\n",
      "elicitation policies, such as CoT and P&S, consistently prove effective in handling calibration, failure prediction, and task\n",
      "success across task difficulties. Additionally, simpler policies like Self-Interventio\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 17, 'page_label': '18'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content[:500])\n",
    "    print(doc.metadata)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([len(chunk.page_content) for chunk in splits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama Embeddingìœ¼ë¡œ bge-m3 ê¸°ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qdrant ë²¡í„°DB(ì¸ë©”ëª¨ë¦¬)ì— ì„ë² ë”© ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "# ë¡œì»¬ ì €ì¥ì†Œë¥¼ ìœ„í•œ Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = QdrantClient(path=\"/tmp/arxiv_paper\")\n",
    "\n",
    "# # ë°€ì§‘ ë²¡í„°ë¡œ ì»¬ë ‰ì…˜ ìƒì„±\n",
    "# client.create_collection(\n",
    "#     collection_name=\"embodied_agent_0413\",\n",
    "#     vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    "# )\n",
    "\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"embodied_agent_0413\",\n",
    "    embedding=embeddings,\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "# uuids = [str(uuid4()) for _ in range(len(splits))]\n",
    "\n",
    "# qdrant.add_documents(documents=splits, ids=uuids)\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œë¥¼ ê²€ìƒ‰ê¸°ë¡œ ì„¤ì •\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë²¡í„°DB ê¸°ë°˜ Retriever êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', '_id': 'd40b3ddb-0965-4d47-8d44-80a29acf5121', '_collection_name': 'embodied_agent_0413'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', '_id': 'a8c2d274-dae5-4317-86b8-ef534f6530a1', '_collection_name': 'embodied_agent_0413'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVEâ€™s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', '_id': '040ae1f1-1371-44d2-a235-23a545587c87', '_collection_name': 'embodied_agent_0413'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes.\n",
      "To address this, we introduce a set of policies that gener-\n",
      "ate additional observations and diverse action trajectories,\n",
      "promoting robust confidence assessment:\n",
      "âŸ³Action Sampling : The agent can generate multiple pos-\n",
      "sible actions by sampling from a learned policy distribution\n",
      "over the action space, conditioned on the current state and\n",
      "task objectives. By doing so, the agent can explore multiple\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', '_id': '48f0aa47-5125-45b1-95e5-09a63d6fe176', '_collection_name': 'embodied_agent_0413'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "and other types of language-based guidance. For a given\n",
      "taskT, the agent operates under a policy Ï€:I â†’ A that\n",
      "maps input Ito actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score câˆˆ[0,1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks.\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-14T01:17:30+00:00', 'author': 'Tianjiao Yu,, Vedant Shah,, Muntasir Wahed,, Kiet A. Nguyen,, Adheesh Juvekar, Tal August,, Ismini Lourentzou', 'keywords': 'Embodied Agents, Uncertainty in Embodied AI, Verbalized Uncertainty', 'moddate': '2025-03-14T01:17:30+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2025', 'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'trapped': '/False', 'source': 'data/arxiv_paper.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', '_id': '67e06c28-e160-4552-9115-405401398b8d', '_collection_name': 'embodied_agent_0413'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retriever = qdrant.as_retriever()\n",
    "\n",
    "search_result=retriever.invoke(\"Embodied Agentê°€ ë­ì•¼?\", k=5)\n",
    "\n",
    "for doc in search_result:\n",
    "    print(doc.page_content[:500])\n",
    "    print(doc.metadata)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "ë‹¹ì‹ ì€ Q&A ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë‹¨ìˆœ ë‹µë³€ Streamingí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥´ë©´, Embodied AgentëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   **ë¶ˆí™•ì‹¤ì„± íšë“ì„ ìœ„í•œ ì •ì±… ìƒì„±:** ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•˜ê³  ë‹¤ì–‘í•œ ê²°ê³¼ì— ëŒ€ë¹„í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê´€ì°°ì„ ìƒì„±í•˜ê³  ë‹¤ì–‘í•œ í–‰ë™ ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ” ì •ì±…ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "*   **í–‰ë™ ìƒ˜í”Œë§:** í˜„ì¬ ìƒíƒœì™€ ì‘ì—… ëª©í‘œì— ë”°ë¼ í•™ìŠµëœ ì •ì±… ë¶„í¬ì—ì„œ ì—¬ëŸ¬ ê°€ëŠ¥í•œ í–‰ë™ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  ê°€ì¥ ì„±ê³µí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ í–‰ë™ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
      "*   **ì‹œë‚˜ë¦¬ì˜¤ ì¬í•´ì„:** ë™ì¼í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì¬í•´ì„í•˜ì—¬ ë‹¤ë¥¸ í–‰ë™ ê²½ë¡œë¥¼ ì œì•ˆí•˜ê³  ê´€ë ¨ í™˜ê²½ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
      "*   **ì‹¤í–‰ ì •ì±…:** ì‹œë‚˜ë¦¬ì˜¤ ì¬í•´ì„, í–‰ë™ ìƒ˜í”Œë§, ê°€ì„¤ì  ì¶”ë¡ ì„ í†µí•´ ì‹ ë¢°ë„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ í–¥ìƒì‹œí‚¤ëŠ” ì •ì±…ì…ë‹ˆë‹¤.\n",
      "*   **ë³µì¡í•œ í™˜ê²½ì—ì„œì˜ ì‹ ë¢° í‘œí˜„:** ë¶ˆí™•ì‹¤ì„± í‰ê°€, ì¶”ë¡ , í™˜ê²½ ìƒí˜¸ ì‘ìš©ì„ í†µí•©í•˜ì—¬ ë³µì¡í•œ í™˜ê²½ì—ì„œ ì‹ ë¢°ë¥¼ í‘œí˜„í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.\n",
      "\n",
      "ìš”ì•½í•˜ìë©´, Embodied AgentëŠ” ë¶ˆí™•ì‹¤ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  í‘œí˜„í•˜ì—¬ ë³µì¡í•œ í™˜ê²½ì—ì„œ ì‹ ë¢°ë¥¼ íšë“í•˜ê³  ìº˜ë¦¬ë¸Œë ˆì´ì…˜í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶˜ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:4b\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"Embodied Agentë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\"):\n",
    "    print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì°¸ê³ í•œ ì†ŒìŠ¤ë¥¼ í•¨ê»˜ ìŠ¤íŠ¸ë¦¬ë°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Retrieved Documents:\n",
      "ì†ŒìŠ¤ [1]: <Uncertainty in Action: Confidence Elicitation in Embodied Agents>\n",
      "ì°¸ê³  ë‚´ìš©: Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes...\n",
      "\n",
      "ì†ŒìŠ¤ [2]: <Uncertainty in Action: Confidence Elicitation in Embodied Agents>\n",
      "ì°¸ê³  ë‚´ìš©: Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir ...\n",
      "\n",
      "ì†ŒìŠ¤ [3]: <Uncertainty in Action: Confidence Elicitation in Embodied Agents>\n",
      "ì°¸ê³  ë‚´ìš©: tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 pa...\n",
      "\n",
      "ì†ŒìŠ¤ [4]: <Uncertainty in Action: Confidence Elicitation in Embodied Agents>\n",
      "ì°¸ê³  ë‚´ìš©: Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥´ë©´, Embodied AgentëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   **ë¶ˆí™•ì‹¤ì„± íšë“ì„ ìœ„í•œ ì •ì±… ìƒì„±:** ì´ ì—ì´ì „íŠ¸ëŠ” í˜„ì¬ ìƒíƒœì™€ ì‘ì—… ëª©í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ í–‰ë™ ê²½ë¡œë¥¼ ìƒì„±í•˜ì—¬ ì‹ ë¢°ë„ í‰ê°€ë¥¼ ì´‰ì§„í•˜ëŠ” ì •ì±… ì„¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "*   **í–‰ë™ ìƒ˜í”Œë§:** ì—ì´ì „íŠ¸ëŠ” í•™ìŠµëœ ì •ì±… ë¶„í¬ì—ì„œ ì—¬ëŸ¬ ê°€ëŠ¥í•œ í–‰ë™ì„ ìƒì„±í•˜ì—¬ ë‹¤ì–‘í•œ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  ê°€ì¥ ì„±ê³µí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ í–‰ë™ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
      "*   **ì‹œë‚˜ë¦¬ì˜¤ ì¬í•´ì„:** ì—ì´ì „íŠ¸ëŠ” ë‹¤ë¥¸ ê´€ì ì—ì„œ ë™ì¼í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì¬í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ íŠ¹ì • ê°ì²´ì— ì§‘ì¤‘í•˜ê±°ë‚˜, í™˜ê²½ ì¥ì• ë¬¼ì„ ì¬í‰ê°€í•˜ê±°ë‚˜, ëª©í‘œë¬¼ì˜ ê·¼ì ‘ì„±ì„ ì¬í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "*   **ì‹¤í–‰ ì •ì±…:** ì‹œë‚˜ë¦¬ì˜¤ ì¬í•´ì„, í–‰ë™ ìƒ˜í”Œë§, ê°€ì„¤ì  ì¶”ë¡ ì„ í†µí•´ ì‹ ë¢°ë„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤í–‰ ì •ì±…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "ìš”ì•½í•˜ë©´, Embodied AgentëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ê°ì§€í•˜ê³  ì˜ˆì¸¡í•˜ë©°, ë‹¤ì–‘í•œ í–‰ë™ ê²½ë¡œë¥¼ ìƒì„±í•˜ì—¬ ì‹ ë¢°ë„ í‰ê°€ë¥¼ ì´‰ì§„í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì²´ì¸ ì •ì˜\n",
    "rag_chain_from_docs = (\n",
    "    # ì…ë ¥ ë°ì´í„°ì—ì„œ context í•„ë“œë¥¼ ì¶”ì¶œí•˜ì—¬ format_docs í•¨ìˆ˜ë¡œ ì²˜ë¦¬\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    # í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬\n",
    "    | prompt\n",
    "    # í”„ë¡¬í”„íŠ¸ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    | llm\n",
    "    # LLMì˜ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ ê²°ê³¼ì™€ ì§ˆë¬¸ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ìµœì¢… ì²´ì¸\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    # ê²€ìƒ‰ê¸°(retriever)ë¡œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ê³ , ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "# ìœ„ì—ì„œ ìƒì„±ëœ contextì™€ questionì„ rag_chain_from_docsì— ì „ë‹¬í•˜ì—¬ answer í•„ë“œ ì¶”ê°€\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "for chunk in rag_chain_with_source.stream(\"Embodied Agentë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\"):\n",
    "    if \"context\" in chunk:\n",
    "        print(\"ğŸ“š Retrieved Documents:\")\n",
    "        for i, doc in enumerate(chunk[\"context\"]):\n",
    "            print(f\"ì†ŒìŠ¤ [{i+1}]: <{doc.metadata['title']}>\")\n",
    "            print(f\"ì°¸ê³  ë‚´ìš©: {doc.page_content[:100]}...\\n\")\n",
    "        print(\"-\" * 80)\n",
    "    elif \"answer\" in chunk:\n",
    "        print(chunk[\"answer\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
