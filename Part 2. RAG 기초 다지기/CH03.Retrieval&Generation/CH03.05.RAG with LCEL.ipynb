{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langgraphë¡œ RAG êµ¬ì¶•í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì„ ì–¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemma3:4b\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_qdrant import qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path=\"data/arxiv_paper.pdf\",\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qdrant ë²¡í„°DB ì €ì¥ ë° ê²€ìƒ‰ê¸° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from uuid import uuid4\n",
    "\n",
    "client = QdrantClient(path=\"/tmp/0416(1)/arxiv_paper\")\n",
    "\n",
    "# ë°€ì§‘ ë²¡í„°ë¡œ ì»¬ë ‰ì…˜ ìƒì„±\n",
    "client.create_collection(\n",
    "    collection_name=\"embodied_agent_0415\",\n",
    "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"embodied_agent_0415\",\n",
    "    embedding=embeddings,\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(splits))]\n",
    "\n",
    "qdrant.add_documents(documents=splits, ids=uuids)\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œë¥¼ ê²€ìƒ‰ê¸°ë¡œ ì„¤ì •\n",
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result=retriever.invoke(\"Embodied Agentê°€ ë­ì•¼?\")\n",
    "\n",
    "for doc in search_result:\n",
    "    print(doc.page_content[:500])\n",
    "    print(doc.metadata)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "ë‹¹ì‹ ì€ Q&A ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langgraph State, node, edge ì„ ì–¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"Embodied Agentê°€ ë­ì•¼?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Outputìœ¼ë¡œ ë‹µë³€ì— ì£¼ì„ ë‹¬ê¸° - GPT-4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "ë‹¹ì‹ ì€ Q&A ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\")\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (title + page number + year ) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: AnswerWithSources\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(str((doc.metadata,doc.page_content)) for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    structured_llm = llm.with_structured_output(AnswerWithSources)\n",
    "    response = structured_llm.invoke(messages)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = graph.invoke({\"question\": \"Embodied Agentê°€ ë­ì•¼?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer']['sources'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XMLë¡œ ë‹µë³€ì— ì°¸ê³  ë¬¸ì„œ ì£¼ì„ë‹¬ê¸° - Gemma3:4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# XML ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "xml_system = \"\"\"\n",
    "You're a helpful AI assistant. \n",
    "Given a user question and some document snippets,answer the user question and provide citations.\n",
    "If none of the documents answer the question, just say you don't know.\n",
    "\n",
    "Remember,You should return the source information(title, page, year) and answer. \n",
    "A citation consists of a VERBATIM quote that justifies the answer and the source information. \n",
    "Return a citation for every quote across all documents that justifies the answer. \n",
    "\n",
    "Use the following format for your final output:\n",
    "<cited_answer>\n",
    "    <answer></answer>\n",
    "    <citations>\n",
    "        <citation><source></source><quote></quote></citation>\n",
    "        <citation><source></source><quote></quote></citation>\n",
    "        ...\n",
    "    </citations>\n",
    "</cited_answer>\n",
    "\n",
    "Here are the documents:{context}\"\"\"\n",
    "\n",
    "xml_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", xml_system), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "def format_docs_xml(docs: List[Document]) -> str:\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source_info = f\"{doc.metadata.get('title', 'Unknown')}, page {doc.metadata.get('page', 'N/A')}, {doc.metadata.get('creationdate', 'N/A')[:4]}\"\n",
    "        doc_str = f\"\"\"\\\n",
    "        <source id=\\\"{i}\\\">\n",
    "            <source_info>{source_info}</source_info>\n",
    "            <content>{doc.page_content}</content>\n",
    "        </source>\"\"\"\n",
    "        formatted.append(doc_str)\n",
    "    return \"\\n\\n<sources>\" + \"\\n\".join(formatted) + \"</sources>\"\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    citations: List[str]\n",
    "    answer: Dict\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    formatted_docs = format_docs_xml(state[\"context\"])\n",
    "    messages = xml_prompt.invoke(\n",
    "        {\"question\": state[\"question\"], \"context\": formatted_docs}\n",
    "    )\n",
    "    llm = ChatOllama(model=\"gemma3:4b\")\n",
    "    response = llm.invoke(messages)\n",
    "    parsed_response = XMLOutputParser().invoke(response.content)\n",
    "    return {\"answer\": parsed_response, \"citations\": formatted_docs}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = graph.invoke({\"question\": \"Embodied Agentê°€ ë­ì•¼?\"}, config = {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "print(json.dumps(result[\"answer\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for citation in result[\"answer\"]['cited_answer'][1]['citations']:\n",
    "    print(f\"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {citation['citation'][0]['source']}\")\n",
    "    print(f\"ğŸ’¬ ì°¸ê³ í•œ ë¶€ë¶„: {citation['citation'][1]['quote']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"answer\"]['cited_answer'][0]['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
