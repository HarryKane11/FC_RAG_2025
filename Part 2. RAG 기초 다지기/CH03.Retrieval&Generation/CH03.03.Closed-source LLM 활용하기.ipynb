{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "%pip install -U anthropic openai google-generativeai langchain-openai langchain-anthropic langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API ì‚¬ìš© ì˜ˆì œ\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "# API í‚¤ ì„¤ì •\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# GPT-4o ëª¨ë¸ í˜¸ì¶œ\n",
    "response_gpt4o = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": \"íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(Markdown(f\"### ğŸ’¬ ë‹µë³€\\n\\n{response_gpt4o.choices[0].message.content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3-mini ëª¨ë¸ í˜¸ì¶œ (ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©)\n",
    "response_o3mini = client.responses.create(\n",
    "    model=\"o3-mini\",  # o3-mini-highê°€ ì•„ë‹Œ o3-mini ì‚¬ìš©\n",
    "    reasoning={\"effort\": \"low\"}, #low, medium, high ì‚¬ìš© ê°€ëŠ¥\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": \"íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"### ğŸ’¬ ë‹µë³€\\n\\n{response_o3mini.output_text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude API í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude API ì‚¬ìš© ì˜ˆì œ\n",
    "import anthropic\n",
    "\n",
    "# API í‚¤ ì„¤ì •\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def render_response(blocks):\n",
    "    for block in blocks:\n",
    "        if getattr(block, \"type\", None) == \"thinking\":\n",
    "            display(Markdown(f\"### ğŸ¤” Thinking\\n\\n{block.thinking}\"))\n",
    "        elif getattr(block, \"type\", None) == \"text\":\n",
    "            display(Markdown(f\"### ğŸ’¬ ë‹µë³€\\n\\n{block.text}\"))\n",
    "\n",
    "# Claude 3.7 Sonnet í˜¸ì¶œ ì˜ˆì‹œ\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-20250219\",  # Claude 3.7 Sonnet ëª¨ë¸\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    system=\"ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ì‘ë‹µ ì¶œë ¥\n",
    "render_response(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-20250219\",\n",
    "    max_tokens=3000,\n",
    "    thinking={\n",
    "        \"type\": \"enabled\",\n",
    "        \"budget_tokens\": 2000\n",
    "    },\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "render_response(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API ì‚¬ìš© ì˜ˆì œ\n",
    "from google import genai\n",
    "\n",
    "# API í‚¤ ì„¤ì •\n",
    "client = genai.Client()\n",
    "\n",
    "# Gemini ëª¨ë¸ í˜¸ì¶œ (ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"íŒŒì´ì¬ìœ¼ë¡œ ì´ì§„ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\"],\n",
    ")\n",
    "\n",
    "print(\"Gemini 2.0 Flash ì‘ë‹µ:\")\n",
    "display(Markdown(f\"### ğŸ’¬ ë‹µë³€\\n\\n{response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-thinking-exp\",\n",
    "    contents=[\"íŒŒì´ì¬ìœ¼ë¡œ ì´ì§„ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\"],\n",
    ")\n",
    "\n",
    "print(\"gemini-2.0-flash\")\n",
    "display(Markdown(f\"### ğŸ’¬ ë‹µë³€\\n\\n{response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchainì„ í™œìš©í•œ Closed LLM í™œìš©ë²•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¼ë°˜ ëª¨ë¸ í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "gpt4o = ChatOpenAI(model=\"gpt-4o\")\n",
    "claude = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# ì§ˆë¬¸ ì„¤ì •\n",
    "question = \"íŒŒì´ì¬ìœ¼ë¡œ ì´ì§„ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "models = [\n",
    "    {\"name\": \"GPT-4o\", \"model\": gpt4o},\n",
    "    {\"name\": \"Claude\", \"model\": claude},\n",
    "    {\"name\": \"Gemini\", \"model\": gemini}\n",
    "]\n",
    "\n",
    "# ê° ëª¨ë¸ì— ì§ˆë¬¸í•˜ê³  ì‘ë‹µ ë°›ê¸°\n",
    "for model_info in models:\n",
    "    print(f\"\\n{model_info['name']} ì‘ë‹µ:\")\n",
    "    for token in model_info[\"model\"].stream(question):\n",
    "        print(token.content, end=\"\")\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¶”ë¡  ëª¨ë¸ í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "o3_mini = ChatOpenAI(model=\"o3-mini\")\n",
    "\n",
    "for token in o3_mini.stream(\n",
    "    \"íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\",\n",
    "    reasoning_effort=\"medium\"\n",
    "    ):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropic Claude 3.7 sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "claude_3_7 = ChatAnthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=5000,\n",
    "    thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n",
    ")\n",
    "\n",
    "\n",
    "for token in claude_3_7.stream(\n",
    "    \"íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\",\n",
    "    ):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Gemini 2.0 Flash Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_2_0 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-thinking-exp\",\n",
    ")\n",
    "\n",
    "for token in gemini_2_0.stream(\n",
    "    \"íŒŒì´ì¬ìœ¼ë¡œ ì´ì§„ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.\",\n",
    "    ):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchainì˜ init_chat_modelë¡œ ë™ì¼í•œ í˜•ì‹ì˜ ëª¨ë¸ ì„ ì–¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¼ë°˜ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# langchain_openai.ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0)\n",
    "# langchain_anthropic.ChatAnthropic ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "claude_3_7 = init_chat_model(\n",
    "    \"claude-3-7-sonnet-latest\", model_provider=\"anthropic\", temperature=0\n",
    ")\n",
    "# langchain_google_genai.ChatGoogleGenerativeAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "gemini_2_0 = init_chat_model(\n",
    "    \"gemini-2.0-flash-thinking-exp\", model_provider=\"google_genai\", temperature=0\n",
    ")\n",
    "\n",
    "# ëª¨ë“  ëª¨ë¸ í†µí•©ì€ ChatModel ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ë¯€ë¡œ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "print(\"GPT-4o: \" + gpt_4o.invoke(\"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\").content + \"\\n\")\n",
    "print(\"Claude 3.7: \" + claude_3_7.invoke(\"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\").content + \"\\n\")\n",
    "print(\"Gemini 2.0: \" + gemini_2_0.invoke(\"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?\").content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¶”ë¡  ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_mini = init_chat_model(\n",
    "    \"openai:o3-mini\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"model\"\n",
    ")\n",
    "\n",
    "claude_3_7 = init_chat_model(\n",
    "    \"anthropic:claude-3-7-sonnet-latest\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"model\"\n",
    ")\n",
    "\n",
    "gemini_2_0 = init_chat_model(\n",
    "    \"google_genai:gemini-2.0-flash-thinking-exp\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_mini_response = o3_mini.invoke(\n",
    "    \"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”.\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"reasoning_effort\": \"low\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "claude_3_7_response = claude_3_7.invoke(\n",
    "    \"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”.\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"max_tokens\":5000,\n",
    "            \"thinking\":{\"type\": \"enabled\", \"budget_tokens\": 2000},\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"o3-mini: {o3_mini_response.content}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"claude-3-7-sonnet-latest: {claude_3_7_response.content}\")\n",
    "print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
