{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "%pip install -U anthropic openai google-generativeai langchain-openai langchain-anthropic langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 사용 예제\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "# API 키 설정\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# GPT-4o 모델 호출\n",
    "response_gpt4o = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 유용한 AI 비서입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"파이썬으로 퀵소트 알고리즘을 구현해주세요.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(Markdown(f\"### 💬 답변\\n\\n{response_gpt4o.choices[0].message.content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3-mini 모델 호출 (올바른 모델명 사용)\n",
    "response_o3mini = client.responses.create(\n",
    "    model=\"o3-mini\",  # o3-mini-high가 아닌 o3-mini 사용\n",
    "    reasoning={\"effort\": \"low\"}, #low, medium, high 사용 가능\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 유용한 AI 비서입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"파이썬으로 퀵소트 알고리즘을 구현해주세요.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"### 💬 답변\\n\\n{response_o3mini.output_text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude API 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude API 사용 예제\n",
    "import anthropic\n",
    "\n",
    "# API 키 설정\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def render_response(blocks):\n",
    "    for block in blocks:\n",
    "        if getattr(block, \"type\", None) == \"thinking\":\n",
    "            display(Markdown(f\"### 🤔 Thinking\\n\\n{block.thinking}\"))\n",
    "        elif getattr(block, \"type\", None) == \"text\":\n",
    "            display(Markdown(f\"### 💬 답변\\n\\n{block.text}\"))\n",
    "\n",
    "# Claude 3.7 Sonnet 호출 예시\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-20250219\",  # Claude 3.7 Sonnet 모델\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    system=\"당신은 유용한 AI 비서입니다.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"파이썬으로 피보나치 수열을 계산하는 코드를 작성해주세요.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 응답 출력\n",
    "render_response(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-20250219\",\n",
    "    max_tokens=3000,\n",
    "    thinking={\n",
    "        \"type\": \"enabled\",\n",
    "        \"budget_tokens\": 2000\n",
    "    },\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"파이썬으로 피보나치 수열을 계산하는 코드를 작성해주세요.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "render_response(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API 사용 예제\n",
    "from google import genai\n",
    "\n",
    "# API 키 설정\n",
    "client = genai.Client()\n",
    "\n",
    "# Gemini 모델 호출 (올바른 모델명 사용)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"파이썬으로 이진 검색 알고리즘을 구현해주세요.\"],\n",
    ")\n",
    "\n",
    "print(\"Gemini 2.0 Flash 응답:\")\n",
    "display(Markdown(f\"### 💬 답변\\n\\n{response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-thinking-exp\",\n",
    "    contents=[\"파이썬으로 이진 검색 알고리즘을 구현해주세요.\"],\n",
    ")\n",
    "\n",
    "print(\"gemini-2.0-flash\")\n",
    "display(Markdown(f\"### 💬 답변\\n\\n{response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain을 활용한 Closed LLM 활용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반 모델 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# API 키 설정 (환경 변수에서 가져옴)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# 모델 초기화\n",
    "gpt4o = ChatOpenAI(model=\"gpt-4o\")\n",
    "claude = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# 질문 설정\n",
    "question = \"파이썬으로 이진 검색 알고리즘을 구현해주세요.\"\n",
    "\n",
    "# 모델 리스트 생성\n",
    "models = [\n",
    "    {\"name\": \"GPT-4o\", \"model\": gpt4o},\n",
    "    {\"name\": \"Claude\", \"model\": claude},\n",
    "    {\"name\": \"Gemini\", \"model\": gemini}\n",
    "]\n",
    "\n",
    "# 각 모델에 질문하고 응답 받기\n",
    "for model_info in models:\n",
    "    print(f\"\\n{model_info['name']} 응답:\")\n",
    "    for token in model_info[\"model\"].stream(question):\n",
    "        print(token.content, end=\"\")\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추론 모델 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "o3_mini = ChatOpenAI(model=\"o3-mini\")\n",
    "\n",
    "for token in o3_mini.stream(\n",
    "    \"파이썬으로 퀵소트 알고리즘을 구현해주세요.\",\n",
    "    reasoning_effort=\"medium\"\n",
    "    ):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthropic Claude 3.7 sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "claude_3_7 = ChatAnthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=5000,\n",
    "    thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n",
    ")\n",
    "\n",
    "\n",
    "for token in claude_3_7.stream(\n",
    "    \"파이썬으로 퀵소트 알고리즘을 구현해주세요.\",\n",
    "    ):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Gemini 2.0 Flash Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_2_0 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-thinking-exp\",\n",
    ")\n",
    "\n",
    "for token in gemini_2_0.stream(\n",
    "    \"파이썬으로 이진 검색 알고리즘을 구현해주세요.\",\n",
    "    ):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain의 init_chat_model로 동일한 형식의 모델 선언하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# langchain_openai.ChatOpenAI 인스턴스를 반환합니다.\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0)\n",
    "# langchain_anthropic.ChatAnthropic 인스턴스를 반환합니다.\n",
    "claude_3_7 = init_chat_model(\n",
    "    \"claude-3-7-sonnet-latest\", model_provider=\"anthropic\", temperature=0\n",
    ")\n",
    "# langchain_google_genai.ChatGoogleGenerativeAI 인스턴스를 반환합니다.\n",
    "gemini_2_0 = init_chat_model(\n",
    "    \"gemini-2.0-flash-thinking-exp\", model_provider=\"google_genai\", temperature=0\n",
    ")\n",
    "\n",
    "# 모든 모델 통합은 ChatModel 인터페이스를 구현하므로 동일한 방식으로 사용할 수 있습니다.\n",
    "print(\"GPT-4o: \" + gpt_4o.invoke(\"당신의 이름은 무엇인가요?\").content + \"\\n\")\n",
    "print(\"Claude 3.7: \" + claude_3_7.invoke(\"당신의 이름은 무엇인가요?\").content + \"\\n\")\n",
    "print(\"Gemini 2.0: \" + gemini_2_0.invoke(\"당신의 이름은 무엇인가요?\").content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추론 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_mini = init_chat_model(\n",
    "    \"openai:o3-mini\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"model\"\n",
    ")\n",
    "\n",
    "claude_3_7 = init_chat_model(\n",
    "    \"anthropic:claude-3-7-sonnet-latest\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"model\"\n",
    ")\n",
    "\n",
    "gemini_2_0 = init_chat_model(\n",
    "    \"google_genai:gemini-2.0-flash-thinking-exp\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_mini_response = o3_mini.invoke(\n",
    "    \"피보나치 수열을 파이썬으로 구현해주세요.\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"reasoning_effort\": \"low\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "claude_3_7_response = claude_3_7.invoke(\n",
    "    \"피보나치 수열을 파이썬으로 구현해주세요.\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"max_tokens\":5000,\n",
    "            \"thinking\":{\"type\": \"enabled\", \"budget_tokens\": 2000},\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"o3-mini: {o3_mini_response.content}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"claude-3-7-sonnet-latest: {claude_3_7_response.content}\")\n",
    "print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
