{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a4b64c",
   "metadata": {},
   "source": [
    "### Semantic Chunker의 원리 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdfd4b",
   "metadata": {},
   "source": [
    "SemanticChunker는 LangChain에서 제공하는 텍스트 분할 기능 중 하나로, 임베딩모델만 있으면 쉽게 적용할 수 있습니다.\n",
    "\n",
    "그러나 하나씩 로직을 구현해보면서 SemanticChunker의 원리를 이해해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59231d3b",
   "metadata": {},
   "source": [
    "#### 📄 Step 1. PDF 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e757bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 경로 설정 (사용자 파일로 변경하세요)\n",
    "pdf_path = \"data\\국가별 공공부문 AI 도입 및 활용 전략.pdf\"\n",
    "\n",
    "# PDF 문서 로드\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "text = \"\\n\".join([p.page_content for p in pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023fc92",
   "metadata": {},
   "source": [
    "#### ✂️ Step 2. 문장 단위로 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d782324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 기본적인 정규표현식 기반 문장 분리\n",
    "sentences_raw = re.split(r'(?<=[.?!])\\s+', text)\n",
    "sentences = [{'sentence': s, 'index': i} for i, s in enumerate(sentences_raw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ad8db",
   "metadata": {},
   "source": [
    "#### 🔁 Step 3. 인접 문장 결합 (context noise 완화)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4fd7e",
   "metadata": {},
   "source": [
    "✅ 왜? → 너무 짧은 문장은 의미가 희미해서 임베딩이 불안정함\n",
    "\n",
    "\n",
    "→ 앞뒤 문장을 함께 포함시켜 \"의미 밀도\"를 높임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61235af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined = ''\n",
    "        for j in range(i - buffer, i + buffer + 1):\n",
    "            if 0 <= j < len(sentences):\n",
    "                combined += sentences[j]['sentence'] + ' '\n",
    "        sentences[i]['combined'] = combined.strip()\n",
    "    return sentences\n",
    "\n",
    "# 개선\n",
    "# 문장 하나만 사용하면 임베딩이 불안정해질 수 있음\n",
    "# → 앞뒤 문장을 함께 결합하여 의미 밀도를 높이고,\n",
    "#    코사인 거리 기반 분할 시 더 안정적인 의미 단절 판단 가능하게 만듦\n",
    "sentences = combine_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2e271",
   "metadata": {},
   "source": [
    "#### 🔍 Step 4. 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embedding_model = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "embeddings = embedding_model.embed_documents([s['combined'] for s in sentences])\n",
    "\n",
    "# 각 문장 dict에 임베딩 추가\n",
    "for i, emb in enumerate(embeddings):\n",
    "    sentences[i]['embedding'] = emb\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ddb33",
   "metadata": {},
   "source": [
    "#### 🧮 Step 5. 문장 간 코사인 거리 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93559238",
   "metadata": {},
   "source": [
    "✅ 목적: 앞뒤 문장의 의미 차이를 수치화하여 단절점 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "distances = []\n",
    "for i in range(len(sentences) - 1):\n",
    "    sim = cosine_similarity([sentences[i]['embedding']], [sentences[i + 1]['embedding']])[0][0]\n",
    "    distance = 1 - sim\n",
    "    distances.append(distance)\n",
    "    sentences[i]['distance_to_next'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886ed40",
   "metadata": {},
   "source": [
    "#### 📊 Step 6. 거리 시각화(한글 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ed1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "# ✅ 시스템별 한글 폰트 설정 (Windows: 맑은 고딕, macOS: AppleGothic, Linux: 나눔고딕)\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "elif platform.system() == 'Darwin':\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "else:\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "\n",
    "threshold = np.percentile(distances, 90)\n",
    "\n",
    "# 코사인 거리 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(distances)\n",
    "plt.axhline(threshold, color='red', linestyle='--', label='90% 임계값')\n",
    "plt.title(\"문장 간 임베딩 거리\")\n",
    "plt.xlabel(\"문장 위치\")\n",
    "plt.ylabel(\"코사인 거리\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82575d",
   "metadata": {},
   "source": [
    "#### ✂️ Step 7. 의미 기반 청크 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74d07e",
   "metadata": {},
   "source": [
    "✅ 거리 값이 급격히 커지는 지점을 단락 구분점으로 간주\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_points = [i for i, d in enumerate(distances) if d > threshold]\n",
    "\n",
    "chunks_manual = []\n",
    "start = 0\n",
    "for idx in break_points:\n",
    "    text = ' '.join([s['sentence'] for s in sentences[start:idx + 1]])\n",
    "    chunks_manual.append(text)\n",
    "    start = idx + 1\n",
    "if start < len(sentences):\n",
    "    chunks_manual.append(' '.join([s['sentence'] for s in sentences[start:]]))\n",
    "\n",
    "# 🔍 예시 청크 출력\n",
    "print(\"수동 청크 개수:\", len(chunks_manual))\n",
    "print(\"\\n예시 청크 1:\\n\", chunks_manual[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d137d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 청크의 길이 계산 및 출력\n",
    "chunk_lengths = [len(chunk) for chunk in chunks_manual]\n",
    "print(\"각 청크별 길이:\", chunk_lengths)\n",
    "\n",
    "# 특정 청크 내용 출력 \n",
    "print(\"\\n<3번째 청크 내용>\")\n",
    "print(\"-\" * 80)  # 구분선 추가\n",
    "print(chunks_manual[2])\n",
    "print(\"-\" * 80)  # 구분선 추가\n",
    "\n",
    "print(\"\\n<4번째 청크 내용>\")\n",
    "print(\"-\" * 80)  # 구분선 추가\n",
    "print(chunks_manual[3])\n",
    "print(\"-\" * 80)  # 구분선 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e57df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks_manual[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bc8e1",
   "metadata": {},
   "source": [
    "### 🧠 LangChain의 SemanticChunker로 자동 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 경로 설정 (사용자 파일로 변경하세요)\n",
    "pdf_path = \"data\\국가별 공공부문 AI 도입 및 활용 전략.pdf\"\n",
    "\n",
    "# PDF 문서 로드\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "text = \"\\n\".join([p.page_content for p in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# SemanticChunker 적용\n",
    "semantic_chunker = SemanticChunker(embedding_model, \n",
    "    breakpoint_threshold_type=\"percentile\", \n",
    "    breakpoint_threshold_amount=85)\n",
    "    \n",
    "sem_chunks = semantic_chunker.create_documents([text])\n",
    "\n",
    "print(\"\\nSemanticChunker 청크 수:\", len(sem_chunks))\n",
    "print(\"\\n예시 청크 1:\\n\", sem_chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc181bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sem_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(chunk.page_content) for chunk in sem_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 청크의 길이 계산 및 출력\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in sem_chunks]\n",
    "print(\"각 청크별 길이:\", chunk_lengths)\n",
    "\n",
    "# 특정 청크 내용 출력 \n",
    "print(\"\\n<3번째 청크 내용>\")\n",
    "print(\"-\" * 80)  # 구분선 추가\n",
    "print(sem_chunks[2].page_content[-500:])\n",
    "print(\"-\" * 80)  # 구분선 추가\n",
    "\n",
    "print(\"\\n<4번째 청크 내용>\")\n",
    "print(\"-\" * 80)  # 구분선 추가\n",
    "print(sem_chunks[3].page_content[:500])\n",
    "print(\"-\" * 80)  # 구분선 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
