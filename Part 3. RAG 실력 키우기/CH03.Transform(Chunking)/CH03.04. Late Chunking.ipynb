{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dd0543",
   "metadata": {},
   "source": [
    "## Late Chunking ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install einops -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a903b0e",
   "metadata": {},
   "source": [
    "ëª¨ë¸ ë¡œë“œ ì‹œì— ë¬¸ì œê°€ ìƒê¸°ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# cache_path = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "# if os.path.exists(cache_path):\n",
    "#     shutil.rmtree(cache_path)\n",
    "#     print(\"âœ… Hugging Face ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "# else:\n",
    "#     print(\"â„¹ï¸ ìºì‹œ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04324b71",
   "metadata": {},
   "source": [
    "### ğŸ”§ jina-embeddings-v2 ëª¨ë¸ë¡œ late-embedding í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fed07",
   "metadata": {},
   "source": [
    "#### ğŸ“ ë¬¸ì¥ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹ í•¨ìˆ˜ (Text Chunking by Sentences)\n",
    "ì´ í•¨ìˆ˜ëŠ” late chunkingìš© íŠ¹ìˆ˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¡œ, ì¼ë°˜ ìŠ¤í”Œë¦¬í„°ì™€ ë‹¬ë¦¬:\n",
    "\n",
    "1. í† í° ìœ„ì¹˜ ì¶”ì : ê° ë¬¸ì¥ ì²­í¬ê°€ ì›ë³¸ì˜ ì–´ë–¤ í† í°ì— í•´ë‹¹í•˜ëŠ”ì§€ ì •í™•íˆ ë§¤í•‘\n",
    "2. ëª¨ë¸ í† í¬ë‚˜ì´ì € í†µí•©: íŠ¹ì • ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ëŠ” ë°©ì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ë¶„í•  ì œê³µ\n",
    "3. ì •ë°€í•œ ë¬¸ì¥ ê²½ê³„: ë§ˆì¹¨í‘œ+ê³µë°± íŒ¨í„´ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af19515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        input_text: ë¶„í• í•  í…ìŠ¤íŠ¸\n",
    "        tokenizer: ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € ê°ì²´\n",
    "    \n",
    "    Returns:\n",
    "        chunks: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "        span_annotations: ê° ë¬¸ì¥ì˜ í† í° ìœ„ì¹˜ ì •ë³´ [(ì‹œì‘_í† í°_ì¸ë±ìŠ¤, ë_í† í°_ì¸ë±ìŠ¤), ...]\n",
    "    \n",
    "    Note:\n",
    "        ì˜¤í”„ì…‹(offset)ì€ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ê° í† í°ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸ë±ìŠ¤ ì •ë³´ì…ë‹ˆë‹¤.\n",
    "        ì˜ˆë¥¼ ë“¤ì–´, (5, 8)ì´ë¼ëŠ” ì˜¤í”„ì…‹ì€ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ 5ë²ˆì§¸ ë¬¸ìë¶€í„° 8ë²ˆì§¸ ë¬¸ìê¹Œì§€ê°€ í•´ë‹¹ í† í°ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "        ì´ë¥¼ í†µí•´ í† í°í™” í›„ì—ë„ ì›ë³¸ í…ìŠ¤íŠ¸ì™€ì˜ ë§¤í•‘ ê´€ê³„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ì²˜ë¦¬í•˜ì—¬ í…ì„œì™€ ì˜¤í”„ì…‹ ë§¤í•‘ ì •ë³´ ì–»ê¸°\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    # ë§ˆì¹¨í‘œ(.)ì™€ [SEP] í† í°ì˜ ID ê°€ì ¸ì˜¤ê¸°\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    # í† í°ì˜ ì˜¤í”„ì…‹ê³¼ ID ì •ë³´ ì¶”ì¶œ\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    \n",
    "    # ë¬¸ì¥ ë ìœ„ì¹˜ ì°¾ê¸° (ë§ˆì¹¨í‘œ ë‹¤ìŒì— ê³µë°±ì´ ìˆê±°ë‚˜ [SEP] í† í°ì´ ì˜¤ëŠ” ê²½ìš°)\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0  # ë§ˆì¹¨í‘œ ë‹¤ìŒì— ê³µë°±ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "            or token_ids[i + 1] == sep_id  # ë˜ëŠ” [SEP] í† í°ì´ ë‹¤ìŒì— ì˜¤ëŠ”ì§€ í™•ì¸\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # ì°¾ì€ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    \n",
    "    # ê° ì²­í¬ì˜ í† í° ìœ„ì¹˜ ì •ë³´ ì €ì¥\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    \n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = (\n",
    "    \"Berlin is the capital and largest city of Germany, both by area and by population. \"\n",
    "    \"Its more than 3.85 million inhabitants make it the European Union's most populous city, \"\n",
    "    \"as measured by population within city limits. The city is also one of the states of Germany, \"\n",
    "    \"and is the third smallest state in the country in terms of area.\"\n",
    ")\n",
    "\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcaff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks)\n",
    "print(span_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def late_chunking(model_output: 'BatchEncoding', span_annotation: list, max_length=None):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì¶œë ¥ì—ì„œ íŠ¹ì • ìŠ¤íŒ¬ì˜ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ì—¬ í‰ê·  í’€ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        model_output: ëª¨ë¸ì˜ ì¶œë ¥ (BatchEncoding ê°ì²´)\n",
    "        span_annotation: ê° ë¬¸ì¥ì— ëŒ€í•œ ìŠ¤íŒ¬(ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ë) ì •ë³´ ëª©ë¡ [(start, end), ...]\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ (ì„ íƒì )\n",
    "    \n",
    "    Returns:\n",
    "        outputs: ê° ìŠ¤íŒ¬ì— ëŒ€í•œ í‰ê·  ì„ë² ë”© ë²¡í„°ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0]  # (1, seq_len, hidden) í˜•íƒœì˜ í† í° ì„ë² ë”©\n",
    "    outputs = []\n",
    "    \n",
    "    # ê° ë°°ì¹˜ì˜ ì„ë² ë”©ê³¼ ìŠ¤íŒ¬ ì •ë³´ë¥¼ ìˆœíšŒ\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        # max_lengthê°€ ì§€ì •ëœ ê²½ìš° ìŠ¤íŒ¬ ë²”ìœ„ ì œí•œ\n",
    "        if max_length is not None:\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations if start < (max_length - 1)\n",
    "            ]\n",
    "        \n",
    "        # ê° ìŠ¤íŒ¬ì— ëŒ€í•´ í‰ê·  ì„ë² ë”© ê³„ì‚° (ìŠ¤íŒ¬ì˜ ê¸¸ì´ê°€ 1 ì´ìƒì¸ ê²½ìš°ë§Œ)\n",
    "        pooled = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations if (end - start) >= 1\n",
    "        ]\n",
    "        \n",
    "        # ê³„ì‚°ëœ ì„ë² ë”©ì„ CPU ë©”ëª¨ë¦¬ë¡œ ì´ë™í•˜ê³  ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜\n",
    "        outputs.append([p.detach().cpu().numpy() for p in pooled])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# ì „í†µì : ì²­í¬ ë‹¨ìœ„ë¡œ ì„ë² ë”©\n",
    "traditional_embeddings = model.encode(chunks)\n",
    "\n",
    "# Late Chunking: ì „ì²´ í…ìŠ¤íŠ¸ â†’ pooling\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**inputs)\n",
    "\n",
    "late_embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d530418",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(late_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213434e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Berlin\"\n",
    "query_vec = model.encode(query)\n",
    "\n",
    "print(f\"\\nğŸ” Query: '{query}'\")\n",
    "\n",
    "for chunk, emb_late, emb_trad in zip(chunks, late_embeddings, traditional_embeddings):\n",
    "    sim_late = cos_sim(torch.tensor(query_vec), torch.tensor(emb_late)).item()\n",
    "    sim_trad = cos_sim(torch.tensor(query_vec), torch.tensor(emb_trad)).item()\n",
    "    print(f\"\\nğŸ“Œ Chunk: {chunk.strip()}\\n- LateChunking ìœ ì‚¬ë„: {sim_late:.4f}\\n- ì „í†µ ë°©ì‹ ìœ ì‚¬ë„: {sim_trad:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606d025",
   "metadata": {},
   "source": [
    "### ğŸ§  PDF ë¬¸ì„œë¥¼ Late Chunking í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "v3_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfb340",
   "metadata": {},
   "source": [
    "#### ğŸ“„ STEP 1. PDF ë¬¸ì„œ ë¡œë”© ë° í…ìŠ¤íŠ¸ í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"./data/êµ­ê°€ë³„ ê³µê³µë¶€ë¬¸ AI ë„ì… ë° í™œìš© ì „ëµ.pdf\"  # â† ì‚¬ìš©ìì˜ PDF ê²½ë¡œ\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "full_text = \"\\n\".join([p.page_content for p in pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489293a2",
   "metadata": {},
   "source": [
    "#### âœ‚ï¸ STEP 2. ê¸´ ë¬¸ì„œ í† í° ê¸°ì¤€ ìŠ¬ë¼ì´ìŠ¤ (ëª¨ë¸ ì…ë ¥ ì´ˆê³¼ ë°©ì§€)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c771f",
   "metadata": {},
   "source": [
    "âœ… ëª¨ë¸ ì…ë ¥ ì œí•œ (8192 tokens) ê¸°ì¤€ìœ¼ë¡œ ìŠ¬ë¼ì´ìŠ¤ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(page_content=full_text)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    v3_tokenizer,\n",
    "    chunk_size=8192\n",
    ")\n",
    "split_docs = splitter.split_documents([doc])\n",
    "text_chunks = [d.page_content for d in split_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce9687",
   "metadata": {},
   "source": [
    "#### ğŸ§  STEP 3. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•œ ë’¤, ì„ë² ë”© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ê° ì²­í¬ì—ì„œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  ë° ì„ë² ë”© ì²˜ë¦¬\n",
    "all_sentences = []\n",
    "all_embeddings = []\n",
    "\n",
    "for chunk_text in text_chunks:\n",
    "    # ê° ì²­í¬ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "    sentences, span_annotations = chunk_by_sentences(chunk_text, v3_tokenizer)\n",
    "    \n",
    "    # í˜„ì¬ ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±\n",
    "    inputs = v3_tokenizer(chunk_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = v3_model(**inputs)\n",
    "    \n",
    "    # Late Chunkingìœ¼ë¡œ ë¬¸ì¥ë³„ ì„ë² ë”© ì¶”ì¶œ\n",
    "    chunk_embeddings = late_chunking(model_output, [span_annotations])[0]\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    all_sentences.extend(sentences)\n",
    "    all_embeddings.extend(chunk_embeddings)\n",
    "\n",
    "# ì´ì œ all_sentencesì™€ all_embeddingsì— ëª¨ë“  ë¬¸ì¥ê³¼ í•´ë‹¹ ì„ë² ë”©ì´ ì €ì¥ë¨\n",
    "print(f\"ì´ ë¬¸ì¥ ìˆ˜: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05165e1",
   "metadata": {},
   "source": [
    "#### ğŸ“Œ STEP 4. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ìƒì„± ë° ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "query = \"ì˜êµ­ ì¤‘ì•™ ë””ì§€í„¸ë°ì´í„°ì²­(CDDO)ì˜ ì—­í• ê³¼ ê·¸ ì „ëµì  íŒŒíŠ¸ë„ˆ ê¸°ê´€ì€ ëˆ„êµ¬ì´ë©°, í•¨ê»˜ ìˆ˜ë¦½í•œ ì „ëµì˜ ì£¼ìš” ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€?\"\n",
    "query_vec = v3_model.encode(query, task=\"retrieval.query\")\n",
    "\n",
    "print(f\"\\nğŸ” Query: '{query}'\")\n",
    "\n",
    "# ì²­í¬ì™€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "results = []\n",
    "\n",
    "for chunk, emb_late in zip(all_sentences, all_embeddings):\n",
    "    sim_late = cos_sim(torch.tensor(query_vec), torch.tensor(emb_late)).item()\n",
    "    results.append({\"chunk\": chunk.strip(), \"similarity\": sim_late})\n",
    "\n",
    "# ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "results = sorted(results, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "\n",
    "# ê°€ì¥ ì—°ê´€ì„± ë†’ì€ 3ê°œ ì²­í¬ë§Œ ì¶œë ¥\n",
    "print(\"\\nğŸ” ê°€ì¥ ì—°ê´€ì„± ë†’ì€ 3ê°œ ì²­í¬:\")\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f\"\\nğŸ“Œ Chunk {i+1}: {result['chunk']}\\n- ìœ ì‚¬ë„: {result['similarity']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
