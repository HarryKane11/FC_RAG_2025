{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dd0543",
   "metadata": {},
   "source": [
    "## Late Chunking 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install einops -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a903b0e",
   "metadata": {},
   "source": [
    "모델 로드 시에 문제가 생기면 아래 코드를 실행해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# cache_path = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "# if os.path.exists(cache_path):\n",
    "#     shutil.rmtree(cache_path)\n",
    "#     print(\"✅ Hugging Face 캐시가 삭제되었습니다.\")\n",
    "# else:\n",
    "#     print(\"ℹ️ 캐시 폴더가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04324b71",
   "metadata": {},
   "source": [
    "### 🔧 jina-embeddings-v2 모델로 late-embedding 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fed07",
   "metadata": {},
   "source": [
    "#### 📝 문장 기반 텍스트 청킹 함수 (Text Chunking by Sentences)\n",
    "이 함수는 late chunking용 특수 텍스트 분할기로, 일반 스플리터와 달리:\n",
    "\n",
    "1. 토큰 위치 추적: 각 문장 청크가 원본의 어떤 토큰에 해당하는지 정확히 매핑\n",
    "2. 모델 토크나이저 통합: 특정 모델이 텍스트를 이해하는 방식과 일치하는 분할 제공\n",
    "3. 정밀한 문장 경계: 마침표+공백 패턴으로 의미 단위 보존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af19515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer):\n",
    "    \"\"\"\n",
    "    입력 텍스트를 문장 단위로 분할하는 함수입니다.\n",
    "    \n",
    "    Args:\n",
    "        input_text: 분할할 텍스트\n",
    "        tokenizer: 사용할 토크나이저 객체\n",
    "    \n",
    "    Returns:\n",
    "        chunks: 문장 단위로 분할된 텍스트 리스트\n",
    "        span_annotations: 각 문장의 토큰 위치 정보 [(시작_토큰_인덱스, 끝_토큰_인덱스), ...]\n",
    "    \n",
    "    Note:\n",
    "        오프셋(offset)은 텍스트 내에서 각 토큰의 시작과 끝 위치를 나타내는 인덱스 정보입니다.\n",
    "        예를 들어, (5, 8)이라는 오프셋은 원본 텍스트의 5번째 문자부터 8번째 문자까지가 해당 토큰이라는 의미입니다.\n",
    "        이를 통해 토큰화 후에도 원본 텍스트와의 매핑 관계를 유지할 수 있습니다.\n",
    "    \"\"\"\n",
    "    # 입력 텍스트를 토크나이저로 처리하여 텐서와 오프셋 매핑 정보 얻기\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    # 마침표(.)와 [SEP] 토큰의 ID 가져오기\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    # 토큰의 오프셋과 ID 정보 추출\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    \n",
    "    # 문장 끝 위치 찾기 (마침표 다음에 공백이 있거나 [SEP] 토큰이 오는 경우)\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0  # 마침표 다음에 공백이 있는지 확인\n",
    "            or token_ids[i + 1] == sep_id  # 또는 [SEP] 토큰이 다음에 오는지 확인\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 찾은 위치를 기반으로 문장 단위로 텍스트 분할\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    \n",
    "    # 각 청크의 토큰 위치 정보 저장\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    \n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = (\n",
    "    \"Berlin is the capital and largest city of Germany, both by area and by population. \"\n",
    "    \"Its more than 3.85 million inhabitants make it the European Union's most populous city, \"\n",
    "    \"as measured by population within city limits. The city is also one of the states of Germany, \"\n",
    "    \"and is the third smallest state in the country in terms of area.\"\n",
    ")\n",
    "\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcaff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks)\n",
    "print(span_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def late_chunking(model_output: 'BatchEncoding', span_annotation: list, max_length=None):\n",
    "    \"\"\"\n",
    "    모델 출력에서 특정 스팬의 임베딩을 추출하여 평균 풀링을 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        model_output: 모델의 출력 (BatchEncoding 객체)\n",
    "        span_annotation: 각 문장에 대한 스팬(문장의 시작과 끝) 정보 목록 [(start, end), ...]\n",
    "        max_length: 최대 시퀀스 길이 제한 (선택적)\n",
    "    \n",
    "    Returns:\n",
    "        outputs: 각 스팬에 대한 평균 임베딩 벡터의 리스트\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0]  # (1, seq_len, hidden) 형태의 토큰 임베딩\n",
    "    outputs = []\n",
    "    \n",
    "    # 각 배치의 임베딩과 스팬 정보를 순회\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        # max_length가 지정된 경우 스팬 범위 제한\n",
    "        if max_length is not None:\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations if start < (max_length - 1)\n",
    "            ]\n",
    "        \n",
    "        # 각 스팬에 대해 평균 임베딩 계산 (스팬의 길이가 1 이상인 경우만)\n",
    "        pooled = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations if (end - start) >= 1\n",
    "        ]\n",
    "        \n",
    "        # 계산된 임베딩을 CPU 메모리로 이동하고 넘파이 배열로 변환\n",
    "        outputs.append([p.detach().cpu().numpy() for p in pooled])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# 전통적: 청크 단위로 임베딩\n",
    "traditional_embeddings = model.encode(chunks)\n",
    "\n",
    "# Late Chunking: 전체 텍스트 → pooling\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**inputs)\n",
    "\n",
    "late_embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d530418",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(late_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213434e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Berlin\"\n",
    "query_vec = model.encode(query)\n",
    "\n",
    "print(f\"\\n🔍 Query: '{query}'\")\n",
    "\n",
    "for chunk, emb_late, emb_trad in zip(chunks, late_embeddings, traditional_embeddings):\n",
    "    sim_late = cos_sim(torch.tensor(query_vec), torch.tensor(emb_late)).item()\n",
    "    sim_trad = cos_sim(torch.tensor(query_vec), torch.tensor(emb_trad)).item()\n",
    "    print(f\"\\n📌 Chunk: {chunk.strip()}\\n- LateChunking 유사도: {sim_late:.4f}\\n- 전통 방식 유사도: {sim_trad:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606d025",
   "metadata": {},
   "source": [
    "### 🧠 PDF 문서를 Late Chunking 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "v3_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfb340",
   "metadata": {},
   "source": [
    "#### 📄 STEP 1. PDF 문서 로딩 및 텍스트 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"./data/국가별 공공부문 AI 도입 및 활용 전략.pdf\"  # ← 사용자의 PDF 경로\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "full_text = \"\\n\".join([p.page_content for p in pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489293a2",
   "metadata": {},
   "source": [
    "#### ✂️ STEP 2. 긴 문서 토큰 기준 슬라이스 (모델 입력 초과 방지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c771f",
   "metadata": {},
   "source": [
    "✅ 모델 입력 제한 (8192 tokens) 기준으로 슬라이스 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(page_content=full_text)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    v3_tokenizer,\n",
    "    chunk_size=8192\n",
    ")\n",
    "split_docs = splitter.split_documents([doc])\n",
    "text_chunks = [d.page_content for d in split_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce9687",
   "metadata": {},
   "source": [
    "#### 🧠 STEP 3. 문장 단위로 분할한 뒤, 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 각 청크에서 문장 단위 분할 및 임베딩 처리\n",
    "all_sentences = []\n",
    "all_embeddings = []\n",
    "\n",
    "for chunk_text in text_chunks:\n",
    "    # 각 청크를 문장 단위로 분할\n",
    "    sentences, span_annotations = chunk_by_sentences(chunk_text, v3_tokenizer)\n",
    "    \n",
    "    # 현재 청크에 대한 임베딩 생성\n",
    "    inputs = v3_tokenizer(chunk_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = v3_model(**inputs)\n",
    "    \n",
    "    # Late Chunking으로 문장별 임베딩 추출\n",
    "    chunk_embeddings = late_chunking(model_output, [span_annotations])[0]\n",
    "    \n",
    "    # 결과 저장\n",
    "    all_sentences.extend(sentences)\n",
    "    all_embeddings.extend(chunk_embeddings)\n",
    "\n",
    "# 이제 all_sentences와 all_embeddings에 모든 문장과 해당 임베딩이 저장됨\n",
    "print(f\"총 문장 수: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05165e1",
   "metadata": {},
   "source": [
    "#### 📌 STEP 4. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성 및 유사도 검색\n",
    "query = \"영국 중앙 디지털데이터청(CDDO)의 역할과 그 전략적 파트너 기관은 누구이며, 함께 수립한 전략의 주요 목표는 무엇인가?\"\n",
    "query_vec = v3_model.encode(query, task=\"retrieval.query\")\n",
    "\n",
    "print(f\"\\n🔍 Query: '{query}'\")\n",
    "\n",
    "# 청크와 유사도 점수를 저장할 리스트\n",
    "results = []\n",
    "\n",
    "for chunk, emb_late in zip(all_sentences, all_embeddings):\n",
    "    sim_late = cos_sim(torch.tensor(query_vec), torch.tensor(emb_late)).item()\n",
    "    results.append({\"chunk\": chunk.strip(), \"similarity\": sim_late})\n",
    "\n",
    "# 유사도 기준으로 내림차순 정렬\n",
    "results = sorted(results, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "\n",
    "# 가장 연관성 높은 3개 청크만 출력\n",
    "print(\"\\n🔝 가장 연관성 높은 3개 청크:\")\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f\"\\n📌 Chunk {i+1}: {result['chunk']}\\n- 유사도: {result['similarity']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
