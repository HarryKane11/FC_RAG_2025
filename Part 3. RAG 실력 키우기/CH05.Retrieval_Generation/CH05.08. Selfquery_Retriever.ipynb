{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Self-Query Retriever 튜토리얼\n",
    "\n",
    "이 노트북에서는 LangChain의 Self-Query Retriever를 사용하여 자연어 질의를 통해 메타데이터 기반 검색을 수행하는 방법을 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "1. Self-Query Retriever의 개념과 동작 원리 이해\n",
    "2. 메타데이터 태깅을 통한 문서 분류 방법 학습\n",
    "3. 자연어 질의를 메타데이터 필터와 검색 쿼리로 분해하는 과정 이해\n",
    "4. 실제 문서 데이터를 활용한 Self-Query 구현 실습\n",
    "\n",
    "## 목차\n",
    "1. 데이터 수집 및 전처리\n",
    "2. 메타데이터 태깅\n",
    "3. 벡터 스토어 구성\n",
    "4. Self-Query Retriever 설정\n",
    "5. 자연어 질의 실습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. 데이터 수집 및 전처리\n",
    "\n",
    "먼저 arXiv에서 AI 관련 논문들을 수집하여 테스트 데이터로 활용하겠습니다.\n",
    "\n",
    "### 1.1 필요한 라이브러리 import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 URL: http://export.arxiv.org/api/query?search_query=cat:cs.AI&start=0&max_results=1\n",
      "\n",
      "=== arXiv API 응답 (첫 200자) ===\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=cat:cs.AI&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/HQttLwSI9ar2vqfmj4lJ7u3JUas</id>\n",
      "  <updated>2025-06-10T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http:...\n",
      "\n",
      "응답 크기: 2331 문자\n"
     ]
    }
   ],
   "source": [
    "# arXiv API 기본 사용법 1: 단일 논문 검색 및 응답 구조 확인\n",
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "query = 'search_query=cat:cs.AI&start=0&max_results=1'\n",
    "url = base_url + query\n",
    "\n",
    "print(f\"요청 URL: {url}\")\n",
    "print(\"\\n=== arXiv API 응답 (첫 200자) ===\")\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(response.text[:500] + \"...\")\n",
    "    print(f\"\\n응답 크기: {len(response.text)} 문자\")\n",
    "else:\n",
    "    print(f\"API 요청 실패: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=cat:cs.AI&amp;id_list=&amp;start=0&amp;max_results=1</title>\\n  <id>http://arxiv.org/api/HQttLwSI9ar2vqfmj4lJ7u3JUas</id>\\n  <updated>2025-06-10T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">130079</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/9308101v1</id>\\n    <updated>1993-08-01T00:00:00Z</updated>\\n    <published>1993-08-01T00:00:00Z</published>\\n    <title>Dynamic Backtracking</title>\\n    <summary>  Because of their occasional need to return to shallow points in a search\\ntree, existing backtracking methods can sometimes erase meaningful progress\\ntoward solving a search problem. In this paper, we present a method by which\\nbacktrack points can be moved deeper in the search space, thereby avoiding this\\ndifficulty. The technique developed is a variant of dependency-directed\\nbacktracking that uses only polynomial space while still providing useful\\ncontrol information and retaining the completeness guarantees provided by\\nearlier approaches.\\n</summary>\\n    <author>\\n      <name>M. L. Ginsberg</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">See http://www.jair.org/ for an online appendix and other files\\n  accompanying this article</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Artificial Intelligence Research, Vol 1, (1993), 25-46</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/9308101v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/9308101v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XML 구조 분석 ===\n",
      "루트 태그: {http://www.w3.org/2005/Atom}feed\n",
      "총 entry 개수: 1\n",
      "\n",
      "=== 첫 번째 논문 정보 ===\n",
      "제목: Dynamic Backtracking\n",
      "저자: M. L. Ginsberg...\n",
      "게시일: 1993-08-01T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "# arXiv API 기본 사용법 2: XML 파싱 및 논문 정보 추출\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# 위에서 받은 응답을 XML로 파싱\n",
    "try:\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    # arXiv API XML 네임스페이스 정의\n",
    "    ns = {\n",
    "        'atom': 'http://www.w3.org/2005/Atom',\n",
    "        'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "    }\n",
    "    \n",
    "    print(\"=== XML 구조 분석 ===\")\n",
    "    print(f\"루트 태그: {root.tag}\")\n",
    "    print(f\"총 entry 개수: {len(root.findall('atom:entry', ns))}\")\n",
    "    \n",
    "    # 첫 번째 논문 정보 추출\n",
    "    first_entry = root.find('atom:entry', ns)\n",
    "    if first_entry is not None:\n",
    "        title = first_entry.find('atom:title', ns).text.strip()\n",
    "        authors = [author.find('atom:name', ns).text for author in first_entry.findall('atom:author', ns)]\n",
    "        published = first_entry.find('atom:published', ns).text\n",
    "        \n",
    "        print(f\"\\n=== 첫 번째 논문 정보 ===\")\n",
    "        print(f\"제목: {title}\")\n",
    "        print(f\"저자: {', '.join(authors[:3])}...\")  # 처음 3명만 표시\n",
    "        print(f\"게시일: {published}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"XML 파싱 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 대상 날짜: 2025-06-09\n",
      "=== 카테고리별 최신 논문 검색 ===\n",
      "\n",
      "📚 cs.AI 카테고리:\n",
      "  - StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning   from Pa...\n",
      "  - Vision Transformers Don't Need Trained Registers...\n",
      "  📊 최근 2일 내 논문 수: 5개\n",
      "\n",
      "📚 cs.LG 카테고리:\n",
      "  - StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning   from Pa...\n",
      "  - Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion...\n",
      "  📊 최근 2일 내 논문 수: 5개\n",
      "\n",
      "📚 cs.CL 카테고리:\n",
      "  - Play to Generalize: Learning to Reason Through Game Play...\n",
      "  - Reinforcement Pre-Training...\n",
      "  📊 최근 2일 내 논문 수: 5개\n",
      "\n",
      "✅ 기본 arXiv API 사용법 완료!\n"
     ]
    }
   ],
   "source": [
    "# arXiv API 기본 사용법 3: 여러 카테고리에서 최신 논문 검색\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 여러 AI 카테고리 정의\n",
    "ai_categories = ['cs.AI', 'cs.LG', 'cs.CL']\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "print(f\"검색 대상 날짜: {yesterday.date()}\")\n",
    "print(\"=== 카테고리별 최신 논문 검색 ===\")\n",
    "\n",
    "for category in ai_categories:\n",
    "    print(f\"\\n📚 {category} 카테고리:\")\n",
    "    \n",
    "    # 각 카테고리별 최신 논문 5개 요청\n",
    "    query_url = f'http://export.arxiv.org/api/query?search_query=cat:{category}&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(query_url)\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        ns = {\n",
    "            'atom': 'http://www.w3.org/2005/Atom',\n",
    "            'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "        }\n",
    "        \n",
    "        entries = root.findall('atom:entry', ns)\n",
    "        recent_count = 0\n",
    "        \n",
    "        for entry in entries:\n",
    "            published = entry.find('atom:published', ns).text\n",
    "            pub_date = datetime.strptime(published[:10], '%Y-%m-%d')\n",
    "            \n",
    "            # 최근 2일 이내 논문만 카운트\n",
    "            if (datetime.now() - pub_date).days <= 2:\n",
    "                recent_count += 1\n",
    "                if recent_count <= 2:  # 처음 2개만 제목 표시\n",
    "                    title = entry.find('atom:title', ns).text.strip().replace('\\n', ' ')\n",
    "                    print(f\"  - {title[:80]}...\")\n",
    "        \n",
    "        print(f\"  📊 최근 2일 내 논문 수: {recent_count}개\")\n",
    "        \n",
    "        time.sleep(0.5)  # API 호출 간격 조절\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ {category} 검색 오류: {e}\")\n",
    "\n",
    "print(f\"\\n✅ 기본 arXiv API 사용법 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.2 arXiv 논문 수집 함수 정의\n",
    "\n",
    "arXiv API를 사용하여 AI 관련 논문들을 수집하는 함수를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yesterday_arxiv_ai_papers():\n",
    "    \"\"\"어제 올라온 arXiv AI 관련 논문들을 가져오는 함수\"\"\"\n",
    "    yesterday = datetime.now() - timedelta(days=1)\n",
    "    \n",
    "    # AI 관련 카테고리들\n",
    "    ai_categories = [\n",
    "        'cs.AI',  # Artificial Intelligence\n",
    "        'cs.LG',  # Machine Learning\n",
    "        'cs.CL',  # Computation and Language (NLP)\n",
    "        'cs.CV',  # Computer Vision\n",
    "        'cs.NE',  # Neural and Evolutionary Computing\n",
    "        'stat.ML'  # Machine Learning (Statistics)\n",
    "    ]\n",
    "    \n",
    "    all_papers = []\n",
    "    \n",
    "    for category in ai_categories:\n",
    "        print(f\"Fetching papers from {category}...\")\n",
    "        \n",
    "        # arXiv API 쿼리 구성\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        query = f'search_query=cat:{category}&start=0&max_results=100&sortBy=submittedDate&sortOrder=descending'\n",
    "        url = base_url + query\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # XML 파싱\n",
    "            root = ET.fromstring(response.content)\n",
    "            \n",
    "            # 네임스페이스 정의\n",
    "            ns = {\n",
    "                'atom': 'http://www.w3.org/2005/Atom',\n",
    "                'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "            }\n",
    "            \n",
    "            # 각 논문 정보 추출\n",
    "            for entry in root.findall('atom:entry', ns):\n",
    "                published = entry.find('atom:published', ns).text\n",
    "                pub_date = datetime.strptime(published[:10], '%Y-%m-%d')\n",
    "                \n",
    "                # 어제 올라온 논문만 필터링\n",
    "                if pub_date.date() == yesterday.date():\n",
    "                    paper_info = {\n",
    "                        'title': entry.find('atom:title', ns).text.strip().replace('\\\\n', ' '),\n",
    "                        'authors': [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)],\n",
    "                        'summary': entry.find('atom:summary', ns).text.strip().replace('\\\\n', ' '),\n",
    "                        'published': published,\n",
    "                        'id': entry.find('atom:id', ns).text,\n",
    "                        'category': category,\n",
    "                        'link': entry.find('atom:id', ns).text.replace('http://arxiv.org/abs/', 'https://arxiv.org/abs/')\n",
    "                    }\n",
    "                    \n",
    "                    primary_category = entry.find('arxiv:primary_category', ns)\n",
    "                    if primary_category is not None:\n",
    "                        paper_info['primary_category'] = primary_category.get('term')\n",
    "                    \n",
    "                    all_papers.append(paper_info)\n",
    "            \n",
    "            time.sleep(1)  # API 호출 간격 조절\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {category}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.3 Document 변환 함수 정의\n",
    "\n",
    "수집한 논문 데이터를 LangChain Document 객체로 변환하는 함수를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_papers_documents(papers):\n",
    "    \"\"\"논문 리스트를 langchain Document 객체 리스트로 변환\"\"\"\n",
    "    if not papers:\n",
    "        print(\"No papers found for yesterday.\")\n",
    "        return []\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for paper in papers:\n",
    "        # 저자 리스트를 문자열로 변환\n",
    "        authors_str = ', '.join(paper['authors'])\n",
    "        \n",
    "        # 메타데이터 설정 (초록을 제외한 모든 정보)\n",
    "        metadata = {\n",
    "            'title': paper['title'],\n",
    "            'authors': authors_str,\n",
    "            'published': paper['published'],\n",
    "            'id': paper['id'],\n",
    "            'category': paper['category'],\n",
    "            'primary_category': paper.get('primary_category', 'N/A'),\n",
    "            'link': paper['link']\n",
    "        }\n",
    "        \n",
    "        # Document 객체 생성 (page_content는 초록만)\n",
    "        doc = Document(\n",
    "            page_content=paper['summary'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        \n",
    "        documents.append(doc)\n",
    "    \n",
    "    # 중복 제거 (같은 논문이 여러 카테고리에 속할 수 있음)\n",
    "    unique_documents = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['id'] not in seen_ids:\n",
    "            unique_documents.append(doc)\n",
    "            seen_ids.add(doc.metadata['id'])\n",
    "    \n",
    "    return unique_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.4 논문 데이터 수집 실행\n",
    "\n",
    "위에서 정의한 함수들을 사용하여 실제로 논문 데이터를 수집합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers from cs.AI...\n",
      "Fetching papers from cs.LG...\n",
      "Fetching papers from cs.CL...\n",
      "Fetching papers from cs.CV...\n",
      "Fetching papers from cs.NE...\n",
      "Fetching papers from stat.ML...\n",
      "✅ 258개의 논문이 Document 형태로 변환되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 논문 데이터 수집 및 Document 변환\n",
    "papers = get_yesterday_arxiv_ai_papers()\n",
    "documents = create_papers_documents(papers)\n",
    "print(f\"✅ {len(documents)}개의 논문이 Document 형태로 변환되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.5 첫 번째 논문 확인\n",
    "\n",
    "수집된 논문 중 첫 번째 논문의 내용을 확인해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 논문: StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning\n",
      "  from Partially Annotated Sy...\n",
      "저자: ['Anh-Quan Cao', 'Ivan Lopes', 'Raoul de Charette']\n",
      "카테고리: cs.AI\n",
      "초록 일부: Multi-task learning for dense prediction is limited by the need for extensive\n",
      "annotation for every task, though recent works have explored training with\n",
      "partial task labels. Leveraging the generalizat...\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 논문 확인\n",
    "if papers:\n",
    "    print(f\"첫 번째 논문: {papers[0]['title'][:100]}...\")\n",
    "    print(f\"저자: {papers[0]['authors'][:3]}\")  # 처음 3명의 저자만 표시\n",
    "    print(f\"카테고리: {papers[0]['category']}\")\n",
    "    print(f\"초록 일부: {papers[0]['summary'][:200]}...\")\n",
    "else:\n",
    "    print(\"수집된 논문이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Qdrant 벡터 스토어 설정 및 문서 저장\n",
    "\n",
    "수집한 arXiv 논문들을 Qdrant 벡터 스토어에 저장하여 Self-Query Retriever에서 사용할 수 있도록 준비합니다.\n",
    "\n",
    "### 2.1 필요한 라이브러리 import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Qdrant 클라이언트 설정 및 컬렉션 생성\n",
    "\n",
    "Qdrant 서버에 연결하고 논문 데이터를 저장할 컬렉션을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'arxiv_papers' 컬렉션이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Qdrant 클라이언트 설정\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# 컬렉션 생성 (이미 존재하는 경우 에러 무시)\n",
    "try:\n",
    "    client.create_collection(\n",
    "        collection_name=\"arxiv_papers_0610\",\n",
    "        vectors_config={\"dense\": VectorParams(size=1024, distance=Distance.COSINE)}\n",
    "    )\n",
    "    print(\"✅ 'arxiv_papers' 컬렉션이 생성되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"컬렉션이 이미 존재하거나 생성 중 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.3 벡터 스토어 초기화 및 문서 저장\n",
    "\n",
    "LangChain의 QdrantVectorStore를 초기화하고 논문 문서들을 임베딩하여 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 논문 문서 임베딩 및 저장 중...\n",
      "✅ 258개 논문 문서가 벡터 저장소에 추가되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# LangChain QdrantVectorStore 초기화\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"arxiv_papers_0610\",\n",
    "    embedding=OllamaEmbeddings(model=\"bge-m3\"),\n",
    "    vector_name=\"dense\"\n",
    ")\n",
    "\n",
    "print(\"🔄 논문 문서 임베딩 및 저장 중...\")\n",
    "vectorstore.add_documents(documents)\n",
    "print(f\"✅ {len(documents)}개 논문 문서가 벡터 저장소에 추가되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.4 저장된 데이터 확인\n",
    "\n",
    "벡터 스토어에 저장된 논문 데이터를 간단한 검색으로 확인해봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 테스트 결과:\n",
      "\n",
      "1. 제목: $τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control\n",
      "  Environment\n",
      "   카테고리: cs.AI\n",
      "   초록 일부: Existing benchmarks for conversational AI agents simulate single-control\n",
      "environments, where only the AI agent can use tools to interact with the worl...\n",
      "\n",
      "2. 제목: IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents\n",
      "   카테고리: cs.AI\n",
      "   초록 일부: LLM agents are increasingly deployed to automate real-world tasks by invoking\n",
      "APIs through natural language instructions. While powerful, they often s...\n",
      "\n",
      "3. 제목: MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid\n",
      "  Computer Use Agents\n",
      "   카테고리: cs.AI\n",
      "   초록 일부: (M)LLM-powered computer use agents (CUA) are emerging as a transformative\n",
      "technique to automate human-computer interaction. However, existing CUA\n",
      "benc...\n"
     ]
    }
   ],
   "source": [
    "# 간단한 유사도 검색 테스트\n",
    "test_results = vectorstore.similarity_search(\"AI 에이전트\", k=3)\n",
    "\n",
    "print(\"검색 테스트 결과:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n{i}. 제목: {doc.metadata['title']}\")\n",
    "    print(f\"   카테고리: {doc.metadata['category']}\")\n",
    "    print(f\"   초록 일부: {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Self-Query Retriever 설정\n",
    "\n",
    "이제 Self-Query Retriever를 설정하여 자연어 질의를 통해 메타데이터 기반 검색을 수행할 수 있도록 합니다.\n",
    "\n",
    "### 3.1 메타데이터 필드 정보 정의\n",
    "\n",
    "Self-Query Retriever가 이해할 수 있도록 논문 메타데이터의 각 필드에 대한 정보를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 메타데이터 필드 정보가 정의되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "# 메타데이터 필드 정보 정의\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"논문의 제목\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"authors\",\n",
    "        description=\"논문의 저자들 (쉼표로 구분된 문자열)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"논문의 arXiv 카테고리 (예: cs.AI, cs.LG, cs.CL, cs.CV, cs.NE, stat.ML)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"primary_category\",\n",
    "        description=\"논문의 주요 카테고리\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"published\",\n",
    "        description=\"논문의 게시 날짜 (ISO 형식)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"id\",\n",
    "        description=\"논문의 arXiv ID\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 문서 내용에 대한 설명\n",
    "document_content_description = \"arXiv AI 관련 논문의 초록 (abstract)\"\n",
    "\n",
    "print(\"✅ 메타데이터 필드 정보가 정의되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3.2 LLM 모델 설정 및 Self-Query Retriever 생성\n",
    "\n",
    "LLM을 설정하고 Self-Query Retriever를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Self-Query Retriever가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# LLM 모델 설정 (Ollama 사용)\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "qwen= ChatOllama(\n",
    "    model=\"qwen3\",\n",
    "    temperature=0\n",
    ")\n",
    "# Self-Query Retriever 생성\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=qwen,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    enable_limit=True,  # 결과 수 제한 기능 활성화\n",
    "    verbose=True  # 디버깅을 위한 상세 출력\n",
    ")\n",
    "\n",
    "print(\"✅ Self-Query Retriever가 생성되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Self-Query Retriever 실습\n",
    "\n",
    "이제 Self-Query Retriever를 사용하여 다양한 자연어 질의를 수행해보겠습니다.\n",
    "\n",
    "### 4.1 기본 검색 - 카테고리 기반 필터링\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질의: 컴퓨터 비전 카테고리에서 이미지 분할에 관한 논문을 찾아주세요\n",
      "==================================================\n",
      "검색 결과: 5개 논문\n",
      "\n",
      "1. 제목: Text-guided multi-stage cross-perception network for medical image\n",
      "  segmentation\n",
      "   카테고리: cs.CV (주요: eess.IV)\n",
      "   저자: Gaoyu Chen...\n",
      "   초록: Medical image segmentation plays a crucial role in clinical medicine, serving\n",
      "as a tool for auxiliary diagnosis, treatment planning, and disease monitoring,\n",
      "thus facilitating physicians in the study a...\n",
      "\n",
      "2. 제목: Cross-channel Perception Learning for H&E-to-IHC Virtual Staining\n",
      "   카테고리: cs.CV (주요: cs.CV)\n",
      "   저자: Hao Yang, JianYu Wu, Run Fang, Xuelian Zhao, Yuan Ji, Zhiyu Chen, Guibin He, Junceng Guo, Yang Liu, ...\n",
      "   초록: With the rapid development of digital pathology, virtual staining has become\n",
      "a key technology in multimedia medical information systems, offering new\n",
      "possibilities for the analysis and diagnosis of pa...\n",
      "\n",
      "3. 제목: Adaptive Blind Super-Resolution Network for Spatial-Specific and\n",
      "  Spatial-Agnostic Degradations\n",
      "   카테고리: cs.CV (주요: cs.CV)\n",
      "   저자: Weilei Wen, Chunle Guo, Wenqi Ren, Hongpeng Wang, Xiuli Shao...\n",
      "   초록: Prior methodologies have disregarded the diversities among distinct\n",
      "degradation types during image reconstruction, employing a uniform network\n",
      "model to handle multiple deteriorations. Nevertheless, we...\n",
      "\n",
      "4. 제목: Difference Inversion: Interpolate and Isolate the Difference with Token\n",
      "  Consistency for Image Analogy Generation\n",
      "   카테고리: cs.CV (주요: cs.CV)\n",
      "   저자: Hyunsoo Kim, Donghyun Kim, Suhyun Kim...\n",
      "   초록: How can we generate an image B' that satisfies A:A'::B:B', given the input\n",
      "images A,A' and B? Recent works have tackled this challenge through approaches\n",
      "like visual in-context learning or visual inst...\n",
      "\n",
      "5. 제목: PIG: Physically-based Multi-Material Interaction with 3D Gaussians\n",
      "   카테고리: cs.CV (주요: cs.GR)\n",
      "   저자: Zeyu Xiao, Zhenyi Wu, Mingyang Sun, Qipeng Yan, Yufan Guo, Zhuoer Liang, Lihua Zhang...\n",
      "   초록: 3D Gaussian Splatting has achieved remarkable success in reconstructing both\n",
      "static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\n",
      "primitives, interactions between objects suffe...\n"
     ]
    }
   ],
   "source": [
    "# 컴퓨터 비전 카테고리의 논문 검색\n",
    "qwen_think=\"/no_think\"\n",
    "original_query = \"컴퓨터 비전 카테고리에서 이미지 분할에 관한 논문을 찾아주세요\"\n",
    "query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "print(f\"질의: {original_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"검색 결과: {len(results)}개 논문\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. 제목: {doc.metadata['title']}\")\n",
    "    print(f\"   카테고리: {doc.metadata['category']} (주요: {doc.metadata['primary_category']})\")\n",
    "    print(f\"   저자: {doc.metadata['authors'][:100]}...\")\n",
    "    print(f\"   초록: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.2 복합 조건 검색 - 저자와 주제 조합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질의: 자연어 처리 카테고리에서 transformer나 attention에 관한 논문을 찾아주세요\n",
      "==================================================\n",
      "검색 결과: 4개 논문\n",
      "\n",
      "1. 제목: Quantum Graph Transformer for NLP Sentiment Classification\n",
      "   카테고리: cs.CL (주요: cs.CL)\n",
      "   저자: Shamminuj Aktar, Andreas Bärtschi, Abdel-Hameed A. Badawy, Stephan Eidenbenz...\n",
      "   초록: Quantum machine learning is a promising direction for building more efficient\n",
      "and expressive models, particularly in domains where understanding complex,\n",
      "structured data is critical. We present the Qu...\n",
      "\n",
      "2. 제목: Learning to Focus: Causal Attention Distillation via Gradient-Guided\n",
      "  Token Pruning\n",
      "   카테고리: cs.CL (주요: cs.CL)\n",
      "   저자: Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, Yankai Lin...\n",
      "   초록: Large language models (LLMs) have demonstrated significant improvements in\n",
      "contextual understanding. However, their ability to attend to truly critical\n",
      "information during long-context reasoning and ge...\n",
      "\n",
      "3. 제목: Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical\n",
      "  Intent Annotation\n",
      "   카테고리: cs.CL (주요: cs.CL)\n",
      "   저자: Kseniia Petukhova, Ekaterina Kochmar...\n",
      "   초록: Large language models (LLMs) hold great promise for educational applications,\n",
      "particularly in intelligent tutoring systems. However, effective tutoring\n",
      "requires alignment with pedagogical strategies -...\n",
      "\n",
      "4. 제목: MiniCPM4: Ultra-Efficient LLMs on End Devices\n",
      "   카테고리: cs.AI (주요: cs.CL)\n",
      "   저자:  MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin...\n",
      "   초록: This paper introduces MiniCPM4, a highly efficient large language model (LLM)\n",
      "designed explicitly for end-side devices. We achieve this efficiency through\n",
      "systematic innovation in four key dimensions:...\n"
     ]
    }
   ],
   "source": [
    "# 특정 저자의 논문 중 딥러닝 관련 연구 검색\n",
    "qwen_think=\"no_think\"\n",
    "original_query = \"자연어 처리 카테고리에서 transformer나 attention에 관한 논문을 찾아주세요\"\n",
    "query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "\n",
    "print(f\"질의: {original_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"검색 결과: {len(results)}개 논문\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. 제목: {doc.metadata['title']}\")\n",
    "    print(f\"   카테고리: {doc.metadata['category']} (주요: {doc.metadata['primary_category']})\")\n",
    "    print(f\"   저자: {doc.metadata['authors'][:100]}...\")\n",
    "    print(f\"   초록: {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.3 결과 수 제한 검색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질의: AI 카테고리에서 강화학습에 관한 논문 3개만 찾아주세요\n",
      "==================================================\n",
      "검색 결과: 3개 논문\n",
      "\n",
      "1. 제목: Curriculum Learning With Counterfactual Group Relative Policy Advantage\n",
      "  For Multi-Agent Reinforcement Learning\n",
      "   카테고리: cs.AI (주요: cs.AI)\n",
      "   저자: Weiqiang Jin, Hongyang Du, Guizhong Liu, Dong In Kim...\n",
      "   초록: Multi-agent reinforcement learning (MARL) has achieved strong performance in\n",
      "cooperative adversarial tasks. However, most existing methods typically train\n",
      "agents against fixed opponent strategies and ...\n",
      "\n",
      "2. 제목: Learning What Reinforcement Learning Can't: Interleaved Online\n",
      "  Fine-Tuning for Hardest Questions\n",
      "   카테고리: cs.AI (주요: cs.AI)\n",
      "   저자: Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Ru...\n",
      "   초록: Recent advances in large language model (LLM) reasoning have shown that\n",
      "sophisticated behaviors such as planning and self-reflection can emerge through\n",
      "reinforcement learning (RL). However, despite th...\n",
      "\n",
      "3. 제목: REMoH: A Reflective Evolution of Multi-objective Heuristics approach via\n",
      "  Large Language Models\n",
      "   카테고리: cs.AI (주요: cs.AI)\n",
      "   저자: Diego Forniés-Tabuenca, Alejandro Uribe, Urtzi Otamendi, Arkaitz Artetxe, Juan Carlos Rivera, Oier L...\n",
      "   초록: Multi-objective optimization is fundamental in complex decision-making tasks.\n",
      "Traditional algorithms, while effective, often demand extensive\n",
      "problem-specific modeling and struggle to adapt to nonline...\n"
     ]
    }
   ],
   "source": [
    "# 결과 수 제한과 함께 검색\n",
    "qwen_think=\"/no_think\"\n",
    "original_query = \"AI 카테고리에서 강화학습에 관한 논문 3개만 찾아주세요\"\n",
    "query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "\n",
    "print(f\"질의: {original_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"검색 결과: {len(results)}개 논문\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. 제목: {doc.metadata['title']}\")\n",
    "    print(f\"   카테고리: {doc.metadata['category']} (주요: {doc.metadata['primary_category']})\")\n",
    "    print(f\"   저자: {doc.metadata['authors'][:100]}...\")\n",
    "    print(f\"   초록: {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.4 다양한 질의 유형 실험\n",
    "\n",
    "다른 형태의 자연어 질의들을 시도해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 질의: 인공지능 카테고리의 논문들 중에서 robot이나 robotics가 포함된 것들을 찾아주세요\n",
      "--------------------------------------------------------------------------------\n",
      "검색 중 오류 발생: Parsing text\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"query\": \"robot\",\n",
      "    \"filter\": \"and(like(\\\"primary_category\\\", \\\"cs.AI\\\"), or(like(\\\"title\\\", \\\"robot\\\"), like(\\\"title\\\", \\\"robotics\\\")), like(\\\"authors\\\", \\\"robot\\\"), like(\\\"authors\\\", \\\"robotics\\\")))\",\n",
      "    \"limit\": 0\n",
      "}\n",
      "```\n",
      " raised following error:\n",
      "Unexpected token Token('RPAR', ')') at line 1, column 149.\n",
      "Expected one of: \n",
      "\t* $END\n",
      "\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "🔍 질의: generative model이나 diffusion에 관한 연구를 찾아주세요\n",
      "--------------------------------------------------------------------------------\n",
      "검색 결과: 4개 논문\n",
      "\n",
      "1. 제목: FunDiff: Diffusion Models over Function Spaces for Physics-Informed\n",
      "  Generative Modeling\n",
      "   카테고리: cs.LG\n",
      "   초록: Recent advances in generative modeling -- particularly diffusion models and\n",
      "flow matching -- have achieved remarkable success in synthesizing discrete...\n",
      "\n",
      "2. 제목: Explore the vulnerability of black-box models via diffusion models\n",
      "   카테고리: cs.LG\n",
      "   초록: Recent advancements in diffusion models have enabled high-fidelity and\n",
      "photorealistic image generation across diverse applications. However, these\n",
      "mod...\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "🔍 질의: cs.CV 카테고리에서 object detection 관련 논문들을 보여주세요\n",
      "--------------------------------------------------------------------------------\n",
      "검색 결과: 10개 논문\n",
      "\n",
      "1. 제목: SAM2Auto: Auto Annotation Using FLASH\n",
      "   카테고리: cs.CV\n",
      "   초록: Vision-Language Models (VLMs) lag behind Large Language Models due to the\n",
      "scarcity of annotated datasets, as creating paired visual-textual annotation...\n",
      "\n",
      "2. 제목: CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian\n",
      "  Crosswalk Detection in Aerial Images with High-Performance Computing\n",
      "   카테고리: cs.CV\n",
      "   초록: With the increasing availability of aerial and satellite imagery, deep\n",
      "learning presents significant potential for transportation asset management,\n",
      "sa...\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 여러 가지 질의 유형 테스트\n",
    "test_queries = [\n",
    "    \"인공지능 카테고리의 논문들 중에서 robot이나 robotics가 포함된 것들을 찾아주세요\",\n",
    "    \"generative model이나 diffusion에 관한 연구를 찾아주세요\",\n",
    "    \"cs.CV 카테고리에서 object detection 관련 논문들을 보여주세요\"\n",
    "]\n",
    "\n",
    "for original_query in test_queries:\n",
    "    qwen_think=\"/no_think\"\n",
    "    query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "    \n",
    "    print(f\"\\n🔍 질의: {original_query}\")\n",
    "    print(\"-\" * 80)\n",
    "    try:\n",
    "        results = retriever.invoke(query)\n",
    "        print(f\"검색 결과: {len(results)}개 논문\")\n",
    "        \n",
    "        for i, doc in enumerate(results[:2], 1):  # 각 질의당 최대 2개 결과만 표시\n",
    "            print(f\"\\n{i}. 제목: {doc.metadata['title']}\")\n",
    "            print(f\"   카테고리: {doc.metadata['category']}\")\n",
    "            print(f\"   초록: {doc.page_content[:150]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"검색 중 오류 발생: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
