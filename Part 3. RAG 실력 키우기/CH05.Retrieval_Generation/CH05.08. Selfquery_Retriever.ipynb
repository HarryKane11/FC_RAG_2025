{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Self-Query Retriever íŠœí† ë¦¬ì–¼\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” LangChainì˜ Self-Query Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì§ˆì˜ë¥¼ í†µí•´ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. Self-Query Retrieverì˜ ê°œë…ê³¼ ë™ì‘ ì›ë¦¬ ì´í•´\n",
    "2. ë©”íƒ€ë°ì´í„° íƒœê¹…ì„ í†µí•œ ë¬¸ì„œ ë¶„ë¥˜ ë°©ë²• í•™ìŠµ\n",
    "3. ìì—°ì–´ ì§ˆì˜ë¥¼ ë©”íƒ€ë°ì´í„° í•„í„°ì™€ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë¶„í•´í•˜ëŠ” ê³¼ì • ì´í•´\n",
    "4. ì‹¤ì œ ë¬¸ì„œ ë°ì´í„°ë¥¼ í™œìš©í•œ Self-Query êµ¬í˜„ ì‹¤ìŠµ\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬\n",
    "2. ë©”íƒ€ë°ì´í„° íƒœê¹…\n",
    "3. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±\n",
    "4. Self-Query Retriever ì„¤ì •\n",
    "5. ìì—°ì–´ ì§ˆì˜ ì‹¤ìŠµ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "ë¨¼ì € arXivì—ì„œ AI ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í™œìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìš”ì²­ URL: http://export.arxiv.org/api/query?search_query=cat:cs.AI&start=0&max_results=1\n",
      "\n",
      "=== arXiv API ì‘ë‹µ (ì²« 200ì) ===\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=cat:cs.AI&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/HQttLwSI9ar2vqfmj4lJ7u3JUas</id>\n",
      "  <updated>2025-06-10T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http:...\n",
      "\n",
      "ì‘ë‹µ í¬ê¸°: 2331 ë¬¸ì\n"
     ]
    }
   ],
   "source": [
    "# arXiv API ê¸°ë³¸ ì‚¬ìš©ë²• 1: ë‹¨ì¼ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì‘ë‹µ êµ¬ì¡° í™•ì¸\n",
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "query = 'search_query=cat:cs.AI&start=0&max_results=1'\n",
    "url = base_url + query\n",
    "\n",
    "print(f\"ìš”ì²­ URL: {url}\")\n",
    "print(\"\\n=== arXiv API ì‘ë‹µ (ì²« 200ì) ===\")\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(response.text[:500] + \"...\")\n",
    "    print(f\"\\nì‘ë‹µ í¬ê¸°: {len(response.text)} ë¬¸ì\")\n",
    "else:\n",
    "    print(f\"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=cat:cs.AI&amp;id_list=&amp;start=0&amp;max_results=1</title>\\n  <id>http://arxiv.org/api/HQttLwSI9ar2vqfmj4lJ7u3JUas</id>\\n  <updated>2025-06-10T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">130079</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/9308101v1</id>\\n    <updated>1993-08-01T00:00:00Z</updated>\\n    <published>1993-08-01T00:00:00Z</published>\\n    <title>Dynamic Backtracking</title>\\n    <summary>  Because of their occasional need to return to shallow points in a search\\ntree, existing backtracking methods can sometimes erase meaningful progress\\ntoward solving a search problem. In this paper, we present a method by which\\nbacktrack points can be moved deeper in the search space, thereby avoiding this\\ndifficulty. The technique developed is a variant of dependency-directed\\nbacktracking that uses only polynomial space while still providing useful\\ncontrol information and retaining the completeness guarantees provided by\\nearlier approaches.\\n</summary>\\n    <author>\\n      <name>M. L. Ginsberg</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">See http://www.jair.org/ for an online appendix and other files\\n  accompanying this article</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Artificial Intelligence Research, Vol 1, (1993), 25-46</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/9308101v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/9308101v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XML êµ¬ì¡° ë¶„ì„ ===\n",
      "ë£¨íŠ¸ íƒœê·¸: {http://www.w3.org/2005/Atom}feed\n",
      "ì´ entry ê°œìˆ˜: 1\n",
      "\n",
      "=== ì²« ë²ˆì§¸ ë…¼ë¬¸ ì •ë³´ ===\n",
      "ì œëª©: Dynamic Backtracking\n",
      "ì €ì: M. L. Ginsberg...\n",
      "ê²Œì‹œì¼: 1993-08-01T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "# arXiv API ê¸°ë³¸ ì‚¬ìš©ë²• 2: XML íŒŒì‹± ë° ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# ìœ„ì—ì„œ ë°›ì€ ì‘ë‹µì„ XMLë¡œ íŒŒì‹±\n",
    "try:\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    # arXiv API XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜\n",
    "    ns = {\n",
    "        'atom': 'http://www.w3.org/2005/Atom',\n",
    "        'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "    }\n",
    "    \n",
    "    print(\"=== XML êµ¬ì¡° ë¶„ì„ ===\")\n",
    "    print(f\"ë£¨íŠ¸ íƒœê·¸: {root.tag}\")\n",
    "    print(f\"ì´ entry ê°œìˆ˜: {len(root.findall('atom:entry', ns))}\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ\n",
    "    first_entry = root.find('atom:entry', ns)\n",
    "    if first_entry is not None:\n",
    "        title = first_entry.find('atom:title', ns).text.strip()\n",
    "        authors = [author.find('atom:name', ns).text for author in first_entry.findall('atom:author', ns)]\n",
    "        published = first_entry.find('atom:published', ns).text\n",
    "        \n",
    "        print(f\"\\n=== ì²« ë²ˆì§¸ ë…¼ë¬¸ ì •ë³´ ===\")\n",
    "        print(f\"ì œëª©: {title}\")\n",
    "        print(f\"ì €ì: {', '.join(authors[:3])}...\")  # ì²˜ìŒ 3ëª…ë§Œ í‘œì‹œ\n",
    "        print(f\"ê²Œì‹œì¼: {published}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"XML íŒŒì‹± ì˜¤ë¥˜: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ìƒ‰ ëŒ€ìƒ ë‚ ì§œ: 2025-06-09\n",
      "=== ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰ ===\n",
      "\n",
      "ğŸ“š cs.AI ì¹´í…Œê³ ë¦¬:\n",
      "  - StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning   from Pa...\n",
      "  - Vision Transformers Don't Need Trained Registers...\n",
      "  ğŸ“Š ìµœê·¼ 2ì¼ ë‚´ ë…¼ë¬¸ ìˆ˜: 5ê°œ\n",
      "\n",
      "ğŸ“š cs.LG ì¹´í…Œê³ ë¦¬:\n",
      "  - StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning   from Pa...\n",
      "  - Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion...\n",
      "  ğŸ“Š ìµœê·¼ 2ì¼ ë‚´ ë…¼ë¬¸ ìˆ˜: 5ê°œ\n",
      "\n",
      "ğŸ“š cs.CL ì¹´í…Œê³ ë¦¬:\n",
      "  - Play to Generalize: Learning to Reason Through Game Play...\n",
      "  - Reinforcement Pre-Training...\n",
      "  ğŸ“Š ìµœê·¼ 2ì¼ ë‚´ ë…¼ë¬¸ ìˆ˜: 5ê°œ\n",
      "\n",
      "âœ… ê¸°ë³¸ arXiv API ì‚¬ìš©ë²• ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# arXiv API ê¸°ë³¸ ì‚¬ìš©ë²• 3: ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ì—¬ëŸ¬ AI ì¹´í…Œê³ ë¦¬ ì •ì˜\n",
    "ai_categories = ['cs.AI', 'cs.LG', 'cs.CL']\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ëŒ€ìƒ ë‚ ì§œ: {yesterday.date()}\")\n",
    "print(\"=== ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰ ===\")\n",
    "\n",
    "for category in ai_categories:\n",
    "    print(f\"\\nğŸ“š {category} ì¹´í…Œê³ ë¦¬:\")\n",
    "    \n",
    "    # ê° ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë…¼ë¬¸ 5ê°œ ìš”ì²­\n",
    "    query_url = f'http://export.arxiv.org/api/query?search_query=cat:{category}&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(query_url)\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        ns = {\n",
    "            'atom': 'http://www.w3.org/2005/Atom',\n",
    "            'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "        }\n",
    "        \n",
    "        entries = root.findall('atom:entry', ns)\n",
    "        recent_count = 0\n",
    "        \n",
    "        for entry in entries:\n",
    "            published = entry.find('atom:published', ns).text\n",
    "            pub_date = datetime.strptime(published[:10], '%Y-%m-%d')\n",
    "            \n",
    "            # ìµœê·¼ 2ì¼ ì´ë‚´ ë…¼ë¬¸ë§Œ ì¹´ìš´íŠ¸\n",
    "            if (datetime.now() - pub_date).days <= 2:\n",
    "                recent_count += 1\n",
    "                if recent_count <= 2:  # ì²˜ìŒ 2ê°œë§Œ ì œëª© í‘œì‹œ\n",
    "                    title = entry.find('atom:title', ns).text.strip().replace('\\n', ' ')\n",
    "                    print(f\"  - {title[:80]}...\")\n",
    "        \n",
    "        print(f\"  ğŸ“Š ìµœê·¼ 2ì¼ ë‚´ ë…¼ë¬¸ ìˆ˜: {recent_count}ê°œ\")\n",
    "        \n",
    "        time.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {category} ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… ê¸°ë³¸ arXiv API ì‚¬ìš©ë²• ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.2 arXiv ë…¼ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ AI ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yesterday_arxiv_ai_papers():\n",
    "    \"\"\"ì–´ì œ ì˜¬ë¼ì˜¨ arXiv AI ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    yesterday = datetime.now() - timedelta(days=1)\n",
    "    \n",
    "    # AI ê´€ë ¨ ì¹´í…Œê³ ë¦¬ë“¤\n",
    "    ai_categories = [\n",
    "        'cs.AI',  # Artificial Intelligence\n",
    "        'cs.LG',  # Machine Learning\n",
    "        'cs.CL',  # Computation and Language (NLP)\n",
    "        'cs.CV',  # Computer Vision\n",
    "        'cs.NE',  # Neural and Evolutionary Computing\n",
    "        'stat.ML'  # Machine Learning (Statistics)\n",
    "    ]\n",
    "    \n",
    "    all_papers = []\n",
    "    \n",
    "    for category in ai_categories:\n",
    "        print(f\"Fetching papers from {category}...\")\n",
    "        \n",
    "        # arXiv API ì¿¼ë¦¬ êµ¬ì„±\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        query = f'search_query=cat:{category}&start=0&max_results=100&sortBy=submittedDate&sortOrder=descending'\n",
    "        url = base_url + query\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # XML íŒŒì‹±\n",
    "            root = ET.fromstring(response.content)\n",
    "            \n",
    "            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜\n",
    "            ns = {\n",
    "                'atom': 'http://www.w3.org/2005/Atom',\n",
    "                'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "            }\n",
    "            \n",
    "            # ê° ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ\n",
    "            for entry in root.findall('atom:entry', ns):\n",
    "                published = entry.find('atom:published', ns).text\n",
    "                pub_date = datetime.strptime(published[:10], '%Y-%m-%d')\n",
    "                \n",
    "                # ì–´ì œ ì˜¬ë¼ì˜¨ ë…¼ë¬¸ë§Œ í•„í„°ë§\n",
    "                if pub_date.date() == yesterday.date():\n",
    "                    paper_info = {\n",
    "                        'title': entry.find('atom:title', ns).text.strip().replace('\\\\n', ' '),\n",
    "                        'authors': [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)],\n",
    "                        'summary': entry.find('atom:summary', ns).text.strip().replace('\\\\n', ' '),\n",
    "                        'published': published,\n",
    "                        'id': entry.find('atom:id', ns).text,\n",
    "                        'category': category,\n",
    "                        'link': entry.find('atom:id', ns).text.replace('http://arxiv.org/abs/', 'https://arxiv.org/abs/')\n",
    "                    }\n",
    "                    \n",
    "                    primary_category = entry.find('arxiv:primary_category', ns)\n",
    "                    if primary_category is not None:\n",
    "                        paper_info['primary_category'] = primary_category.get('term')\n",
    "                    \n",
    "                    all_papers.append(paper_info)\n",
    "            \n",
    "            time.sleep(1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {category}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.3 Document ë³€í™˜ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "ìˆ˜ì§‘í•œ ë…¼ë¬¸ ë°ì´í„°ë¥¼ LangChain Document ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_papers_documents(papers):\n",
    "    \"\"\"ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ langchain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    if not papers:\n",
    "        print(\"No papers found for yesterday.\")\n",
    "        return []\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for paper in papers:\n",
    "        # ì €ì ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        authors_str = ', '.join(paper['authors'])\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì„¤ì • (ì´ˆë¡ì„ ì œì™¸í•œ ëª¨ë“  ì •ë³´)\n",
    "        metadata = {\n",
    "            'title': paper['title'],\n",
    "            'authors': authors_str,\n",
    "            'published': paper['published'],\n",
    "            'id': paper['id'],\n",
    "            'category': paper['category'],\n",
    "            'primary_category': paper.get('primary_category', 'N/A'),\n",
    "            'link': paper['link']\n",
    "        }\n",
    "        \n",
    "        # Document ê°ì²´ ìƒì„± (page_contentëŠ” ì´ˆë¡ë§Œ)\n",
    "        doc = Document(\n",
    "            page_content=paper['summary'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        \n",
    "        documents.append(doc)\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±° (ê°™ì€ ë…¼ë¬¸ì´ ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— ì†í•  ìˆ˜ ìˆìŒ)\n",
    "    unique_documents = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['id'] not in seen_ids:\n",
    "            unique_documents.append(doc)\n",
    "            seen_ids.add(doc.metadata['id'])\n",
    "    \n",
    "    return unique_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.4 ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰\n",
    "\n",
    "ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œë¡œ ë…¼ë¬¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers from cs.AI...\n",
      "Fetching papers from cs.LG...\n",
      "Fetching papers from cs.CL...\n",
      "Fetching papers from cs.CV...\n",
      "Fetching papers from cs.NE...\n",
      "Fetching papers from stat.ML...\n",
      "âœ… 258ê°œì˜ ë…¼ë¬¸ì´ Document í˜•íƒœë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ ë° Document ë³€í™˜\n",
    "papers = get_yesterday_arxiv_ai_papers()\n",
    "documents = create_papers_documents(papers)\n",
    "print(f\"âœ… {len(documents)}ê°œì˜ ë…¼ë¬¸ì´ Document í˜•íƒœë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.5 ì²« ë²ˆì§¸ ë…¼ë¬¸ í™•ì¸\n",
    "\n",
    "ìˆ˜ì§‘ëœ ë…¼ë¬¸ ì¤‘ ì²« ë²ˆì§¸ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ ë…¼ë¬¸: StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning\n",
      "  from Partially Annotated Sy...\n",
      "ì €ì: ['Anh-Quan Cao', 'Ivan Lopes', 'Raoul de Charette']\n",
      "ì¹´í…Œê³ ë¦¬: cs.AI\n",
      "ì´ˆë¡ ì¼ë¶€: Multi-task learning for dense prediction is limited by the need for extensive\n",
      "annotation for every task, though recent works have explored training with\n",
      "partial task labels. Leveraging the generalizat...\n"
     ]
    }
   ],
   "source": [
    "# ì²« ë²ˆì§¸ ë…¼ë¬¸ í™•ì¸\n",
    "if papers:\n",
    "    print(f\"ì²« ë²ˆì§¸ ë…¼ë¬¸: {papers[0]['title'][:100]}...\")\n",
    "    print(f\"ì €ì: {papers[0]['authors'][:3]}\")  # ì²˜ìŒ 3ëª…ì˜ ì €ìë§Œ í‘œì‹œ\n",
    "    print(f\"ì¹´í…Œê³ ë¦¬: {papers[0]['category']}\")\n",
    "    print(f\"ì´ˆë¡ ì¼ë¶€: {papers[0]['summary'][:200]}...\")\n",
    "else:\n",
    "    print(\"ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Qdrant ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ë° ë¬¸ì„œ ì €ì¥\n",
    "\n",
    "ìˆ˜ì§‘í•œ arXiv ë…¼ë¬¸ë“¤ì„ Qdrant ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•˜ì—¬ Self-Query Retrieverì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 2.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Qdrant í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ë° ì»¬ë ‰ì…˜ ìƒì„±\n",
    "\n",
    "Qdrant ì„œë²„ì— ì—°ê²°í•˜ê³  ë…¼ë¬¸ ë°ì´í„°ë¥¼ ì €ì¥í•  ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'arxiv_papers' ì»¬ë ‰ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Qdrant í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# ì»¬ë ‰ì…˜ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì—ëŸ¬ ë¬´ì‹œ)\n",
    "try:\n",
    "    client.create_collection(\n",
    "        collection_name=\"arxiv_papers_0610\",\n",
    "        vectors_config={\"dense\": VectorParams(size=1024, distance=Distance.COSINE)}\n",
    "    )\n",
    "    print(\"âœ… 'arxiv_papers' ì»¬ë ‰ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.3 ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì €ì¥\n",
    "\n",
    "LangChainì˜ QdrantVectorStoreë¥¼ ì´ˆê¸°í™”í•˜ê³  ë…¼ë¬¸ ë¬¸ì„œë“¤ì„ ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ë…¼ë¬¸ ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥ ì¤‘...\n",
      "âœ… 258ê°œ ë…¼ë¬¸ ë¬¸ì„œê°€ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# LangChain QdrantVectorStore ì´ˆê¸°í™”\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"arxiv_papers_0610\",\n",
    "    embedding=OllamaEmbeddings(model=\"bge-m3\"),\n",
    "    vector_name=\"dense\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ ë…¼ë¬¸ ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥ ì¤‘...\")\n",
    "vectorstore.add_documents(documents)\n",
    "print(f\"âœ… {len(documents)}ê°œ ë…¼ë¬¸ ë¬¸ì„œê°€ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.4 ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "\n",
    "ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ëœ ë…¼ë¬¸ ë°ì´í„°ë¥¼ ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ í™•ì¸í•´ë´…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n",
      "\n",
      "1. ì œëª©: $Ï„^2$-Bench: Evaluating Conversational Agents in a Dual-Control\n",
      "  Environment\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI\n",
      "   ì´ˆë¡ ì¼ë¶€: Existing benchmarks for conversational AI agents simulate single-control\n",
      "environments, where only the AI agent can use tools to interact with the worl...\n",
      "\n",
      "2. ì œëª©: IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI\n",
      "   ì´ˆë¡ ì¼ë¶€: LLM agents are increasingly deployed to automate real-world tasks by invoking\n",
      "APIs through natural language instructions. While powerful, they often s...\n",
      "\n",
      "3. ì œëª©: MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid\n",
      "  Computer Use Agents\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI\n",
      "   ì´ˆë¡ ì¼ë¶€: (M)LLM-powered computer use agents (CUA) are emerging as a transformative\n",
      "technique to automate human-computer interaction. However, existing CUA\n",
      "benc...\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "test_results = vectorstore.similarity_search(\"AI ì—ì´ì „íŠ¸\", k=3)\n",
    "\n",
    "print(\"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n{i}. ì œëª©: {doc.metadata['title']}\")\n",
    "    print(f\"   ì¹´í…Œê³ ë¦¬: {doc.metadata['category']}\")\n",
    "    print(f\"   ì´ˆë¡ ì¼ë¶€: {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Self-Query Retriever ì„¤ì •\n",
    "\n",
    "ì´ì œ Self-Query Retrieverë¥¼ ì„¤ì •í•˜ì—¬ ìì—°ì–´ ì§ˆì˜ë¥¼ í†µí•´ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 3.1 ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ë³´ ì •ì˜\n",
    "\n",
    "Self-Query Retrieverê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ì˜ ê° í•„ë“œì— ëŒ€í•œ ì •ë³´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ë³´ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "# ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ë³´ ì •ì˜\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"ë…¼ë¬¸ì˜ ì œëª©\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"authors\",\n",
    "        description=\"ë…¼ë¬¸ì˜ ì €ìë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"ë…¼ë¬¸ì˜ arXiv ì¹´í…Œê³ ë¦¬ (ì˜ˆ: cs.AI, cs.LG, cs.CL, cs.CV, cs.NE, stat.ML)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"primary_category\",\n",
    "        description=\"ë…¼ë¬¸ì˜ ì£¼ìš” ì¹´í…Œê³ ë¦¬\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"published\",\n",
    "        description=\"ë…¼ë¬¸ì˜ ê²Œì‹œ ë‚ ì§œ (ISO í˜•ì‹)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"id\",\n",
    "        description=\"ë…¼ë¬¸ì˜ arXiv ID\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ì„¤ëª…\n",
    "document_content_description = \"arXiv AI ê´€ë ¨ ë…¼ë¬¸ì˜ ì´ˆë¡ (abstract)\"\n",
    "\n",
    "print(\"âœ… ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ë³´ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3.2 LLM ëª¨ë¸ ì„¤ì • ë° Self-Query Retriever ìƒì„±\n",
    "\n",
    "LLMì„ ì„¤ì •í•˜ê³  Self-Query Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Self-Query Retrieverê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# LLM ëª¨ë¸ ì„¤ì • (Ollama ì‚¬ìš©)\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "qwen= ChatOllama(\n",
    "    model=\"qwen3\",\n",
    "    temperature=0\n",
    ")\n",
    "# Self-Query Retriever ìƒì„±\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=qwen,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    enable_limit=True,  # ê²°ê³¼ ìˆ˜ ì œí•œ ê¸°ëŠ¥ í™œì„±í™”\n",
    "    verbose=True  # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì¶œë ¥\n",
    ")\n",
    "\n",
    "print(\"âœ… Self-Query Retrieverê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Self-Query Retriever ì‹¤ìŠµ\n",
    "\n",
    "ì´ì œ Self-Query Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìì—°ì–´ ì§ˆì˜ë¥¼ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 4.1 ê¸°ë³¸ ê²€ìƒ‰ - ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í•„í„°ë§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆì˜: ì»´í“¨í„° ë¹„ì „ ì¹´í…Œê³ ë¦¬ì—ì„œ ì´ë¯¸ì§€ ë¶„í• ì— ê´€í•œ ë…¼ë¬¸ì„ ì°¾ì•„ì£¼ì„¸ìš”\n",
      "==================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼: 5ê°œ ë…¼ë¬¸\n",
      "\n",
      "1. ì œëª©: Text-guided multi-stage cross-perception network for medical image\n",
      "  segmentation\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV (ì£¼ìš”: eess.IV)\n",
      "   ì €ì: Gaoyu Chen...\n",
      "   ì´ˆë¡: Medical image segmentation plays a crucial role in clinical medicine, serving\n",
      "as a tool for auxiliary diagnosis, treatment planning, and disease monitoring,\n",
      "thus facilitating physicians in the study a...\n",
      "\n",
      "2. ì œëª©: Cross-channel Perception Learning for H&E-to-IHC Virtual Staining\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV (ì£¼ìš”: cs.CV)\n",
      "   ì €ì: Hao Yang, JianYu Wu, Run Fang, Xuelian Zhao, Yuan Ji, Zhiyu Chen, Guibin He, Junceng Guo, Yang Liu, ...\n",
      "   ì´ˆë¡: With the rapid development of digital pathology, virtual staining has become\n",
      "a key technology in multimedia medical information systems, offering new\n",
      "possibilities for the analysis and diagnosis of pa...\n",
      "\n",
      "3. ì œëª©: Adaptive Blind Super-Resolution Network for Spatial-Specific and\n",
      "  Spatial-Agnostic Degradations\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV (ì£¼ìš”: cs.CV)\n",
      "   ì €ì: Weilei Wen, Chunle Guo, Wenqi Ren, Hongpeng Wang, Xiuli Shao...\n",
      "   ì´ˆë¡: Prior methodologies have disregarded the diversities among distinct\n",
      "degradation types during image reconstruction, employing a uniform network\n",
      "model to handle multiple deteriorations. Nevertheless, we...\n",
      "\n",
      "4. ì œëª©: Difference Inversion: Interpolate and Isolate the Difference with Token\n",
      "  Consistency for Image Analogy Generation\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV (ì£¼ìš”: cs.CV)\n",
      "   ì €ì: Hyunsoo Kim, Donghyun Kim, Suhyun Kim...\n",
      "   ì´ˆë¡: How can we generate an image B' that satisfies A:A'::B:B', given the input\n",
      "images A,A' and B? Recent works have tackled this challenge through approaches\n",
      "like visual in-context learning or visual inst...\n",
      "\n",
      "5. ì œëª©: PIG: Physically-based Multi-Material Interaction with 3D Gaussians\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV (ì£¼ìš”: cs.GR)\n",
      "   ì €ì: Zeyu Xiao, Zhenyi Wu, Mingyang Sun, Qipeng Yan, Yufan Guo, Zhuoer Liang, Lihua Zhang...\n",
      "   ì´ˆë¡: 3D Gaussian Splatting has achieved remarkable success in reconstructing both\n",
      "static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\n",
      "primitives, interactions between objects suffe...\n"
     ]
    }
   ],
   "source": [
    "# ì»´í“¨í„° ë¹„ì „ ì¹´í…Œê³ ë¦¬ì˜ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "qwen_think=\"/no_think\"\n",
    "original_query = \"ì»´í“¨í„° ë¹„ì „ ì¹´í…Œê³ ë¦¬ì—ì„œ ì´ë¯¸ì§€ ë¶„í• ì— ê´€í•œ ë…¼ë¬¸ì„ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "print(f\"ì§ˆì˜: {original_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. ì œëª©: {doc.metadata['title']}\")\n",
    "    print(f\"   ì¹´í…Œê³ ë¦¬: {doc.metadata['category']} (ì£¼ìš”: {doc.metadata['primary_category']})\")\n",
    "    print(f\"   ì €ì: {doc.metadata['authors'][:100]}...\")\n",
    "    print(f\"   ì´ˆë¡: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.2 ë³µí•© ì¡°ê±´ ê²€ìƒ‰ - ì €ìì™€ ì£¼ì œ ì¡°í•©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆì˜: ìì—°ì–´ ì²˜ë¦¬ ì¹´í…Œê³ ë¦¬ì—ì„œ transformerë‚˜ attentionì— ê´€í•œ ë…¼ë¬¸ì„ ì°¾ì•„ì£¼ì„¸ìš”\n",
      "==================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼: 4ê°œ ë…¼ë¬¸\n",
      "\n",
      "1. ì œëª©: Quantum Graph Transformer for NLP Sentiment Classification\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CL (ì£¼ìš”: cs.CL)\n",
      "   ì €ì: Shamminuj Aktar, Andreas BÃ¤rtschi, Abdel-Hameed A. Badawy, Stephan Eidenbenz...\n",
      "   ì´ˆë¡: Quantum machine learning is a promising direction for building more efficient\n",
      "and expressive models, particularly in domains where understanding complex,\n",
      "structured data is critical. We present the Qu...\n",
      "\n",
      "2. ì œëª©: Learning to Focus: Causal Attention Distillation via Gradient-Guided\n",
      "  Token Pruning\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CL (ì£¼ìš”: cs.CL)\n",
      "   ì €ì: Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, Yankai Lin...\n",
      "   ì´ˆë¡: Large language models (LLMs) have demonstrated significant improvements in\n",
      "contextual understanding. However, their ability to attend to truly critical\n",
      "information during long-context reasoning and ge...\n",
      "\n",
      "3. ì œëª©: Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical\n",
      "  Intent Annotation\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CL (ì£¼ìš”: cs.CL)\n",
      "   ì €ì: Kseniia Petukhova, Ekaterina Kochmar...\n",
      "   ì´ˆë¡: Large language models (LLMs) hold great promise for educational applications,\n",
      "particularly in intelligent tutoring systems. However, effective tutoring\n",
      "requires alignment with pedagogical strategies -...\n",
      "\n",
      "4. ì œëª©: MiniCPM4: Ultra-Efficient LLMs on End Devices\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI (ì£¼ìš”: cs.CL)\n",
      "   ì €ì:  MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin...\n",
      "   ì´ˆë¡: This paper introduces MiniCPM4, a highly efficient large language model (LLM)\n",
      "designed explicitly for end-side devices. We achieve this efficiency through\n",
      "systematic innovation in four key dimensions:...\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ì • ì €ìì˜ ë…¼ë¬¸ ì¤‘ ë”¥ëŸ¬ë‹ ê´€ë ¨ ì—°êµ¬ ê²€ìƒ‰\n",
    "qwen_think=\"no_think\"\n",
    "original_query = \"ìì—°ì–´ ì²˜ë¦¬ ì¹´í…Œê³ ë¦¬ì—ì„œ transformerë‚˜ attentionì— ê´€í•œ ë…¼ë¬¸ì„ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "\n",
    "print(f\"ì§ˆì˜: {original_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. ì œëª©: {doc.metadata['title']}\")\n",
    "    print(f\"   ì¹´í…Œê³ ë¦¬: {doc.metadata['category']} (ì£¼ìš”: {doc.metadata['primary_category']})\")\n",
    "    print(f\"   ì €ì: {doc.metadata['authors'][:100]}...\")\n",
    "    print(f\"   ì´ˆë¡: {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.3 ê²°ê³¼ ìˆ˜ ì œí•œ ê²€ìƒ‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆì˜: AI ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•í™”í•™ìŠµì— ê´€í•œ ë…¼ë¬¸ 3ê°œë§Œ ì°¾ì•„ì£¼ì„¸ìš”\n",
      "==================================================\n",
      "ê²€ìƒ‰ ê²°ê³¼: 3ê°œ ë…¼ë¬¸\n",
      "\n",
      "1. ì œëª©: Curriculum Learning With Counterfactual Group Relative Policy Advantage\n",
      "  For Multi-Agent Reinforcement Learning\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI (ì£¼ìš”: cs.AI)\n",
      "   ì €ì: Weiqiang Jin, Hongyang Du, Guizhong Liu, Dong In Kim...\n",
      "   ì´ˆë¡: Multi-agent reinforcement learning (MARL) has achieved strong performance in\n",
      "cooperative adversarial tasks. However, most existing methods typically train\n",
      "agents against fixed opponent strategies and ...\n",
      "\n",
      "2. ì œëª©: Learning What Reinforcement Learning Can't: Interleaved Online\n",
      "  Fine-Tuning for Hardest Questions\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI (ì£¼ìš”: cs.AI)\n",
      "   ì €ì: Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Ru...\n",
      "   ì´ˆë¡: Recent advances in large language model (LLM) reasoning have shown that\n",
      "sophisticated behaviors such as planning and self-reflection can emerge through\n",
      "reinforcement learning (RL). However, despite th...\n",
      "\n",
      "3. ì œëª©: REMoH: A Reflective Evolution of Multi-objective Heuristics approach via\n",
      "  Large Language Models\n",
      "   ì¹´í…Œê³ ë¦¬: cs.AI (ì£¼ìš”: cs.AI)\n",
      "   ì €ì: Diego ForniÃ©s-Tabuenca, Alejandro Uribe, Urtzi Otamendi, Arkaitz Artetxe, Juan Carlos Rivera, Oier L...\n",
      "   ì´ˆë¡: Multi-objective optimization is fundamental in complex decision-making tasks.\n",
      "Traditional algorithms, while effective, often demand extensive\n",
      "problem-specific modeling and struggle to adapt to nonline...\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ìˆ˜ ì œí•œê³¼ í•¨ê»˜ ê²€ìƒ‰\n",
    "qwen_think=\"/no_think\"\n",
    "original_query = \"AI ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•í™”í•™ìŠµì— ê´€í•œ ë…¼ë¬¸ 3ê°œë§Œ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "\n",
    "print(f\"ì§ˆì˜: {original_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. ì œëª©: {doc.metadata['title']}\")\n",
    "    print(f\"   ì¹´í…Œê³ ë¦¬: {doc.metadata['category']} (ì£¼ìš”: {doc.metadata['primary_category']})\")\n",
    "    print(f\"   ì €ì: {doc.metadata['authors'][:100]}...\")\n",
    "    print(f\"   ì´ˆë¡: {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.4 ë‹¤ì–‘í•œ ì§ˆì˜ ìœ í˜• ì‹¤í—˜\n",
    "\n",
    "ë‹¤ë¥¸ í˜•íƒœì˜ ìì—°ì–´ ì§ˆì˜ë“¤ì„ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ì§ˆì˜: ì¸ê³µì§€ëŠ¥ ì¹´í…Œê³ ë¦¬ì˜ ë…¼ë¬¸ë“¤ ì¤‘ì—ì„œ robotì´ë‚˜ roboticsê°€ í¬í•¨ëœ ê²ƒë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”\n",
      "--------------------------------------------------------------------------------\n",
      "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: Parsing text\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"query\": \"robot\",\n",
      "    \"filter\": \"and(like(\\\"primary_category\\\", \\\"cs.AI\\\"), or(like(\\\"title\\\", \\\"robot\\\"), like(\\\"title\\\", \\\"robotics\\\")), like(\\\"authors\\\", \\\"robot\\\"), like(\\\"authors\\\", \\\"robotics\\\")))\",\n",
      "    \"limit\": 0\n",
      "}\n",
      "```\n",
      " raised following error:\n",
      "Unexpected token Token('RPAR', ')') at line 1, column 149.\n",
      "Expected one of: \n",
      "\t* $END\n",
      "\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ” ì§ˆì˜: generative modelì´ë‚˜ diffusionì— ê´€í•œ ì—°êµ¬ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”\n",
      "--------------------------------------------------------------------------------\n",
      "ê²€ìƒ‰ ê²°ê³¼: 4ê°œ ë…¼ë¬¸\n",
      "\n",
      "1. ì œëª©: FunDiff: Diffusion Models over Function Spaces for Physics-Informed\n",
      "  Generative Modeling\n",
      "   ì¹´í…Œê³ ë¦¬: cs.LG\n",
      "   ì´ˆë¡: Recent advances in generative modeling -- particularly diffusion models and\n",
      "flow matching -- have achieved remarkable success in synthesizing discrete...\n",
      "\n",
      "2. ì œëª©: Explore the vulnerability of black-box models via diffusion models\n",
      "   ì¹´í…Œê³ ë¦¬: cs.LG\n",
      "   ì´ˆë¡: Recent advancements in diffusion models have enabled high-fidelity and\n",
      "photorealistic image generation across diverse applications. However, these\n",
      "mod...\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ” ì§ˆì˜: cs.CV ì¹´í…Œê³ ë¦¬ì—ì„œ object detection ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë³´ì—¬ì£¼ì„¸ìš”\n",
      "--------------------------------------------------------------------------------\n",
      "ê²€ìƒ‰ ê²°ê³¼: 10ê°œ ë…¼ë¬¸\n",
      "\n",
      "1. ì œëª©: SAM2Auto: Auto Annotation Using FLASH\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV\n",
      "   ì´ˆë¡: Vision-Language Models (VLMs) lag behind Large Language Models due to the\n",
      "scarcity of annotated datasets, as creating paired visual-textual annotation...\n",
      "\n",
      "2. ì œëª©: CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian\n",
      "  Crosswalk Detection in Aerial Images with High-Performance Computing\n",
      "   ì¹´í…Œê³ ë¦¬: cs.CV\n",
      "   ì´ˆë¡: With the increasing availability of aerial and satellite imagery, deep\n",
      "learning presents significant potential for transportation asset management,\n",
      "sa...\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ ê°€ì§€ ì§ˆì˜ ìœ í˜• í…ŒìŠ¤íŠ¸\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ì¹´í…Œê³ ë¦¬ì˜ ë…¼ë¬¸ë“¤ ì¤‘ì—ì„œ robotì´ë‚˜ roboticsê°€ í¬í•¨ëœ ê²ƒë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”\",\n",
    "    \"generative modelì´ë‚˜ diffusionì— ê´€í•œ ì—°êµ¬ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”\",\n",
    "    \"cs.CV ì¹´í…Œê³ ë¦¬ì—ì„œ object detection ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë³´ì—¬ì£¼ì„¸ìš”\"\n",
    "]\n",
    "\n",
    "for original_query in test_queries:\n",
    "    qwen_think=\"/no_think\"\n",
    "    query=f\"{qwen_think} \\n original_query:{original_query}\"\n",
    "    \n",
    "    print(f\"\\nğŸ” ì§ˆì˜: {original_query}\")\n",
    "    print(\"-\" * 80)\n",
    "    try:\n",
    "        results = retriever.invoke(query)\n",
    "        print(f\"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸\")\n",
    "        \n",
    "        for i, doc in enumerate(results[:2], 1):  # ê° ì§ˆì˜ë‹¹ ìµœëŒ€ 2ê°œ ê²°ê³¼ë§Œ í‘œì‹œ\n",
    "            print(f\"\\n{i}. ì œëª©: {doc.metadata['title']}\")\n",
    "            print(f\"   ì¹´í…Œê³ ë¦¬: {doc.metadata['category']}\")\n",
    "            print(f\"   ì´ˆë¡: {doc.page_content[:150]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
