{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 📋 HWP RAG 시스템 구축 프로젝트\n",
    "\n",
    "이 노트북에서는 HWP(한글) 문서를 PDF로 변환하고, AI 기반 문서 분석을 통해 RAG(Retrieval-Augmented Generation) 시스템을 구축하는 전체 과정을 다룹니다.\n",
    "\n",
    "#### 🔧 필수 패키지 설치\n",
    "HWP 파일을 처리하기 위해 필요한 `olefile` 패키지를 설치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install olefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 📄 HWP → PDF 일괄 변환\n",
    "Windows COM 객체를 사용하여 HWP 파일들을 PDF 형식으로 일괄 변환합니다. 한글 프로그램이 설치된 Windows 환경에서만 작동합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import win32com.client as win32\n",
    "\n",
    "def hwp_to_pdf_batch(folder: str, out_dir: str):\n",
    "    folder = Path(folder).resolve()\n",
    "    out_dir = Path(out_dir).resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    hwp = win32.gencache.EnsureDispatch(\"HWPFrame.HwpObject\")\n",
    "    hwp.RegisterModule(\"FilePathCheckDLL\", \"FilePathCheckerModule\")\n",
    "\n",
    "    for hwp_file in folder.glob(\"*.hwp\"):\n",
    "        try:\n",
    "            hwp.Open(str(hwp_file))\n",
    "        except Exception as e:\n",
    "            print(\"✖ 열기 실패:\", hwp_file.name, e)\n",
    "            continue\n",
    "\n",
    "        pdf_path = out_dir / f\"{hwp_file.stem}.pdf\"\n",
    "        hwp.SaveAs(str(pdf_path), \"PDF\", \"\")\n",
    "        hwp.Run(\"FileClose\")          # ← 핵심 변경: 문서 닫기\n",
    "        print(\"✔\", pdf_path.name)\n",
    "\n",
    "    hwp.Quit()\n",
    "\n",
    "hwp_to_pdf_batch(\"./data\", \"./data/pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 🤖 Docling을 이용한 고급 PDF 처리\n",
    "Docling 라이브러리를 사용하여 PDF 문서를 마크다운으로 변환하고, 이미지와 표를 추출합니다. 고품질 문서 파싱과 구조화된 데이터 추출이 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 4bba35b4916b4b6fae4b5936f7e9e6a2\n",
      "INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'\n",
      "INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'\n",
      "INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']\n",
      "INFO:docling.pipeline.base_pipeline:Processing document proposal_request_2025_ai_economic_policy_service.pdf\n",
      "c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# PDF 문서 변환 실행\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m conv_res \u001b[38;5;241m=\u001b[39m doc_converter\u001b[38;5;241m.\u001b[39mconvert(input_doc_path)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py:39\u001b[0m, in \u001b[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrapped)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py:136\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_complete__:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_validators()\n\u001b[1;32m--> 136\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(pydantic_core\u001b[38;5;241m.\u001b[39mArgsKwargs(args, kwargs))\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\document_converter.py:222\u001b[0m, in \u001b[0;36mDocumentConverter.convert\u001b[1;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;129m@validate_call\u001b[39m(config\u001b[38;5;241m=\u001b[39mConfigDict(strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m     page_range: PageRange \u001b[38;5;241m=\u001b[39m DEFAULT_PAGE_RANGE,\n\u001b[0;32m    213\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConversionResult:\n\u001b[0;32m    214\u001b[0m     all_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_all(\n\u001b[0;32m    215\u001b[0m         source\u001b[38;5;241m=\u001b[39m[source],\n\u001b[0;32m    216\u001b[0m         raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         page_range\u001b[38;5;241m=\u001b[39mpage_range,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(all_res)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\document_converter.py:245\u001b[0m, in \u001b[0;36mDocumentConverter.convert_all\u001b[1;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[0;32m    242\u001b[0m conv_res_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert(conv_input, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error)\n\u001b[0;32m    244\u001b[0m had_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv_res \u001b[38;5;129;01min\u001b[39;00m conv_res_iter:\n\u001b[0;32m    246\u001b[0m     had_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raises_on_error \u001b[38;5;129;01mand\u001b[39;00m conv_res\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    248\u001b[0m         ConversionStatus\u001b[38;5;241m.\u001b[39mSUCCESS,\n\u001b[0;32m    249\u001b[0m         ConversionStatus\u001b[38;5;241m.\u001b[39mPARTIAL_SUCCESS,\n\u001b[0;32m    250\u001b[0m     }:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\document_converter.py:280\u001b[0m, in \u001b[0;36mDocumentConverter._convert\u001b[1;34m(self, conv_input, raises_on_error)\u001b[0m\n\u001b[0;32m    271\u001b[0m _log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoing to convert document batch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# parallel processing only within input_batch\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# with ThreadPoolExecutor(\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m#    max_workers=settings.perf.doc_batch_concurrency\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# ) as pool:\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m#   yield from pool.map(self.process_document, input_batch)\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Note: PDF backends are not thread-safe, thread pool usage was disabled.\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    281\u001b[0m     partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_document, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error),\n\u001b[0;32m    282\u001b[0m     input_batch,\n\u001b[0;32m    283\u001b[0m ):\n\u001b[0;32m    284\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    285\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\document_converter.py:326\u001b[0m, in \u001b[0;36mDocumentConverter._process_document\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m    322\u001b[0m valid \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_formats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m in_doc\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_formats\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 326\u001b[0m     conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_pipeline(in_doc, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not allowed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_doc\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\document_converter.py:349\u001b[0m, in \u001b[0;36mDocumentConverter._execute_pipeline\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m    347\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipeline(in_doc\u001b[38;5;241m.\u001b[39mformat)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     conv_res \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mexecute(in_doc, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raises_on_error:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py:46\u001b[0m, in \u001b[0;36mBasePipeline.execute\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TimeRecorder(\n\u001b[0;32m     42\u001b[0m         conv_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, scope\u001b[38;5;241m=\u001b[39mProfilingScope\u001b[38;5;241m.\u001b[39mDOCUMENT\n\u001b[0;32m     43\u001b[0m     ):\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# These steps are building and assembling the structure of the\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m# output DoclingDocument.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m         conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_document(conv_res)\n\u001b[0;32m     47\u001b[0m         conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assemble_document(conv_res)\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# From this stage, all operations should rely only on conv_res.output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py:160\u001b[0m, in \u001b[0;36mPaginatedPipeline._build_document\u001b[1;34m(self, conv_res)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# 2. Run pipeline stages\u001b[39;00m\n\u001b[0;32m    158\u001b[0m pipeline_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_on_pages(conv_res, init_pages)\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pipeline_pages:  \u001b[38;5;66;03m# Must exhaust!\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Cleanup cached images\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_images:\n\u001b[0;32m    163\u001b[0m         p\u001b[38;5;241m.\u001b[39m_image_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\pipeline\\base_pipeline.py:126\u001b[0m, in \u001b[0;36mPaginatedPipeline._apply_on_pages\u001b[1;34m(self, conv_res, page_batch)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_pipe:\n\u001b[0;32m    124\u001b[0m     page_batch \u001b[38;5;241m=\u001b[39m model(conv_res, page_batch)\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m page_batch\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\models\\page_assemble_model.py:70\u001b[0m, in \u001b[0;36mPageAssembleModel.__call__\u001b[1;34m(self, conv_res, page_batch)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m, conv_res: ConversionResult, page_batch: Iterable[Page]\n\u001b[0;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[Page]:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m page_batch:\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m page\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m page\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mis_valid():\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling\\models\\table_structure_model.py:254\u001b[0m, in \u001b[0;36mTableStructureModel.__call__\u001b[1;34m(self, conv_res, page_batch)\u001b[0m\n\u001b[0;32m    245\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    246\u001b[0m             {\n\u001b[0;32m    247\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_cell\u001b[38;5;241m.\u001b[39mindex,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m             }\n\u001b[0;32m    251\u001b[0m         )\n\u001b[0;32m    252\u001b[0m page_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokens\n\u001b[1;32m--> 254\u001b[0m tf_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_predictor\u001b[38;5;241m.\u001b[39mmulti_table_predict(\n\u001b[0;32m    255\u001b[0m     page_input, [tbl_box], do_matching\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_cell_matching\n\u001b[0;32m    256\u001b[0m )\n\u001b[0;32m    257\u001b[0m table_out \u001b[38;5;241m=\u001b[39m tf_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    258\u001b[0m table_cells \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py:485\u001b[0m, in \u001b[0;36mTFPredictor.multi_table_predict\u001b[1;34m(self, iocr_page, table_bboxes, do_matching, correct_overlapping_cells, sort_row_col_indexes)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# table_image = page_image\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_matching:\n\u001b[1;32m--> 485\u001b[0m     tf_responses, predict_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m    486\u001b[0m         iocr_page,\n\u001b[0;32m    487\u001b[0m         table_bbox,\n\u001b[0;32m    488\u001b[0m         table_image,\n\u001b[0;32m    489\u001b[0m         scale_factor,\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    491\u001b[0m         correct_overlapping_cells,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     tf_responses, predict_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_dummy(\n\u001b[0;32m    495\u001b[0m         iocr_page, table_bbox, table_image, scale_factor, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py:815\u001b[0m, in \u001b[0;36mTFPredictor.predict\u001b[1;34m(self, iocr_page, table_bbox, table_image, scale_factor, eval_res_preds, correct_overlapping_cells)\u001b[0m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_post_process:\n\u001b[0;32m    814\u001b[0m             AggProfiler()\u001b[38;5;241m.\u001b[39mbegin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_process\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prof)\n\u001b[1;32m--> 815\u001b[0m             matching_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processor\u001b[38;5;241m.\u001b[39mprocess(\n\u001b[0;32m    816\u001b[0m                 matching_details, correct_overlapping_cells\n\u001b[0;32m    817\u001b[0m             )\n\u001b[0;32m    818\u001b[0m             AggProfiler()\u001b[38;5;241m.\u001b[39mend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_process\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prof)\n\u001b[0;32m    820\u001b[0m \u001b[38;5;66;03m# Generate the expected Docling responses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling_ibm_models\\tableformer\\data_management\\matching_post_processor.py:1353\u001b[0m, in \u001b[0;36mMatchingPostProcessor.process\u001b[1;34m(self, matching_details, correct_overlapping_cells)\u001b[0m\n\u001b[0;32m   1351\u001b[0m     aligned_table_cells2 \u001b[38;5;241m=\u001b[39m dedupl_table_cells_sorted\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1353\u001b[0m     aligned_table_cells2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_table_cells_to_pdf(\n\u001b[0;32m   1354\u001b[0m         dedupl_table_cells_sorted, pdf_cells, final_matches\n\u001b[0;32m   1355\u001b[0m     )\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;66;03m# 9. Distance-match orphans\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m po1, po2, po3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_orphan_cells(\n\u001b[0;32m   1359\u001b[0m     tab_rows,\n\u001b[0;32m   1360\u001b[0m     tab_columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     final_matches,\n\u001b[0;32m   1365\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages\\docling_ibm_models\\tableformer\\data_management\\matching_post_processor.py\u001b[0m, in \u001b[0;36mMatchingPostProcessor._align_table_cells_to_pdf\u001b[1;34m(self, table_cells, pdf_cells, matches)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# 로깅 설정 - INFO 레벨로 변환 과정을 모니터링\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_log = logging.getLogger(__name__)\n",
    "\n",
    "# 입력 PDF 파일 경로와 출력 디렉토리 설정\n",
    "input_doc_path = Path(\"data/pdf/proposal_request_2025_ai_economic_policy_service.pdf\")\n",
    "output_dir = Path(\"data/pdf/output\")\n",
    "\n",
    "# PDF 파이프라인 옵션 구성\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = 2.0  # 이미지 해상도 스케일 설정\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "# 문서 변환기 초기화 - PDF 형식 옵션과 함께 설정\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "# 변환 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# PDF 문서 변환 실행\n",
    "conv_res = doc_converter.convert(input_doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 💾 마크다운 파일 저장 및 변환 완료\n",
    "변환된 문서를 마크다운 형식으로 저장하고 이미지 참조를 포함한 파일을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "# 출력 디렉토리 생성 (부모 디렉토리도 함께 생성)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "doc_filename = conv_res.input.file.stem\n",
    "\n",
    "# 이미지 참조가 포함된 전체 마크다운 파일 저장\n",
    "md_filename_with_refs = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "conv_res.document.save_as_markdown(md_filename_with_refs, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "# 변환 완료 시간 계산\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "# 변환 완료 로그 출력\n",
    "_log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🖼️ AI 기반 이미지 설명 생성\n",
    "Gemini 2.5 Flash 모델을 사용하여 추출된 이미지들에 대한 자동 캡션을 생성하고, 마크다운 파일의 이미지 참조를 텍스트 설명으로 교체합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "# 이미지 폴더에서 PNG 파일들 가져오기\n",
    "image_folder = \"data/pdf/output/proposal_request_2025_ai_economic_policy_service-with-image-refs_artifacts/\"\n",
    "image_paths = glob.glob(f\"{image_folder}*.png\")\n",
    "\n",
    "# 배치 처리를 위한 메시지 리스트 생성\n",
    "messages_batch = []\n",
    "image_filenames = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        encoded_image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    message = HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": \"이 이미지를 한글로 간단하고 명확하게 설명해주세요. 표나 차트의 경우 주요 내용을 요약해주세요.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{encoded_image}\"}\n",
    "    ])\n",
    "    \n",
    "    messages_batch.append([message])\n",
    "    image_filenames.append(Path(image_path).name)\n",
    "\n",
    "# 배치 처리로 모든 이미지 설명 생성\n",
    "print(\"이미지 설명 생성 중...\")\n",
    "batch_results = llm.batch(messages_batch)\n",
    "\n",
    "# 이미지 파일명과 설명을 매핑\n",
    "image_descriptions = {}\n",
    "for filename, result in zip(image_filenames, batch_results):\n",
    "    image_descriptions[filename] = result.content\n",
    "    print(f\"{filename}: {result.content}\\n\")\n",
    "\n",
    "# 마크다운 파일 읽기\n",
    "with open(md_filename_with_refs, 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "# 이미지 참조를 캡션으로 교체\n",
    "def replace_image_with_caption(match):\n",
    "    full_path = match.group(1)  # 전체 경로 추출\n",
    "    image_filename = Path(full_path).name  # 파일명만 추출\n",
    "    if image_filename in image_descriptions:\n",
    "        caption = image_descriptions[image_filename]\n",
    "        return f\"\\n**[이미지 설명]** {caption}\\n\"\n",
    "    return match.group(0)  # 설명이 없으면 원본 유지\n",
    "\n",
    "# 이미지 참조 패턴 찾기 및 교체 (백슬래시와 슬래시 모두 처리)\n",
    "image_pattern = r'!\\[.*?\\]\\(([^)]+\\.png)\\)'\n",
    "updated_markdown = re.sub(image_pattern, replace_image_with_caption, markdown_content)\n",
    "\n",
    "# 캡션이 포함된 마크다운 파일 저장\n",
    "md_filename_with_captions = output_dir / f\"{doc_filename}-with-captions.md\"\n",
    "with open(md_filename_with_captions, 'w', encoding='utf-8') as f:\n",
    "    f.write(updated_markdown)\n",
    "\n",
    "print(f\"캡션이 포함된 마크다운 파일이 저장되었습니다: {md_filename_with_captions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=500,\n",
    "    separators=[\"##\\n\",\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 캡션이 적용된 파일 내용 읽기\n",
    "with open(md_filename_with_captions, 'r', encoding='utf-8') as f:\n",
    "    captioned_content = f.read()\n",
    "\n",
    "# Document 객체로 생성\n",
    "document = Document(page_content=captioned_content, metadata={\"source\": str(md_filename_with_captions)})\n",
    "\n",
    "# split_documents를 사용하여 Document 객체들로 분할\n",
    "splits = text_splitter.split_documents([document])\n",
    "\n",
    "# splits 길이 분포 파악\n",
    "split_lengths = [len(split.page_content) for split in splits]\n",
    "print(f\"총 분할된 청크 수: {len(splits)}\")\n",
    "print(f\"평균 청크 길이: {sum(split_lengths) / len(split_lengths):.1f} 문자\")\n",
    "print(f\"최소 청크 길이: {min(split_lengths)} 문자\")\n",
    "print(f\"최대 청크 길이: {max(split_lengths)} 문자\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🗄️ Qdrant 벡터 데이터베이스 설정\n",
    "하이브리드 검색을 위해 Dense 임베딩(BGE-M3)과 Sparse 임베딩(BM25)을 함께 사용하는 Qdrant 벡터 스토어를 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, SparseVectorParams, VectorParams\n",
    "\n",
    "# Qdrant 클라이언트 설정\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "dense_embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# 컬렉션 이름 설정\n",
    "collection_name = f\"hwp_rag\"\n",
    "try:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\"dense\": VectorParams(size=1024, distance=Distance.COSINE)},\n",
    "        sparse_vectors_config={\n",
    "            \"sparse\": SparseVectorParams(index=models.SparseIndexParams(on_disk=False))\n",
    "        },\n",
    "    )\n",
    "    print(f\"새 컬렉션 '{collection_name}' 생성됨\")\n",
    "    qdrant = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=dense_embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "        vector_name=\"dense\",\n",
    "        sparse_vector_name=\"sparse\",\n",
    "    )\n",
    "\n",
    "    # 문서를 벡터 스토어에 추가\n",
    "    print(\"문서를 벡터 스토어에 추가 중...\")\n",
    "    qdrant.add_documents(splits)\n",
    "    print(f\"✅ {len(splits)}개의 문서가 Qdrant에 저장되었습니다.\")\n",
    "    \n",
    "except:\n",
    "    qdrant = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=dense_embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "        vector_name=\"dense\",\n",
    "        sparse_vector_name=\"sparse\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🎯 리랭커 기반 검색 품질 향상\n",
    "BGE-Reranker-v2-M3 모델을 사용하여 초기 검색 결과를 재순위화하고, 가장 관련성 높은 문서들만 선별합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# 모델 초기화\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 상위 개의 문서 선택\n",
    "compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "\n",
    "# 문서 압축 검색기 초기화\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(docs, title):\n",
    "    \"\"\"검색 결과를 이쁘게 출력하는 함수\"\"\"\n",
    "    print(f\"{title}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"검색된 문서 수: {len(docs)}개\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"📄 문서 {i}\")\n",
    "        print(f\"📂 출처: {doc.metadata.get('source', 'N/A')}\")\n",
    "        print(f\"📃 페이지: {doc.metadata.get(' page', 'N/A')}\")\n",
    "        print(f\"📝 내용 미리보기:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 쿼리\n",
    "query = \"AI 기반 경제정책정보 제공 서비스의 구조를 설명해주세요\"\n",
    "\n",
    "# 기본 retriever 검색\n",
    "basic_docs = retriever.invoke(query)\n",
    "print_search_results(basic_docs, \"🔍 기본 Retriever 검색 결과:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker 적용된 검색\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "print_search_results(compressed_docs, \"🎯 Reranker 적용된 검색 결과:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# State 정의\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# RAG 프롬프트 템플릿\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "다음 문서들을 참고하여 질문에 답변해주세요.\n",
    "\n",
    "문서들:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변은 한국어로 작성하고, 문서의 내용을 바탕으로 정확하고 자세하게 답변해주세요.\n",
    "답변:\n",
    "\"\"\")\n",
    "\n",
    "def retrieve_documents(state: RAGState) -> RAGState:\n",
    "    \"\"\"문서 검색 단계\"\"\"\n",
    "    print(\"📚 문서 검색 중...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = compression_retriever.invoke(question)\n",
    "    print(f\"✅ {len(documents)}개의 문서를 검색했습니다.\")\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": documents,\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "\n",
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    \"\"\"답변 생성 단계\"\"\"\n",
    "    print(\"🤖 답변 생성 중...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # 문서 내용을 컨텍스트로 결합\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # LLM 체인 구성\n",
    "    chain = rag_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # 답변 생성\n",
    "    answer = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    print(\"✅ 답변이 생성되었습니다.\")\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": documents,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "# LangGraph 워크플로우 구성\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"retrieve\", retrieve_documents)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "\n",
    "# 엣지 추가\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "rag_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"/no_think 입찰가격 평가는 어떻게 이뤄지나요?\"\n",
    "initial_state = {\"question\": question, \"documents\": [], \"answer\": \"\"}\n",
    "\n",
    "for chunk, metadata in rag_app.stream(initial_state, stream_mode=\"messages\"):\n",
    "    if chunk.content:\n",
    "        print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
