{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "## 📚 강의 개요 (Overview)\n",
    "\n",
    "이 강의에서는 RAG(Retrieval-Augmented Generation) 파이프라인에서 중요한 전처리 과정인 ‘Transformation’을 다룹니다. 여기서 ‘Transformation’은 단순한 스케일링이나 정규화 같은 전통적 의미의 데이터 변환이 아니라, 질의응답(Question-Answering) 및 생성(Generation)을 효과적으로 수행하기 위해 텍스트를 다양한 기준으로 나누는(Chunking) 과정을 의미합니다.\n",
    "\n",
    "RAG 모델은 대용량 텍스트 데이터에서 필요한 정보를 검색(Retrieval)한 뒤, 검색된 결과를 입력으로 하여 응답(Generation)을 생성하는 구조입니다. 이때, 문서(또는 여러 형태의 텍스트)를 어떻게 분할(Chunking)하고, 어떤 임베딩을 사용해 의미를 추출하느냐가 모델 성능에 큰 영향을 미칩니다.\n",
    "\n",
    "이 강의에서는 다음과 같은 Transformation(Chunking) 방법을 소개하며, 각각의 사용 용도와 장단점을 살펴봅니다:\n",
    "\n",
    "## 목차: \n",
    "* [CharacterTextSplitter청킹](#charactertextsplitter-청킹)\n",
    "* [RecursiveCharacterTextSplitter청킹](#recursivecharactertextsplitter-청킹)\n",
    "* [코드 청킹](#코드-청킹)\n",
    "* [마크다운 문서 청킹](#마크다운-문서-청킹)\n",
    "* [시맨틱 청킹](#시맨틱-청킹)\n",
    "* [오픈소스 임베딩 모델을 활용한 시맨틱 청킹](#오픈소스-임베딩-모델을-활용한-시맨틱-청킹)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 설정하기 (.env 파일을 사용하지 않을 경우 여기에 입력해주세요!)\n",
    "import os\n",
    "\n",
    "# 환경변수 설정\n",
    "os.environ[\"API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter 청킹\n",
    "텍스트를 일정 길이(캐릭터 수) 단위로 분할하는 가장 단순한 접근 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 경로 설정\n",
    "file_path = \"data/arxiv_paper.pdf\"\n",
    "\n",
    "# LangChain의 PyPDFLoader를 이용해 PDF 파일을 로드\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 로더 객체 생성\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDF의 각 페이지를 저장할 리스트\n",
    "pages = []\n",
    "\n",
    "# 비동기 방식으로 PDF 페이지를 로드 (async for 사용)\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할을 위한 CharacterTextSplitter 불러오기\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",    # 문단 단위로 분할 (두 개의 개행 문자 기준)\n",
    "    chunk_size=500,      # 하나의 청크(조각) 크기를 500자로 설정\n",
    "    chunk_overlap=200,   # 청크 간 200자 겹치게 설정 (문맥 유지 목적)\n",
    "    length_function=len, # 텍스트 길이를 측정하는 함수 (len 사용)\n",
    "    is_separator_regex=False, # separator를 정규식이 아닌 단순 문자열로 처리\n",
    ")\n",
    "\n",
    "# PDF에서 로드한 데이터를 텍스트 청크로 분할\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\") # 첫 번째 청크의 메타데이터 \n",
    "print(texts[0].page_content) # 첫 번째 청크의 내용 \n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\") # 두 번째 청크의 메타데이터 \n",
    "print(texts[1].page_content)# 두 번째 청크의 내용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "      500 이상 문장 개수: {\n",
    "          len(\n",
    "              [i for i in texts if len(i.page_content) > 500]\n",
    "              )\n",
    "          }\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter 청킹\n",
    "\n",
    "`RecursiveCharacterTextSplitter`는 문장을 여러 계층의 구분자(예: 문단, 줄바꿈, 공백 등)를 기준으로 재귀적으로 나누는 역할을 합니다.\n",
    "\n",
    "일반적인 `CharacterTextSplitter`보다 유연하게 텍스트를 나누는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter를 사용하여 텍스트 분할 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # 하나의 청크 크기를 500자로 설정\n",
    "    chunk_overlap=200,    # 청크 간 200자 겹치게 설정 (문맥 유지 목적)\n",
    "    length_function=len,  # 텍스트 길이를 측정하는 함수 (len 사용)\n",
    "    is_separator_regex=False,  # separator가 정규식이 아님을 명시\n",
    ")\n",
    "\n",
    "# PDF에서 로드한 데이터를 텍스트 청크로 분할\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\") # 첫 번째 청크의 메타데이터 \n",
    "print(texts[0].page_content) # 첫 번째 청크의 내용\n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\") # 두 번째 청크의 메타데이터\n",
    "print(texts[1].page_content) # 두 번째 청크의 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "      500 이상 문장 개수: {\n",
    "          len(\n",
    "              [i for i in texts if len(i.page_content) > 500]\n",
    "              )\n",
    "              }\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 구분자를 활용한 RecursiveCharacterTextSplitter 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",   # 두 개의 개행 문자 (문단 분리)\n",
    "        \"\\n\",     # 단일 개행 문자 (줄바꿈)\n",
    "        \" \",      # 공백 (단어 단위 분리)\n",
    "        \".\",      # 마침표\n",
    "        \",\",      # 쉼표\n",
    "        \"\\u200b\",  # Zero-width space (보이지 않는 공백 문자)\n",
    "        \"\\uff0c\",  # Fullwidth comma (중국어, 일본어에서 사용)\n",
    "        \"\\u3001\",  # Ideographic comma (중국어, 일본어에서 사용)\n",
    "        \"\\uff0e\",  # Fullwidth full stop (중국어, 일본어에서 사용)\n",
    "        \"\\u3002\",  # Ideographic full stop (중국어, 일본어에서 사용)\n",
    "        \"\",       # 마지막 분할 기준 (기본적으로 아무 구분자가 없을 경우)\n",
    "    ],\n",
    "    chunk_size=500,       # 기존 설정 유지\n",
    "    chunk_overlap=200,    # 기존 설정 유지\n",
    "    length_function=len,  # 기존 설정 유지\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 청킹\n",
    "\n",
    "코드 블록이나 함수 단위로 텍스트를 나누어, 코드 문서 처리에 특화된 청킹 기법을 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# 지원되는 언어 목록 출력\n",
    "print([e.value for e in Language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 코드에 대한 기본적인 구분자 확인\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 Python 코드\n",
    "PYTHON_CODE = \"\"\"\n",
    "class hello:\n",
    "    def hello_world():\n",
    "        print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "# Python 언어에 최적화된 text splitter 생성\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,  # 사용할 언어를 Python으로 설정\n",
    "    chunk_size=50,             # 청크 크기를 50자로 설정\n",
    "    chunk_overlap=0            # 청크 간 오버랩 없음\n",
    ")\n",
    "# Python 코드 문서를 분할하여 생성\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마크다운 문서 청킹\n",
    "\n",
    "Markdown 헤더나 문서 구조를 기준으로 텍스트를 분할하여, 문서 구조를 활용하는 방법을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# 🦜️🔗 LangChain\n",
    "\n",
    "⚡ Building applications with LLMs through composability ⚡\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "# Hopefully this code block isn't split\n",
    "LangChain is a framework for...\n",
    "\n",
    "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "\"\"\"\n",
    "# LangChain의 RecursiveCharacterTextSplitter를 사용하여 Markdown 텍스트 분할\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,  # Markdown 언어 지정\n",
    "    chunk_size=100,  # 청크 크기를 60자로 설정\n",
    "    chunk_overlap=0  # 청크 간 오버랩 없음\n",
    ")\n",
    "\n",
    "# Markdown 문서를 청크로 나누기\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Markdown 형식의 샘플 텍스트\n",
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "# 헤더를 기준으로 Markdown을 분할하기 위한 규칙 설정\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),   # `#`은 Header 1로 분류\n",
    "    (\"##\", \"Header 2\"),  # `##`는 Header 2로 분류\n",
    "    (\"###\", \"Header 3\"), # `###`는 Header 3로 분류\n",
    "]\n",
    "\n",
    "# 헤더 기반으로 Markdown을 분할하는 Splitter 생성\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "# 설정한 헤더를 기준으로 Markdown을 분할\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시맨틱 청킹\n",
    "\n",
    "텍스트의 의미(Semantic) 기반으로 청크를 생성해, 문맥적으로 연관된 내용을 하나의 청크로 묶는 접근입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "file_path = (\n",
    "    \"data/arxiv_paper.pdf\"\n",
    ")\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 로더 객체 생성\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDF의 각 페이지를 저장할 리스트\n",
    "pages = []\n",
    "\n",
    "# 비동기 방식으로 PDF의 각 페이지를 로드 (async for 사용)\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "\n",
    "# SemanticChunker를 사용하여 의미 기반으로 텍스트를 분할\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "# 문서를 의미적 청킹(Semantic Chunking) 수행\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# 첫 번째 청크의 내용 출력\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"총 {len(docs)}개 만큼의 문서로 청킹되었습니다.\")\n",
    "print([len(i.page_content) for i in docs])\n",
    "\n",
    "# 각 청크의 메타데이터 및 내용 출력\n",
    "for i in docs:\n",
    "    print(i.metadata)       # 문서의 메타데이터 출력 (예: 페이지 번호 등)\n",
    "    print(i.page_content)   # 분할된 청크의 내용 출력\n",
    "    print(\"-\" * 100)        # 구분선 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오픈소스 임베딩 모델을 활용한 시맨틱 청킹  \n",
    "오픈소스 임베딩 모델을 활용하여 텍스트의 의미를 벡터로 변환하고, 이를 기반으로 효과적인 청킹을 수행하는 방법을 실습합니다.  \n",
    "\n",
    "**bge-m3**는 BGE(Bilingual General Embeddings) 시리즈 중 하나로, **텍스트 임베딩(embedding)**을 생성하는 강력한 모델입니다. 주로 정보 검색(Retrieval), 문서 분류, 시맨틱 검색(Semantic Search) 등 다양한 자연어 처리(NLP) 작업에서 활용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"bge-m3\" 모델을 다운로드 및 설치 (설치가 안 되어 있다면 다운로드)\n",
    "# !ollama pull bge-m3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# # PDF 파일 경로 설정\n",
    "# file_path = \"data/arxiv_paper.pdf\"\n",
    "\n",
    "# # LangChain의 PyPDFLoader를 사용하여 PDF 로드\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# # PDF 로더 객체 생성\n",
    "# loader = PyPDFLoader(file_path)\n",
    "\n",
    "# # PDF의 각 페이지를 저장할 리스트\n",
    "# pages = []\n",
    "\n",
    "# # 비동기 방식으로 PDF의 각 페이지를 로드 \n",
    "# async for page in loader.alazy_load():\n",
    "#     pages.append(page)\n",
    "\n",
    "# SemanticChunker를 사용하여 의미 기반으로 텍스트를 분할\n",
    "# OllamaEmbeddings의 \"bge-m3\" 모델을 사용하여 임베딩 생성\n",
    "text_splitter = SemanticChunker(OllamaEmbeddings(model=\"bge-m3\"))\n",
    "\n",
    "# 문서를 의미적 청킹(Semantic Chunking) 수행\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# 첫 번째 청크의 내용 출력\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"총 {len(docs)}개 만큼의 문서로 청킹되었습니다.\")\n",
    "\n",
    "# 각 청크의 길이(문자 개수) 출력\n",
    "print([len(i.page_content) for i in docs])\n",
    "\n",
    "# 각 청크의 메타데이터 및 내용 출력\n",
    "for i in docs:\n",
    "    print(i.metadata)       # 문서의 메타데이터 출력 (예: 페이지 번호 등)\n",
    "    print(i.page_content)   # 분할된 청크의 내용 출력\n",
    "    print(\"-\" * 100)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
