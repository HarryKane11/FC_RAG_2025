{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "## ğŸ“š ê°•ì˜ ê°œìš” (Overview)\n",
    "\n",
    "ì´ ê°•ì˜ì—ì„œëŠ” RAG(Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ì—ì„œ ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ê³¼ì •ì¸ â€˜Transformationâ€™ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì—¬ê¸°ì„œ â€˜Transformationâ€™ì€ ë‹¨ìˆœí•œ ìŠ¤ì¼€ì¼ë§ì´ë‚˜ ì •ê·œí™” ê°™ì€ ì „í†µì  ì˜ë¯¸ì˜ ë°ì´í„° ë³€í™˜ì´ ì•„ë‹ˆë¼, ì§ˆì˜ì‘ë‹µ(Question-Answering) ë° ìƒì„±(Generation)ì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”(Chunking) ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "RAG ëª¨ë¸ì€ ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê²€ìƒ‰(Retrieval)í•œ ë’¤, ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ ì‘ë‹µ(Generation)ì„ ìƒì„±í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ë•Œ, ë¬¸ì„œ(ë˜ëŠ” ì—¬ëŸ¬ í˜•íƒœì˜ í…ìŠ¤íŠ¸)ë¥¼ ì–´ë–»ê²Œ ë¶„í• (Chunking)í•˜ê³ , ì–´ë–¤ ì„ë² ë”©ì„ ì‚¬ìš©í•´ ì˜ë¯¸ë¥¼ ì¶”ì¶œí•˜ëŠëƒê°€ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°•ì˜ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ Transformation(Chunking) ë°©ë²•ì„ ì†Œê°œí•˜ë©°, ê°ê°ì˜ ì‚¬ìš© ìš©ë„ì™€ ì¥ë‹¨ì ì„ ì‚´í´ë´…ë‹ˆë‹¤:\n",
    "\n",
    "## ëª©ì°¨: \n",
    "* [CharacterTextSplitterì²­í‚¹](#charactertextsplitter-ì²­í‚¹)\n",
    "* [RecursiveCharacterTextSplitterì²­í‚¹](#recursivecharactertextsplitter-ì²­í‚¹)\n",
    "* [ì½”ë“œ ì²­í‚¹](#ì½”ë“œ-ì²­í‚¹)\n",
    "* [ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì²­í‚¹](#ë§ˆí¬ë‹¤ìš´-ë¬¸ì„œ-ì²­í‚¹)\n",
    "* [ì‹œë§¨í‹± ì²­í‚¹](#ì‹œë§¨í‹±-ì²­í‚¹)\n",
    "* [ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•œ ì‹œë§¨í‹± ì²­í‚¹](#ì˜¤í”ˆì†ŒìŠ¤-ì„ë² ë”©-ëª¨ë¸ì„-í™œìš©í•œ-ì‹œë§¨í‹±-ì²­í‚¹)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •í•˜ê¸° (.env íŒŒì¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ì—¬ê¸°ì— ì…ë ¥í•´ì£¼ì„¸ìš”!)\n",
    "import os\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "os.environ[\"API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter ì²­í‚¹\n",
    "í…ìŠ¤íŠ¸ë¥¼ ì¼ì • ê¸¸ì´(ìºë¦­í„° ìˆ˜) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ì ‘ê·¼ ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = \"data/arxiv_paper.pdf\"\n",
    "\n",
    "# LangChainì˜ PyPDFLoaderë¥¼ ì´ìš©í•´ PDF íŒŒì¼ì„ ë¡œë“œ\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF ë¡œë” ê°ì²´ ìƒì„±\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDFì˜ ê° í˜ì´ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "pages = []\n",
    "\n",
    "# ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ PDF í˜ì´ì§€ë¥¼ ë¡œë“œ (async for ì‚¬ìš©)\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ CharacterTextSplitter ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í•  (ë‘ ê°œì˜ ê°œí–‰ ë¬¸ì ê¸°ì¤€)\n",
    "    chunk_size=500,      # í•˜ë‚˜ì˜ ì²­í¬(ì¡°ê°) í¬ê¸°ë¥¼ 500ìë¡œ ì„¤ì •\n",
    "    chunk_overlap=200,   # ì²­í¬ ê°„ 200ì ê²¹ì¹˜ê²Œ ì„¤ì • (ë¬¸ë§¥ ìœ ì§€ ëª©ì )\n",
    "    length_function=len, # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (len ì‚¬ìš©)\n",
    "    is_separator_regex=False, # separatorë¥¼ ì •ê·œì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ì²˜ë¦¬\n",
    ")\n",
    "\n",
    "# PDFì—ì„œ ë¡œë“œí•œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• \n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\") # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° \n",
    "print(texts[0].page_content) # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš© \n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\") # ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° \n",
    "print(texts[1].page_content)# ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš© "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "      500 ì´ìƒ ë¬¸ì¥ ê°œìˆ˜: {\n",
    "          len(\n",
    "              [i for i in texts if len(i.page_content) > 500]\n",
    "              )\n",
    "          }\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter ì²­í‚¹\n",
    "\n",
    "`RecursiveCharacterTextSplitter`ëŠ” ë¬¸ì¥ì„ ì—¬ëŸ¬ ê³„ì¸µì˜ êµ¬ë¶„ì(ì˜ˆ: ë¬¸ë‹¨, ì¤„ë°”ê¿ˆ, ê³µë°± ë“±)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¼ë°˜ì ì¸ `CharacterTextSplitter`ë³´ë‹¤ ìœ ì—°í•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # í•˜ë‚˜ì˜ ì²­í¬ í¬ê¸°ë¥¼ 500ìë¡œ ì„¤ì •\n",
    "    chunk_overlap=200,    # ì²­í¬ ê°„ 200ì ê²¹ì¹˜ê²Œ ì„¤ì • (ë¬¸ë§¥ ìœ ì§€ ëª©ì )\n",
    "    length_function=len,  # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (len ì‚¬ìš©)\n",
    "    is_separator_regex=False,  # separatorê°€ ì •ê·œì‹ì´ ì•„ë‹˜ì„ ëª…ì‹œ\n",
    ")\n",
    "\n",
    "# PDFì—ì„œ ë¡œë“œí•œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• \n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\") # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° \n",
    "print(texts[0].page_content) # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš©\n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\") # ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°\n",
    "print(texts[1].page_content) # ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "      500 ì´ìƒ ë¬¸ì¥ ê°œìˆ˜: {\n",
    "          len(\n",
    "              [i for i in texts if len(i.page_content) > 500]\n",
    "              )\n",
    "              }\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ êµ¬ë¶„ìë¥¼ í™œìš©í•œ RecursiveCharacterTextSplitter ì„¤ì •\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",   # ë‘ ê°œì˜ ê°œí–‰ ë¬¸ì (ë¬¸ë‹¨ ë¶„ë¦¬)\n",
    "        \"\\n\",     # ë‹¨ì¼ ê°œí–‰ ë¬¸ì (ì¤„ë°”ê¿ˆ)\n",
    "        \" \",      # ê³µë°± (ë‹¨ì–´ ë‹¨ìœ„ ë¶„ë¦¬)\n",
    "        \".\",      # ë§ˆì¹¨í‘œ\n",
    "        \",\",      # ì‰¼í‘œ\n",
    "        \"\\u200b\",  # Zero-width space (ë³´ì´ì§€ ì•ŠëŠ” ê³µë°± ë¬¸ì)\n",
    "        \"\\uff0c\",  # Fullwidth comma (ì¤‘êµ­ì–´, ì¼ë³¸ì–´ì—ì„œ ì‚¬ìš©)\n",
    "        \"\\u3001\",  # Ideographic comma (ì¤‘êµ­ì–´, ì¼ë³¸ì–´ì—ì„œ ì‚¬ìš©)\n",
    "        \"\\uff0e\",  # Fullwidth full stop (ì¤‘êµ­ì–´, ì¼ë³¸ì–´ì—ì„œ ì‚¬ìš©)\n",
    "        \"\\u3002\",  # Ideographic full stop (ì¤‘êµ­ì–´, ì¼ë³¸ì–´ì—ì„œ ì‚¬ìš©)\n",
    "        \"\",       # ë§ˆì§€ë§‰ ë¶„í•  ê¸°ì¤€ (ê¸°ë³¸ì ìœ¼ë¡œ ì•„ë¬´ êµ¬ë¶„ìê°€ ì—†ì„ ê²½ìš°)\n",
    "    ],\n",
    "    chunk_size=500,       # ê¸°ì¡´ ì„¤ì • ìœ ì§€\n",
    "    chunk_overlap=200,    # ê¸°ì¡´ ì„¤ì • ìœ ì§€\n",
    "    length_function=len,  # ê¸°ì¡´ ì„¤ì • ìœ ì§€\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì½”ë“œ ì²­í‚¹\n",
    "\n",
    "ì½”ë“œ ë¸”ë¡ì´ë‚˜ í•¨ìˆ˜ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´, ì½”ë“œ ë¬¸ì„œ ì²˜ë¦¬ì— íŠ¹í™”ëœ ì²­í‚¹ ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# ì§€ì›ë˜ëŠ” ì–¸ì–´ ëª©ë¡ ì¶œë ¥\n",
    "print([e.value for e in Language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ì½”ë“œì— ëŒ€í•œ ê¸°ë³¸ì ì¸ êµ¬ë¶„ì í™•ì¸\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ Python ì½”ë“œ\n",
    "PYTHON_CODE = \"\"\"\n",
    "class hello:\n",
    "    def hello_world():\n",
    "        print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "# Python ì–¸ì–´ì— ìµœì í™”ëœ text splitter ìƒì„±\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,  # ì‚¬ìš©í•  ì–¸ì–´ë¥¼ Pythonìœ¼ë¡œ ì„¤ì •\n",
    "    chunk_size=50,             # ì²­í¬ í¬ê¸°ë¥¼ 50ìë¡œ ì„¤ì •\n",
    "    chunk_overlap=0            # ì²­í¬ ê°„ ì˜¤ë²„ë© ì—†ìŒ\n",
    ")\n",
    "# Python ì½”ë“œ ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ ìƒì„±\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì²­í‚¹\n",
    "\n",
    "Markdown í—¤ë”ë‚˜ ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬, ë¬¸ì„œ êµ¬ì¡°ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# ğŸ¦œï¸ğŸ”— LangChain\n",
    "\n",
    "âš¡ Building applications with LLMs through composability âš¡\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "# Hopefully this code block isn't split\n",
    "LangChain is a framework for...\n",
    "\n",
    "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "\"\"\"\n",
    "# LangChainì˜ RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ Markdown í…ìŠ¤íŠ¸ ë¶„í• \n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,  # Markdown ì–¸ì–´ ì§€ì •\n",
    "    chunk_size=100,  # ì²­í¬ í¬ê¸°ë¥¼ 60ìë¡œ ì„¤ì •\n",
    "    chunk_overlap=0  # ì²­í¬ ê°„ ì˜¤ë²„ë© ì—†ìŒ\n",
    ")\n",
    "\n",
    "# Markdown ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Markdown í˜•ì‹ì˜ ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "# í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Markdownì„ ë¶„í• í•˜ê¸° ìœ„í•œ ê·œì¹™ ì„¤ì •\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),   # `#`ì€ Header 1ë¡œ ë¶„ë¥˜\n",
    "    (\"##\", \"Header 2\"),  # `##`ëŠ” Header 2ë¡œ ë¶„ë¥˜\n",
    "    (\"###\", \"Header 3\"), # `###`ëŠ” Header 3ë¡œ ë¶„ë¥˜\n",
    "]\n",
    "\n",
    "# í—¤ë” ê¸°ë°˜ìœ¼ë¡œ Markdownì„ ë¶„í• í•˜ëŠ” Splitter ìƒì„±\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "# ì„¤ì •í•œ í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Markdownì„ ë¶„í• \n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‹œë§¨í‹± ì²­í‚¹\n",
    "\n",
    "í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸(Semantic) ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ë¥¼ ìƒì„±í•´, ë¬¸ë§¥ì ìœ¼ë¡œ ì—°ê´€ëœ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë¬¶ëŠ” ì ‘ê·¼ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "file_path = (\n",
    "    \"data/arxiv_paper.pdf\"\n",
    ")\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF ë¡œë” ê°ì²´ ìƒì„±\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDFì˜ ê° í˜ì´ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "pages = []\n",
    "\n",
    "# ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ PDFì˜ ê° í˜ì´ì§€ë¥¼ ë¡œë“œ (async for ì‚¬ìš©)\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "\n",
    "# SemanticChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• \n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "# ë¬¸ì„œë¥¼ ì˜ë¯¸ì  ì²­í‚¹(Semantic Chunking) ìˆ˜í–‰\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš© ì¶œë ¥\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì´ {len(docs)}ê°œ ë§Œí¼ì˜ ë¬¸ì„œë¡œ ì²­í‚¹ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print([len(i.page_content) for i in docs])\n",
    "\n",
    "# ê° ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° ë° ë‚´ìš© ì¶œë ¥\n",
    "for i in docs:\n",
    "    print(i.metadata)       # ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¶œë ¥ (ì˜ˆ: í˜ì´ì§€ ë²ˆí˜¸ ë“±)\n",
    "    print(i.page_content)   # ë¶„í• ëœ ì²­í¬ì˜ ë‚´ìš© ì¶œë ¥\n",
    "    print(\"-\" * 100)        # êµ¬ë¶„ì„  ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•œ ì‹œë§¨í‹± ì²­í‚¹  \n",
    "ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íš¨ê³¼ì ì¸ ì²­í‚¹ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.  \n",
    "\n",
    "**bge-m3**ëŠ” BGE(Bilingual General Embeddings) ì‹œë¦¬ì¦ˆ ì¤‘ í•˜ë‚˜ë¡œ, **í…ìŠ¤íŠ¸ ì„ë² ë”©(embedding)**ì„ ìƒì„±í•˜ëŠ” ê°•ë ¥í•œ ëª¨ë¸ì…ë‹ˆë‹¤. ì£¼ë¡œ ì •ë³´ ê²€ìƒ‰(Retrieval), ë¬¸ì„œ ë¶„ë¥˜, ì‹œë§¨í‹± ê²€ìƒ‰(Semantic Search) ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì—ì„œ í™œìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"bge-m3\" ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ (ì„¤ì¹˜ê°€ ì•ˆ ë˜ì–´ ìˆë‹¤ë©´ ë‹¤ìš´ë¡œë“œ)\n",
    "# !ollama pull bge-m3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# # PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "# file_path = \"data/arxiv_paper.pdf\"\n",
    "\n",
    "# # LangChainì˜ PyPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œ\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# # PDF ë¡œë” ê°ì²´ ìƒì„±\n",
    "# loader = PyPDFLoader(file_path)\n",
    "\n",
    "# # PDFì˜ ê° í˜ì´ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "# pages = []\n",
    "\n",
    "# # ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ PDFì˜ ê° í˜ì´ì§€ë¥¼ ë¡œë“œ \n",
    "# async for page in loader.alazy_load():\n",
    "#     pages.append(page)\n",
    "\n",
    "# SemanticChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• \n",
    "# OllamaEmbeddingsì˜ \"bge-m3\" ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "text_splitter = SemanticChunker(OllamaEmbeddings(model=\"bge-m3\"))\n",
    "\n",
    "# ë¬¸ì„œë¥¼ ì˜ë¯¸ì  ì²­í‚¹(Semantic Chunking) ìˆ˜í–‰\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš© ì¶œë ¥\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì´ {len(docs)}ê°œ ë§Œí¼ì˜ ë¬¸ì„œë¡œ ì²­í‚¹ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ê° ì²­í¬ì˜ ê¸¸ì´(ë¬¸ì ê°œìˆ˜) ì¶œë ¥\n",
    "print([len(i.page_content) for i in docs])\n",
    "\n",
    "# ê° ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° ë° ë‚´ìš© ì¶œë ¥\n",
    "for i in docs:\n",
    "    print(i.metadata)       # ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¶œë ¥ (ì˜ˆ: í˜ì´ì§€ ë²ˆí˜¸ ë“±)\n",
    "    print(i.page_content)   # ë¶„í• ëœ ì²­í¬ì˜ ë‚´ìš© ì¶œë ¥\n",
    "    print(\"-\" * 100)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
