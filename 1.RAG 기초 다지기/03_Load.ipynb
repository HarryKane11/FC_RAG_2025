{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "## 📚 강의 개요 (Overview)\n",
    "\n",
    "이 강의에서는 텍스트 데이터를 임베딩하고, 벡터 데이터베이스에 로드(Load)하여 효율적으로 저장하고 검색하는 방법을 다룹니다. RAG (Retrieval-Augmented Generation) 시스템에서 어떤 임베딩 모델을 사용할지, 그리고 어떤 벡터 저장소를 활용할지가 검색 및 응답 성능에 중요한 영향을 미칩니다.\n",
    "\n",
    "이 강의를 통해 다양한 임베딩 모델을 활용하여 텍스트를 벡터로 변환하고, 이를 벡터 데이터베이스에 저장하여 빠르게 검색하는 방법을 배웁니다.\n",
    "\n",
    "## 목차: \n",
    "* [OpenAI 임베딩 모델 활용하기](#openai-임베딩-모델-활용하기)\n",
    "* [Ollama Embedding 모델 활용하기](#ollama-embedding-모델-활용하기)\n",
    "* [FAISS로 임베딩 벡터 저장하기](#faiss로-임베딩-벡터-저장하기)\n",
    "* [Chroma 벡터 DB](#chroma-벡터-db)\n",
    "* [Qdrant 벡터 DB](#qdrant-벡터-db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 설정하기 (.env 파일을 사용하지 않을 경우 여기에 입력해주세요!)\n",
    "import os\n",
    "\n",
    "# 환경변수 설정\n",
    "os.environ[\"API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI 임베딩 모델 활용하기\n",
    "\n",
    "`text-embedding-3-small`: OpenAI에서 제공하는 최신 임베딩 모델 중 하나로, 빠르고 가벼운 임베딩을 제공합니다.\n",
    "\n",
    "문서 임베딩 vs 질의 임베딩\n",
    "* `embed_documents()` → 여러 개의 문장을 한 번에 임베딩 (문서 검색 등에 활용).\n",
    "* `embed_query()` → 질의(Query)를 임베딩 (질문-응답 시스템에서 검색할 때 활용)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://arize.com/wp-content/uploads/2022/06/blog-king-queen-embeddings.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서(문장) 리스트를 벡터로 변환 (임베딩)\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 임베딩된 문서 개수와 개별 임베딩 벡터의 차원 출력\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.010680568404495716,\n",
       " -0.01018487848341465,\n",
       " -0.0019450020045042038,\n",
       " 0.023096051067113876,\n",
       " -0.02682921662926674]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질의(Query) 문장을 임베딩 벡터로 변환\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5] #처음 5개의 값만 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])# 개별 문장의 임베딩 차원 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Embedding 모델 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 문서를 로드하여 페이지별로 저장하는 과정\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"data/arxiv_paper.pdf\"\n",
    ")\n",
    "\n",
    "# PDF 로더 객체 생성\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDF의 각 페이지를 저장할 리스트 초기화\n",
    "pages = []\n",
    "\n",
    "# PDF를 비동기 방식으로 로드하여 페이지별로 저장\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "ronment, we show that structured reasoning ap-\n",
      "proaches, such as Chain-of-Thoughts, improve\n",
      "confidence calibration. However, our findings\n",
      "also reveal persistent challenges in distinguishing\n",
      "uncertainty, particularly under abductive settings,\n",
      "underscoring the need for more sophisticated em-\n",
      "bodied confidence elicitation methods.\n",
      "1. Introduction\n",
      "In complex embodied environments, success depends not\n",
      "only on what an agent knows but also on how well it un-\n",
      "derstands and communicates uncertainty. Whether navi-\n",
      "gating a cluttered space, interacting with objects, or plan-\n",
      "ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter를 사용하여 텍스트 분할 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # 하나의 청크 크기를 1000자로 설정\n",
    "    chunk_overlap=200,     # 청크 간 200자 겹치게 설정 (문맥 유지 목적)\n",
    "    length_function=len,   # 텍스트 길이를 측정하는 함수 (len 사용)\n",
    "    is_separator_regex=False  # separator를 정규식이 아닌 단순 문자열로 처리\n",
    ")\n",
    "\n",
    "# PDF에서 로드한 데이터를 텍스트 청크로 분할\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\")# 첫 번째 청크의 메타데이터\n",
    "print(texts[0].page_content)# 첫 번째 청크의 내용 \n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\")# 두 번째 청크의 메타데이터\n",
    "print(texts[1].page_content)# 두 번째 청크의 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='Uncertainty in Action: Confidence Elicitation in Embodied Agents\\nTianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\\nTal August, Ismini Lourentzou\\nUniversity of Illinois Urbana-Champaign\\n{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\\nhttps://plan-lab.github.io/ece\\nAbstract\\nExpressing confidence is challenging for embod-\\nied agents navigating dynamic multimodal en-\\nvironments, where uncertainty arises from both\\nperception and decision-making processes. We\\npresent the first work investigating embodied con-\\nfidence elicitation in open-ended multimodal en-\\nvironments. We introduce Elicitation Policies,\\nwhich structure confidence assessment across\\ninductive, deductive, and abductive reasoning,\\nalong with Execution Policies, which enhance\\nconfidence calibration through scenario reinter-\\npretation, action sampling, and hypothetical rea-\\nsoning. Evaluating agents in calibration and fail-\\nure prediction tasks within the Minecraft envi-'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='confidence calibration through scenario reinter-\\npretation, action sampling, and hypothetical rea-\\nsoning. Evaluating agents in calibration and fail-\\nure prediction tasks within the Minecraft envi-\\nronment, we show that structured reasoning ap-\\nproaches, such as Chain-of-Thoughts, improve\\nconfidence calibration. However, our findings\\nalso reveal persistent challenges in distinguishing\\nuncertainty, particularly under abductive settings,\\nunderscoring the need for more sophisticated em-\\nbodied confidence elicitation methods.\\n1. Introduction\\nIn complex embodied environments, success depends not\\nonly on what an agent knows but also on how well it un-\\nderstands and communicates uncertainty. Whether navi-\\ngating a cluttered space, interacting with objects, or plan-\\nning long-term strategies, eliciting confidence is pivotal as\\nagents must interpret and interact with dynamic settings\\nin real-time while managing uncertainty from both percep-'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='ning long-term strategies, eliciting confidence is pivotal as\\nagents must interpret and interact with dynamic settings\\nin real-time while managing uncertainty from both percep-\\ntion and decision-making processes (Ren et al., 2023; Liang\\net al., 2024). For humans, this instinctive ability to express\\nand calibrate uncertainty is fundamental to decision-making\\nand social interaction. As AI systems are increasingly de-\\nployed in high-stakes contexts such as autonomous driving\\nor healthcare, they must also acquire this crucial skill.\\n*Preprint. Work in progress.\\nEmbodied Environment\\nElicitation Module\\nAre you sure about yournext action?\\nElicitation Module\\nAre you sure aboutwhat you see?\\nElicitation\\nPolicies\\nExecution\\nPolicies\\nElicitation\\nPolicies\\nExecution\\nPolicies\\nPerception Stage\\nAction Stage\\nFigure 1.Embodied Confidence Estimation Framework consist-\\ning of Elicitation Policies and Execution Policies, which jointly\\nenable an agent to assess and express its confidence. Elicitation'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='Figure 1.Embodied Confidence Estimation Framework consist-\\ning of Elicitation Policies and Execution Policies, which jointly\\nenable an agent to assess and express its confidence. Elicitation\\nModules prompt the agent to evaluate uncertainty in what it sees\\nand does, while Execution Policies refine confidence calibration\\nby expanding the agent’s reasoning space (See §3 for details).\\nSpecifically, accurate confidence elicitation from AI systems\\nprovides critical insights for risk assessment, error mitiga-\\ntion, and system reliability in decision-making (Kuleshov\\n& Deshpande, 2022; Clark, 2015; Yildirim et al., 2019).\\nThis is particularly important in open-ended reasoning tasks,\\nwhere models may generate outputs that are semantically\\nplausible but factually incorrect, a phenomenon commonly\\nreferred to as hallucination (Xiao & Wang, 2021). How-\\never, confidence elicitation in embodied AI is particularly\\nchallenging. For instance, in open-ended environments such'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='referred to as hallucination (Xiao & Wang, 2021). How-\\never, confidence elicitation in embodied AI is particularly\\nchallenging. For instance, in open-ended environments such\\nas Minecraft, an agent may misinterpret visual cues due\\nto limited viewpoints or struggle to determine the correct\\naction sequence to achieve complex goals (e.g., obtaining\\na diamond). These illustrate the broader difficulties in elic-\\niting confidence in embodied environments, where agents\\nmust navigate uncertainty at multiple levels.\\nConfidence elicitation in open-ended embodied environ-\\nments faces several challenges, including: 1) Multimodal\\nunderstanding, where the agent must assess uncertainty from\\ninputs across different interconnected modalities. 2) Granu-\\nlarity of confidence estimation, where the agent evaluates\\nconfidence not only in performing specific actions (e.g., “I\\nam 90% confident I can collect some wood”) but also in\\n1\\narXiv:2503.10628v1  [cs.AI]  13 Mar 2025')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# \"bge-m3\" 모델을 사용하여 텍스트 임베딩 생성\n",
    "embeddings_model=OllamaEmbeddings(model=\"bge-m3\")\n",
    "# 청크된 문서 리스트를 벡터화하여 임베딩 생성\n",
    "embeddings = embeddings_model.embed_documents([i.page_content for i in texts])\n",
    "\n",
    "len(embeddings[0])# 생성된 임베딩 벡터의 차원 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imghub.insilicogen.com/media/photos/51034_43550_1241.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS로 임베딩 벡터 저장하기\n",
    "\n",
    "FAISS(Facebook AI Similarity Search)\n",
    "*  대량의 벡터 데이터를 효율적으로 검색하는 라이브러리\n",
    "* 문서 검색, 추천 시스템, 이미지 검색 등에서 활용됨.\n",
    "* 벡터의 유사도를 계산하여 가장 가까운 문서를 빠르게 찾을 수 있음.\n",
    "\n",
    "FAISS 벡터 저장소 구축\n",
    "* `FAISS.from_documents()` → 텍스트 청크를 벡터로 변환하여 FAISS에 저장.\n",
    "* `embedding=embeddings_model` → Ollama의 `bge-m3` 모델을 사용하여 임베딩을 생성.\n",
    "\n",
    "FAISS 검색 방식\n",
    "* `similarity_search(query, k=1)` → 가장 유사한 k개의 문서를 검색.\n",
    "* `similarity_search(query, k=10, filter={\"page\": 0})` → 특정 조건(예: page=0)에서 유사한 문서 검색.\n",
    "* `similarity_search_with_score(query, k=10)` → 검색된 문서와 함께 유사도 점수(거리) 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain_community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크의 수: 100\n",
      "벡터 저장소에 저장된 문서 수: 100\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#  FAISS 벡터 저장소 생성 (OllamaEmbeddings을 활용)\n",
    "# 앞서 생성한 청크(`texts`)와 임베딩 모델(`embeddings_model`)을 이용하여 벡터 저장소를 구축\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings_model # Ollama 임베딩 모델\n",
    ")\n",
    "\n",
    "# 벡터 저장소 크기 확인\n",
    "print(f\"청크의 수: {len(texts)}\") # 총 청크된 문서 개수\n",
    "print(f\"벡터 저장소에 저장된 문서 수: {vector_store.index.ntotal}\")# 총 청크된 문서 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment is critical. However, the\n",
      "reliance on large pre-trained models raises concerns about\n",
      "energy efficiency and ethical considerations in high-stakes\n",
      "applications, which warrant further exploration.\n",
      "References\n",
      "Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,\n",
      "Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-\n",
      "ravi, A., Acharya, U. R., et al. A review of uncertainty\n",
      "quantification in deep learning: Techniques, applications\n",
      "and challenges. Information Fusion, 2021. [{'source': 'data/arxiv_paper.pdf', 'page': 8}]\n",
      "* sents visual observations and It represents task instructions\n",
      "and other types of language-based guidance. For a given\n",
      "task T , the agent operates under a policy π : I → Athat\n",
      "maps input I to actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score c ∈ [0, 1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.\n",
      "The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks. This requires not only\n",
      "detecting uncertain aspects of the agent’s perception, reason-\n",
      "ing, or actions but also ensuring that confidence estimates\n",
      "are refined and reliable under dynamic multimodal condi-\n",
      "tions. To address this, we propose an embodied confidence\n",
      "estimation framework centered around Elicitation Modules\n",
      "that facilitates confidence elicitation at two critical points [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n",
      "* tend to yield improved confidence calibration. For instance,\n",
      "MineLLM’s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVE’s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence judgments.\n",
      "So, How Effectively Can Embodied Agents Express Con-\n",
      "fidence in Dynamic Embodied Tasks? While embodied\n",
      "agents can convey confidence to some extent, their effec-\n",
      "tiveness depends on how well they integrate reasoning, un-\n",
      "certainty assessment, and environmental interactions. The\n",
      "findings reveal that embodied confidence elicitation remains\n",
      "a challenging problem, requiring a careful balance between\n",
      "general-purpose reasoning and task-specific specialization. [{'source': 'data/arxiv_paper.pdf', 'page': 5}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes.\n",
      "To address this, we introduce a set of policies that gener-\n",
      "ate additional observations and diverse action trajectories,\n",
      "promoting robust confidence assessment:\n",
      "⟳ Action Sampling: The agent can generate multiple pos-\n",
      "sible actions by sampling from a learned policy distribution\n",
      "over the action space, conditioned on the current state and\n",
      "task objectives. By doing so, the agent can explore multiple\n",
      "actions, evaluate different outcomes, and assess which is\n",
      "most likely to succeed based on its perception.\n",
      "⟳ Scenario Reinterpretation: The agent can be prompted\n",
      "to reinterpret the same scenario from different perspec-\n",
      "tives. For example, it could focus on a particular object,\n",
      "re-evaluate environmental obstacles, or re-assess the prox-\n",
      "imity of targets. This enables the agent to propose different\n",
      "courses of action by gathering and redirecting its attention\n",
      "to relevant environmental information. [{'source': 'data/arxiv_paper.pdf', 'page': 4}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., “I am 70%\n",
      "confident I craft a wooden table”). 3) Interactive depen-\n",
      "dencies, where the agent’s actions directly influence the\n",
      "environment, which in turn affects subsequent decisions,\n",
      "requiring ongoing adjustments to confidence estimates as\n",
      "tasks progress. 4) Finally, while state-of-the-art embodied\n",
      "agents leverage proprietary Large Language Models (LLMs)\n",
      "and Vision-Language Models (VLMs) for their strong mul-\n",
      "timodal understanding and reasoning capabilities (Wang\n",
      "et al., 2023a; Qin et al., 2024; Zhu et al., 2023), these of-\n",
      "ten lack access to internal token likelihoods or probabilistic\n",
      "outputs, making traditional confidence estimation methods\n",
      "ineffective (Kumar et al., 2023; Chen et al., 2024b).\n",
      "To address these challenges, we present the first system-\n",
      "atic approach that enables LLM/VLM-powered embodied\n",
      "agents to assess and articulate their confidence across multi- [{'source': 'data/arxiv_paper.pdf', 'page': 1}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agents’ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using bayesian binning. In\n",
      "AAAI Conference on Artificial Intelligence, 2015.\n",
      "Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y ., Ha-\n",
      "jishirzi, H., Singh, S., and Fox, R. Do embodied agents\n",
      "dream of pixelated sheep: Embodied decision making\n",
      "using language guided world modelling. In International\n",
      "Conference on Machine Learning, 2023.\n",
      "Okoli, C. Inductive, abductive and deductive theorising.\n",
      "International Journal of Management Concepts and Phi-\n",
      "losophy, 2023. [{'source': 'data/arxiv_paper.pdf', 'page': 10}]\n",
      "* Figure 1.Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies and Execution Policies, which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation\n",
      "Modules prompt the agent to evaluate uncertainty in what it sees\n",
      "and does, while Execution Policies refine confidence calibration\n",
      "by expanding the agent’s reasoning space (See §3 for details).\n",
      "Specifically, accurate confidence elicitation from AI systems\n",
      "provides critical insights for risk assessment, error mitiga-\n",
      "tion, and system reliability in decision-making (Kuleshov\n",
      "& Deshpande, 2022; Clark, 2015; Yildirim et al., 2019).\n",
      "This is particularly important in open-ended reasoning tasks,\n",
      "where models may generate outputs that are semantically\n",
      "plausible but factually incorrect, a phenomenon commonly\n",
      "referred to as hallucination (Xiao & Wang, 2021). How-\n",
      "ever, confidence elicitation in embodied AI is particularly\n",
      "challenging. For instance, in open-ended environments such [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "language understanding. Trained on a vast dataset of 500,000 Minecraft-specific image-text instruction pairs, MineLLM\n",
      "can generate detailed insights about the game environment, answer complex queries, and provide contextual guidance for\n",
      "planning and execution. Its integration into MP5 enables the framework to address context- and process-dependent tasks\n",
      "with remarkable success rates, achieving a 91% success rate on context-dependent tasks and demonstrating exceptional\n",
      "adaptability in novel scenarios.\n",
      "STEVE: The STEVE series represents another advancement in language model-driven embodied agents for the Minecraft\n",
      "environment (Zhao et al., 2025). Built upon the foundation of LLaMA-2 (Touvron et al., 2023b), STEVE integrates powerful\n",
      "language capabilities tailored to enhance task reasoning, contextual understanding, and interaction. At its core, the language [{'source': 'data/arxiv_paper.pdf', 'page': 16}]\n",
      "* (§3.3) refine and expand confidence assessment through scenario reinterpretation, action sampling, and hypothetical reasoning. Together,\n",
      "they enhance confidence calibration in embodied agents. The orange text represents the vanilla elicitation policy, which incorporates the\n",
      "vanilla confidence prompt (described in Table 1) into the original instruction. The brown arrows\n",
      " denote the Scenario-Reinterpretation\n",
      "execution policy, prompting the agent to generate additional scene insights.\n",
      "to address by designing a unified approach that enhances\n",
      "reliability and robustness in embodied agents.\n",
      "Uncertainty in Embodied Models. Uncertainty estima-\n",
      "tion is well explored in robot learning and reinforcement\n",
      "learning (Wang & Zou, 2021; Ghasemipour et al., 2022; He\n",
      "et al., 2023; Huang et al., 2019; Jin et al., 2023), but remains\n",
      "a challenge for language models (Tian et al., 2023; Groot\n",
      "& Valdenegro Toro, 2024; Zhang et al., 2024b). While\n",
      "recent efforts have sought to quantify and mitigate uncer- [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n"
     ]
    }
   ],
   "source": [
    "# 1개의 유사한 문서를 검색 (기본 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agent가 뭐야?\",k=10)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* Figure 1.Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies and Execution Policies, which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation\n",
      "Modules prompt the agent to evaluate uncertainty in what it sees\n",
      "and does, while Execution Policies refine confidence calibration\n",
      "by expanding the agent’s reasoning space (See §3 for details).\n",
      "Specifically, accurate confidence elicitation from AI systems\n",
      "provides critical insights for risk assessment, error mitiga-\n",
      "tion, and system reliability in decision-making (Kuleshov\n",
      "& Deshpande, 2022; Clark, 2015; Yildirim et al., 2019).\n",
      "This is particularly important in open-ended reasoning tasks,\n",
      "where models may generate outputs that are semantically\n",
      "plausible but factually incorrect, a phenomenon commonly\n",
      "referred to as hallucination (Xiao & Wang, 2021). How-\n",
      "ever, confidence elicitation in embodied AI is particularly\n",
      "challenging. For instance, in open-ended environments such [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n",
      "tion and decision-making processes (Ren et al., 2023; Liang\n",
      "et al., 2024). For humans, this instinctive ability to express\n",
      "and calibrate uncertainty is fundamental to decision-making\n",
      "and social interaction. As AI systems are increasingly de-\n",
      "ployed in high-stakes contexts such as autonomous driving\n",
      "or healthcare, they must also acquire this crucial skill.\n",
      "*Preprint. Work in progress.\n",
      "Embodied Environment\n",
      "Elicitation Module\n",
      "Are you sure about yournext action?\n",
      "Elicitation Module\n",
      "Are you sure aboutwhat you see?\n",
      "Elicitation\n",
      "Policies\n",
      "Execution\n",
      "Policies\n",
      "Elicitation\n",
      "Policies\n",
      "Execution\n",
      "Policies\n",
      "Perception Stage\n",
      "Action Stage\n",
      "Figure 1.Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies and Execution Policies, which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n"
     ]
    }
   ],
   "source": [
    "# 특정 페이지에 대한 필터링 검색 (page=0인 문서에서 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agent가 뭐야?\",k=10,filter={\"page\": 0})\n",
    "\n",
    "# 필터링된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 점수를 포함한 검색 (Query와 문서 간 거리 계산)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied Agent가 뭐야?\",k=10)\n",
    "\n",
    "print(\"여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\\n\")\n",
    "\n",
    "# 검색된 문서와 점수 출력\n",
    "for doc, score in results:\n",
    "    print(f\"* [유사도={score:3f}] {doc.page_content[:100]} [{doc.metadata}]\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma 벡터 DB\n",
    "\n",
    "\n",
    "* ChromaDB는 벡터 데이터베이스로, 텍스트 검색과 추천 시스템 등에 활용됨.\n",
    "* FAISS는 메모리 내(in-memory)에서 작동하지만, Chroma는 영구 저장(Persistent Storage) 가능.\n",
    "* Chroma는 쿼리 시 더 다양한 필터링과 조합이 가능함.\n",
    "\n",
    "**Chroma 벡터 저장소 구축**\n",
    "* `Chroma(collection_name=\"test_01\", embedding_function=embeddings_model)`→ \"test_01\"이라는 컬렉션을 생성하고, Ollama 임베딩 모델을 사용하여 벡터 저장.\n",
    "* `add_documents(documents=texts, ids=ids)`→ 임베딩을 생성한 후 ChromaDB에 저장.\n",
    "\n",
    "**유사도 검색 방식**\n",
    "* `similarity_search(query, k=1)` → 가장 유사한 k개의 문서를 검색.\n",
    "* `similarity_search_with_score(query, k=5)` → 문서와 쿼리 간의 유사도 점수를 함께 반환.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08f1e927-7586-4b72-b522-f5a728e162fd',\n",
       " '5fd0ac08-167d-4818-95b9-60863a79ad63',\n",
       " '6f10d2dc-8abd-40e5-97c1-4eb59cf80969',\n",
       " '5027fbb7-788d-4936-9eef-030e04e93900',\n",
       " '4acd9c25-f2b9-4796-8fd5-caec29b55672',\n",
       " '16b1114c-1184-49b1-b08f-8fa6d7ed4715',\n",
       " '7a3d32e9-0121-4770-a125-7b76db933224',\n",
       " 'baaca446-fffb-45fa-96f7-d14e186a5fe7',\n",
       " 'da749c3d-6718-4aa2-a761-0c5714231862',\n",
       " 'fb0ad5d6-460d-4095-afcb-a0e86c2c8029',\n",
       " '9229acab-ad73-4999-910e-ec60e0633167',\n",
       " 'a02d917d-f1c3-4012-a27f-d71335b31756',\n",
       " '32ae1fbf-0b5e-494c-9790-3dc5961ade49',\n",
       " '974b2322-f4fb-49e9-a1d9-08ffa4a7c5a7',\n",
       " '921de3e4-bee4-4cb4-b3e4-b88948cde0ce',\n",
       " 'cd7bc759-b09b-4fd8-b925-f12f16a7521f',\n",
       " '8308a0d8-f9c2-445c-b98f-93b856ebd5ba',\n",
       " 'f8573157-6cc7-4156-bbfe-3650e0acf51e',\n",
       " '25c136b4-f15d-4c9e-8235-177aaaff2619',\n",
       " '2ab6401c-bb7f-4656-b502-ffdc379ebfd7',\n",
       " 'e6713bab-5bd9-465a-956b-1f97607dd9fc',\n",
       " 'fc1fd8c6-5b0d-4dc4-b0d5-92e1c3df28cc',\n",
       " 'a0e8c55f-c72a-4470-8eb6-b1c0369b234f',\n",
       " '47df9438-c328-4be9-bd25-731ba907fa52',\n",
       " '855b459c-70a5-4ed7-934a-a5788146f5f2',\n",
       " '56b6bf99-01e6-481f-b475-a14680d0652e',\n",
       " 'cf5b1023-c078-494d-a557-eb8a91b46e5f',\n",
       " '63763c2e-ec1f-45ee-a0c9-8864ec55e4a6',\n",
       " '7a3f3add-3591-4237-8e24-c37d10e60600',\n",
       " '1ceabec3-e5e4-4c15-958b-daa6aa39247f',\n",
       " '882f2540-5dc0-4386-96be-7c47ce016e88',\n",
       " '3bb5ae2d-1e92-49b5-bc93-0304bc9fe5b5',\n",
       " 'e02b4ea4-a8b0-47cb-a9f2-a5da78abffe2',\n",
       " 'b10efafa-20e9-4807-a01f-48249f9fb2fc',\n",
       " '0cb063db-d194-400c-827c-d4940ffee13e',\n",
       " '47d75903-b60e-4098-a525-4190c64d0d94',\n",
       " 'aa7438bf-2e7c-4347-ac59-b1bbee33e453',\n",
       " '675228f3-0197-4a7f-8e1d-5fecca6e9d85',\n",
       " '648c95e9-e501-479f-854b-43eae19a04ed',\n",
       " '821a5f76-3313-46a0-b832-bab40af0e0b3',\n",
       " 'e2b0401d-454b-42a3-b563-a8103519581c',\n",
       " 'b6abe948-6334-4648-8c7f-2a12d2b7be15',\n",
       " '8de73063-f397-4f3d-83ac-e8b75d5a3168',\n",
       " '7c9ed7b7-5ffa-48f5-9e7c-718ee666bc37',\n",
       " '35cd41ee-dfec-486e-b84c-070cbfb6a044',\n",
       " '549d13d4-a7bf-499d-934f-69d7a259e749',\n",
       " '187b2511-7738-4d29-8e7e-fb99cc7063b8',\n",
       " '7cbea0ec-1ca3-4b9d-9bf2-2d6ac6460edc',\n",
       " '9851e5e8-7885-4d1c-8719-f20960373cc9',\n",
       " '1006717e-e304-476a-8a53-974a23abd019',\n",
       " 'adbc5597-6cf9-43a7-824a-478b6a371b99',\n",
       " '11e8e275-df9f-4bf3-b6f9-6b82ade771f6',\n",
       " 'ecdbcb83-e7c7-41f1-83ce-5156ba4447df',\n",
       " '4f862c88-1607-4608-b35a-59b1e11c0ee0',\n",
       " '221da9ee-1735-4e9c-8797-2ed533759729',\n",
       " '60b7ac23-838a-49a1-9162-2683cfbe09c2',\n",
       " '049c8425-4aa6-43f3-94e2-bb4c5aab6d33',\n",
       " '0cba8c0d-3964-4734-a122-0b7d4090f6fc',\n",
       " '6fbf6fe5-704d-4883-9b35-cbf6f4e0155e',\n",
       " '967f6563-f3b0-4bce-abc4-7272539be0ce',\n",
       " 'aa9f09ff-074d-4965-bc15-b6851257b954',\n",
       " '2501b3ae-d791-4089-b3b1-2701c81ed389',\n",
       " 'aa99e3c0-35d2-4392-a90f-be6f786df469',\n",
       " '568a22dd-98e8-4a69-a731-c498a7add06e',\n",
       " 'ad42cd19-b725-4d95-8dbb-cefde22fb400',\n",
       " '7f49b18c-f7a2-4771-8009-1c4f56ab825e',\n",
       " '962483c6-bf76-4cfa-860f-662104f64b38',\n",
       " 'bd3a14f4-a5ef-4141-b7ae-3e7b9c6230b7',\n",
       " '0c419c52-39e2-48ed-b311-8a4752a0fc6e',\n",
       " '4230ad0d-dd9b-4fab-be5c-c876722d05ec',\n",
       " 'f60b2745-3dc6-4569-ab06-f9dc6ce272b5',\n",
       " 'cb32d89d-68c5-4e21-8941-242f4242d018',\n",
       " '234f9d77-ba02-4c37-a177-32e011f93553',\n",
       " '19be1cbb-e7dc-4e1c-ab63-d3e0e5446fd6',\n",
       " 'e36db39b-7887-41cf-a140-0bf09768b800',\n",
       " '51de813c-6d2f-46dd-919e-0cda8b7d9e8a',\n",
       " '8c0204f9-b691-460c-8f5e-f17ecd4d9e20',\n",
       " 'b23f652d-f019-404b-b309-97c4f8c0b7d5',\n",
       " '1498d13b-5c9e-4629-89f8-295a6bd095f7',\n",
       " '7e97c175-d7f0-4bdb-8ed5-46c5f7a3e117',\n",
       " '64bbfd90-b810-41c1-93e3-b09f9b31c1ef',\n",
       " '9dbd43de-03f7-44ae-90f0-bc82bab1a0e9',\n",
       " '3aadddf2-e3f5-457d-a109-4e869acdfc1c',\n",
       " '6573954a-3d9d-4830-bf41-224f4229491b',\n",
       " 'd618b2a4-c00f-4059-abf8-8d9120b42277',\n",
       " '2d7cbb4e-9525-451e-9450-12935cf6c85a',\n",
       " '04706390-33dc-4b40-9eaa-9521d3528573',\n",
       " '45301729-8a85-4767-9a87-00604fa8f14e',\n",
       " 'f214b942-b1f6-4796-b39b-f281184d274a',\n",
       " 'c9f77bf6-353b-47be-928f-4176b19ea829',\n",
       " '8bf3128e-abdb-4041-b9cd-0cd1258435d8',\n",
       " '98eb6fda-9aea-4c7d-a0e9-636407ae0d8d',\n",
       " '692d6a48-57e5-4317-af1c-b9662195ea9f',\n",
       " 'c761bf63-0db1-4dd5-b262-35c2ee927b3b',\n",
       " 'ce4a8fe3-0ba9-45e0-8957-9b16fed55504',\n",
       " '785c193e-247f-4831-b53e-622b473e9879',\n",
       " '12c21798-a2aa-4b94-b141-1b1d4953a255',\n",
       " '585accd5-97a2-418d-9637-5cbaf1d99628',\n",
       " '37b93e9b-cf5b-471d-83a7-95e394ae1d78',\n",
       " '0e9761cf-d3a5-4045-ad3f-26f400f57f0a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from uuid import uuid4\n",
    "\n",
    "#  Chroma 벡터 저장소 생성\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_02\",  # 데이터가 저장될 컬렉션 이름\n",
    "    embedding_function=embeddings_model,  # Ollama 임베딩 모델 사용\n",
    ")\n",
    "\n",
    "#  UUID를 활용하여 문서별 고유 ID 생성\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# 벡터 저장소에 문서 추가 (임베딩 자동 생성)\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# 1개의 유사한 문서를 검색 (기본 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agent가 뭐야?\", k=1)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\n",
      "\n",
      "* [유사도=0.832] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir  [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.855] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore [{'page': 10, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.858] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr [{'page': 8, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.859] tend to yield improved confidence calibration. For instance,\n",
      "MineLLM’s ECE achieves 0.32 and 0.30 pa [{'page': 5, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.860] sents visual observations and It represents task instructions\n",
      "and other types of language-based guid [{'page': 2, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 유사도 점수를 포함한 검색 (Query와 문서 간 거리 계산)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agent가 뭐야?\", k=5)\n",
    "\n",
    "print(\"여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\\n\")\n",
    "\n",
    "# 검색된 문서와 점수 출력\n",
    "for doc, score in results:\n",
    "    print(f\"* [유사도={score:.3f}] {doc.page_content[:100]} [{doc.metadata}]\")  # 상위 100자만 출력\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant 벡터 DB\n",
    "\n",
    "* Qdrant는 벡터 검색(Vector Search) 및 필터링을 지원하는 벡터 데이터베이스.\n",
    "* 메모리 내(in-memory) 실행 가능하며, 영구 저장(persistent storage)도 지원.\n",
    "* FAISS나 Chroma와 다르게, 메타데이터 기반 필터링 기능이 강력함.\n",
    "\n",
    "**Qdrant 벡터 저장소 구축**\n",
    "* `QdrantClient(\":memory:\")` → 인메모리 벡터 저장소 생성.\n",
    "* `create_collection()` → 1024차원의 벡터를 저장할 수 있는 컬렉션 생성.\n",
    "* `distance=Distance.COSINE` → 문서 간 코사인 유사도를 계산하여 가장 유사한 문서를 찾음.\n",
    "  \n",
    "**Qdrant 검색 기능**\n",
    "* `similarity_search(query, k=1)` → 가장 유사한 k개의 문서를 검색.\n",
    "* `similarity_search_with_score(query, k=5)` → 문서와 쿼리 간의 유사도 점수를 함께 반환.\n",
    "* `similarity_search(query, k=1, filter=...)` → 특정 조건을 만족하는 문서만 검색."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain_qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain에서 Qdrant 벡터 저장소 불러오기\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Qdrant 클라이언트 라이브러리 불러오기\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Qdrant 클라이언트 생성 (메모리 기반)\n",
    "# \":memory:\" 옵션을 사용하면 휘발성(In-Memory) 데이터베이스로 실행됨.\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "#  Qdrant 컬렉션 생성\n",
    "client.create_collection(\n",
    "    collection_name=\"test\",  # 저장소 이름\n",
    "    vectors_config=VectorParams(\n",
    "        size=1024,  # 벡터 차원 수 (사용하는 임베딩 모델에 맞춰야 함)\n",
    "        distance=Distance.COSINE  # 벡터 간 유사도 측정 방식 (코사인 거리 사용)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Qdrant 벡터 저장소 객체 생성\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,  # Qdrant 클라이언트\n",
    "    collection_name=\"test\",  # 컬렉션 이름\n",
    "    embedding=embeddings_model  # Ollama 임베딩 모델 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da36808d-fec5-4bd3-aa32-5fbe8b3b6e3b',\n",
       " '34b26675-c87c-49c4-a57d-db92449800f4',\n",
       " '2978d1a9-7afa-43bb-87d2-39b3a8f49d4f',\n",
       " '01ab23a7-e09c-4271-a4bd-1f9a82c0e462',\n",
       " '1fc0ce8b-65ea-4d1d-a24d-a1d1a8b9bb39',\n",
       " '3f694b1c-1097-4fc8-a8b0-ff2d366d93ef',\n",
       " 'fd6c7712-bd6b-45f7-b1f3-cbd9589bbfcd',\n",
       " 'fb10eb95-5f84-49f4-b6d9-d0b98005901b',\n",
       " '0fa10f84-e13d-49f2-a968-eb8bf0b47516',\n",
       " 'c3a4f21f-be6f-41ba-8f6f-551e34062aed',\n",
       " '66b833f2-6f03-43fa-9f77-663c462a953b',\n",
       " '5b73b63f-270c-4f99-b225-1fbedfc99f6d',\n",
       " '4180743a-77d1-4a3f-9031-cb310326daa0',\n",
       " 'b97b63f2-8802-4899-a9f1-c0a37ab84473',\n",
       " '4b9ffeb3-2873-499c-b177-d54c1206408d',\n",
       " '7aeefedc-2d75-4bed-89d2-2d5bd8c3822d',\n",
       " '2faa4fee-b58b-4c15-9b32-83e7786c9111',\n",
       " '5ad41bb7-5081-4f48-870f-ac27a6e34d52',\n",
       " 'a4cfe741-a355-40d8-a649-63c86cae48e6',\n",
       " '00c768e4-6f1a-4e11-b9c5-74c49bb1b901',\n",
       " 'a973d117-34d1-4911-83c3-40542eba90a3',\n",
       " '3cca1d72-e76c-44a6-9943-88853a7e03c7',\n",
       " 'e30ff073-5f14-4b54-b5a7-6fe09846d0db',\n",
       " '0815a734-e616-4013-b24f-a61ffdf5750f',\n",
       " '71c0c67e-ceff-4266-aefe-ea3f8d75a572',\n",
       " '0e3a7533-575d-47cd-a453-5da5bb1d7dcd',\n",
       " 'c981d292-f88f-4203-b7c9-1c5696a86aed',\n",
       " 'c44b421d-e6d7-452c-b6ce-56ba53f4a381',\n",
       " 'd6336bd6-286d-4ea9-b49d-5f33a1e749e1',\n",
       " 'd904e9f4-8318-4e4e-b0ed-87899f3ebd6f',\n",
       " '4844a70b-c2a1-4521-9b4f-c4b105e89f82',\n",
       " 'e00a8060-3878-44ca-8a27-1600bf0bb732',\n",
       " '2d81606c-1ec1-4d62-8cb8-fcff4af708e5',\n",
       " 'bce7be57-e5df-4869-a0fb-ffc66033888c',\n",
       " 'e2d276cd-1534-432e-9921-d12fc5013d8d',\n",
       " '7409e63d-f908-4cf6-8458-0ab7b856266f',\n",
       " '55928751-1107-460c-9dc2-784f599f9579',\n",
       " '135fd06c-1337-40b6-b195-94d07077023e',\n",
       " '9bb910f4-31fe-4ec8-b2fe-6b04288ddc7b',\n",
       " '97151fbf-0009-422a-b164-5558be215bd5',\n",
       " '0330e93d-ba77-48c6-a3e5-8b87f190dc9b',\n",
       " '4d508d65-c3f4-40a9-8e28-74ac4ee8fa1f',\n",
       " '833546c2-39e1-423e-9b48-020f4cc4f3ff',\n",
       " '2d6b6f85-0417-47cb-81e4-09bc640e6cae',\n",
       " '9ff0cefb-0f06-41e2-9086-0edc1721ed8a',\n",
       " '209eacf4-51db-4852-a666-299006ccee3a',\n",
       " '2814b883-4541-42db-ab58-c799b883c315',\n",
       " 'd5e74e15-204d-45fb-ac69-fc0721c137c6',\n",
       " '96d80b5c-7e22-42a3-af3b-b9c65c193671',\n",
       " '502fd863-9891-4f70-9d24-5d047eec72ee',\n",
       " 'b512a420-20fa-480a-a3e0-35ec95f1929d',\n",
       " 'a2319cf9-df3e-47b3-b78a-6d3a296c102a',\n",
       " '9a65c377-75da-4c69-948d-1cd252fc953d',\n",
       " '5be91ffe-27cd-492d-8132-c3d80842edf2',\n",
       " 'e6ee4081-ba96-49f3-8b5d-5e23fe9df8b8',\n",
       " '060f7df0-a2e1-4b97-ad50-0a5849b69cd3',\n",
       " '00565e0a-a9ef-4faa-892c-afcf2225ba02',\n",
       " 'fe53b18a-35c3-4cb0-acd3-1c24cfa47945',\n",
       " '57fec2c2-760d-49c4-b998-f82f037f9814',\n",
       " '56d4fe6b-edeb-46e8-a2da-9fcb1b1e5a28',\n",
       " '0552302a-34ec-48c4-aef5-4a9fb3ae6a8b',\n",
       " 'd09924a0-c0e6-429f-ad21-9d9e54875aac',\n",
       " 'd4c5d276-a321-4891-b8fc-29feb5198461',\n",
       " '86e4ae1a-de17-43dc-b4cf-c32a5ecf9b44',\n",
       " '6596e0f0-f6f1-4a95-86d7-82b65c234bad',\n",
       " '90579a3d-ca84-4715-b00c-2d66858264eb',\n",
       " '7da0977f-1abe-4fc6-a4c4-d49cc5c2e771',\n",
       " '5e9dee57-ec38-4d41-bce3-c8b53bf57b71',\n",
       " '209c1bb2-0763-42d9-bee4-a92b8781af54',\n",
       " '88af506c-3ea1-4435-b500-da65e5426889',\n",
       " 'fa16bf6a-ca3c-46c1-9f34-35d5a7b28abe',\n",
       " 'ebdfc993-d501-4444-bcb9-4f97b72e4f07',\n",
       " 'd5d0a8bd-6952-40c7-85e5-5f53041bb535',\n",
       " '4d519063-3df9-4b64-ae92-b561fd7c60fe',\n",
       " 'a182b113-a952-43b1-b774-01e9653bdae7',\n",
       " '03267ee4-0551-4634-b860-31fdc5d8477c',\n",
       " '2efb740f-ceb8-4020-965e-67c61fbbb50b',\n",
       " 'b4e83bc7-7179-48b2-b5a0-4b2c4b553e77',\n",
       " '5923b5a9-a7b3-41cd-a048-8e92e7c213d9',\n",
       " 'faeac1ef-d9bb-472d-a9ea-233a5bc8144c',\n",
       " '9178b675-415a-499f-ad2b-1f13524007a9',\n",
       " 'f8365669-81c8-411d-b93c-994a6506df03',\n",
       " '48fe60ba-45ee-45e5-9d39-1b20c7d43ed9',\n",
       " 'dc051752-4162-4f65-a628-74c8a71f2272',\n",
       " '949c7444-f066-454e-b64f-6b6ee8161fb3',\n",
       " 'ccf9988f-07b8-4274-a795-94362602f95b',\n",
       " '585e0673-e77d-4227-b88b-d23a61f833ef',\n",
       " 'a0ba7c0c-7446-41d8-a6f6-4b29cd4e269e',\n",
       " '19fd249b-e363-45e1-8e69-0c52aa8b594f',\n",
       " 'f3626950-ad43-4651-84b3-1d631b9bb95e',\n",
       " '2ebde9ec-d3bf-443a-801c-e5548a9703be',\n",
       " 'b729e381-1eb9-4997-8e77-1cd7fd79034b',\n",
       " '8a305e94-9f4d-4232-ac0d-9a74d4c0b9d5',\n",
       " '875907fd-ed1a-4d9c-b2f6-318e64dcfcdb',\n",
       " 'd8722449-9357-4c06-b7fa-aa5aea9de11f',\n",
       " '44d2c2fa-8f6f-45c3-b8ab-573b1dd0e905',\n",
       " '022ee1ff-5bf9-4f71-9d21-905236604f63',\n",
       " '1e78524b-1932-43a9-b960-f7a9e8adcdf6',\n",
       " '8fe72f72-6401-4167-9b5e-e859d531a3cb',\n",
       " '443f92e0-6a45-4c44-ace4-70f82848399e']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 ID를 생성하기 위해 UUID 사용\n",
    "from uuid import uuid4\n",
    "\n",
    "# 각 문서에 대해 고유한 ID 생성\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# 벡터 저장소에 문서 추가\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': 'da36808d-fec5-4bd3-aa32-5fbe8b3b6e3b', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "# 1개의 유사한 문서를 검색 (기본 검색)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agent가 뭐야?\", k=1)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., “I am 70%\n",
      "confident I craft a wooden table”). 3) Interactive depen-\n",
      "dencies, where the agent’s actions directly influence the\n",
      "environment, which in turn affects subsequent decisions,\n",
      "requiring ongoing adjustments to confidence estimates as\n",
      "tasks progress. 4) Finally, while state-of-the-art embodied\n",
      "agents leverage proprietary Large Language Models (LLMs)\n",
      "and Vision-Language Models (VLMs) for their strong mul-\n",
      "timodal understanding and reasoning capabilities (Wang\n",
      "et al., 2023a; Qin et al., 2024; Zhu et al., 2023), these of-\n",
      "ten lack access to internal token likelihoods or probabilistic\n",
      "outputs, making traditional confidence estimation methods\n",
      "ineffective (Kumar et al., 2023; Chen et al., 2024b).\n",
      "To address these challenges, we present the first system-\n",
      "atic approach that enables LLM/VLM-powered embodied\n",
      "agents to assess and articulate their confidence across multi- [{'source': 'data/arxiv_paper.pdf', 'page': 1, '_id': '3f694b1c-1097-4fc8-a8b0-ff2d366d93ef', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "#  특정 필터링 조건을 적용한 검색\n",
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"Embodied_agent가 뭐야?\",\n",
    "    k=1,\n",
    "    filter=models.Filter(must=[models.FieldCondition(\n",
    "        key=\"metadata.page\",  # 특정 필드(예: page) 기반 필터링\n",
    "        match=models.MatchValue(value=1),  # page=1인 문서만 검색\n",
    "    )])\n",
    ")\n",
    "\n",
    "# 필터링된 문서 출력\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기서의 score는 query와 문서의 거리를 나타내기 때문에, 낮을수록 유사합니다.\n",
      "\n",
      "* [유사도=0.584] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': 'da36808d-fec5-4bd3-aa32-5fbe8b3b6e3b', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.573] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agents’ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using bayesian binning. In\n",
      "AAAI Conference on Artificial Intelligence, 2015.\n",
      "Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y ., Ha-\n",
      "jishirzi, H., Singh, S., and Fox, R. Do embodied agents\n",
      "dream of pixelated sheep: Embodied decision making\n",
      "using language guided world modelling. In International\n",
      "Conference on Machine Learning, 2023.\n",
      "Okoli, C. Inductive, abductive and deductive theorising.\n",
      "International Journal of Management Concepts and Phi-\n",
      "losophy, 2023. [{'source': 'data/arxiv_paper.pdf', 'page': 10, '_id': '90579a3d-ca84-4715-b00c-2d66858264eb', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.571] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment is critical. However, the\n",
      "reliance on large pre-trained models raises concerns about\n",
      "energy efficiency and ethical considerations in high-stakes\n",
      "applications, which warrant further exploration.\n",
      "References\n",
      "Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,\n",
      "Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-\n",
      "ravi, A., Acharya, U. R., et al. A review of uncertainty\n",
      "quantification in deep learning: Techniques, applications\n",
      "and challenges. Information Fusion, 2021. [{'source': 'data/arxiv_paper.pdf', 'page': 8, '_id': '5be91ffe-27cd-492d-8132-c3d80842edf2', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.570] tend to yield improved confidence calibration. For instance,\n",
      "MineLLM’s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVE’s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence judgments.\n",
      "So, How Effectively Can Embodied Agents Express Con-\n",
      "fidence in Dynamic Embodied Tasks? While embodied\n",
      "agents can convey confidence to some extent, their effec-\n",
      "tiveness depends on how well they integrate reasoning, un-\n",
      "certainty assessment, and environmental interactions. The\n",
      "findings reveal that embodied confidence elicitation remains\n",
      "a challenging problem, requiring a careful balance between\n",
      "general-purpose reasoning and task-specific specialization. [{'source': 'data/arxiv_paper.pdf', 'page': 5, '_id': '9bb910f4-31fe-4ec8-b2fe-6b04288ddc7b', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [유사도=0.570] sents visual observations and It represents task instructions\n",
      "and other types of language-based guidance. For a given\n",
      "task T , the agent operates under a policy π : I → Athat\n",
      "maps input I to actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score c ∈ [0, 1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.\n",
      "The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks. This requires not only\n",
      "detecting uncertain aspects of the agent’s perception, reason-\n",
      "ing, or actions but also ensuring that confidence estimates\n",
      "are refined and reliable under dynamic multimodal condi-\n",
      "tions. To address this, we propose an embodied confidence\n",
      "estimation framework centered around Elicitation Modules\n",
      "that facilitates confidence elicitation at two critical points [{'source': 'data/arxiv_paper.pdf', 'page': 2, '_id': '2faa4fee-b58b-4c15-9b32-83e7786c9111', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 유사도 점수를 포함한 검색 (Query와 문서 간 거리 계산)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agent가 뭐야?\", k=5)\n",
    "\n",
    "print(\"여기서의 score는 query와 문서의 유사한 정도를 나타내기 때문에, 높을수록 유사합니다.\\n\")\n",
    "\n",
    "# 검색된 문서와 점수 출력\n",
    "for doc, score in results:\n",
    "    print(f\"* [유사도={score:.3f}] {doc.page_content[:1000]} [{doc.metadata}]\")  # 첫 1000자 출력\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
