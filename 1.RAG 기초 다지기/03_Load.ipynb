{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "## ğŸ“š ê°•ì˜ ê°œìš” (Overview)\n",
    "\n",
    "ì´ ê°•ì˜ì—ì„œëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ê³ , ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ(Load)í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì—ì„œ ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í• ì§€, ê·¸ë¦¬ê³  ì–´ë–¤ ë²¡í„° ì €ì¥ì†Œë¥¼ í™œìš©í• ì§€ê°€ ê²€ìƒ‰ ë° ì‘ë‹µ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°•ì˜ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì—¬ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨: \n",
    "* [OpenAI ì„ë² ë”© ëª¨ë¸ í™œìš©í•˜ê¸°](#openai-ì„ë² ë”©-ëª¨ë¸-í™œìš©í•˜ê¸°)\n",
    "* [Ollama Embedding ëª¨ë¸ í™œìš©í•˜ê¸°](#ollama-embedding-ëª¨ë¸-í™œìš©í•˜ê¸°)\n",
    "* [FAISSë¡œ ì„ë² ë”© ë²¡í„° ì €ì¥í•˜ê¸°](#faissë¡œ-ì„ë² ë”©-ë²¡í„°-ì €ì¥í•˜ê¸°)\n",
    "* [Chroma ë²¡í„° DB](#chroma-ë²¡í„°-db)\n",
    "* [Qdrant ë²¡í„° DB](#qdrant-ë²¡í„°-db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •í•˜ê¸° (.env íŒŒì¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ì—¬ê¸°ì— ì…ë ¥í•´ì£¼ì„¸ìš”!)\n",
    "import os\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "os.environ[\"API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI ì„ë² ë”© ëª¨ë¸ í™œìš©í•˜ê¸°\n",
    "\n",
    "`text-embedding-3-small`: OpenAIì—ì„œ ì œê³µí•˜ëŠ” ìµœì‹  ì„ë² ë”© ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ë¹ ë¥´ê³  ê°€ë²¼ìš´ ì„ë² ë”©ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¬¸ì„œ ì„ë² ë”© vs ì§ˆì˜ ì„ë² ë”©\n",
    "* `embed_documents()` â†’ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ í•œ ë²ˆì— ì„ë² ë”© (ë¬¸ì„œ ê²€ìƒ‰ ë“±ì— í™œìš©).\n",
    "* `embed_query()` â†’ ì§ˆì˜(Query)ë¥¼ ì„ë² ë”© (ì§ˆë¬¸-ì‘ë‹µ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰í•  ë•Œ í™œìš©)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì„œ(ë¬¸ì¥) ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”©)\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ì„ë² ë”©ëœ ë¬¸ì„œ ê°œìˆ˜ì™€ ê°œë³„ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ì¶œë ¥\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.010634176433086395,\n",
       " -0.01016946416348219,\n",
       " -0.0020040736999362707,\n",
       " 0.023065242916345596,\n",
       " -0.026829415932297707]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§ˆì˜(Query) ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5] #ì²˜ìŒ 5ê°œì˜ ê°’ë§Œ ì¶œë ¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])# ê°œë³„ ë¬¸ì¥ì˜ ì„ë² ë”© ì°¨ì› í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Embedding ëª¨ë¸ í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ë³„ë¡œ ì €ì¥í•˜ëŠ” ê³¼ì •\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"data/arxiv_paper.pdf\"\n",
    ")\n",
    "\n",
    "# PDF ë¡œë” ê°ì²´ ìƒì„±\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDFì˜ ê° í˜ì´ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "pages = []\n",
    "\n",
    "# PDFë¥¼ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ë³„ë¡œ ì €ì¥\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "ronment, we show that structured reasoning ap-\n",
      "proaches, such as Chain-of-Thoughts, improve\n",
      "confidence calibration. However, our findings\n",
      "also reveal persistent challenges in distinguishing\n",
      "uncertainty, particularly under abductive settings,\n",
      "underscoring the need for more sophisticated em-\n",
      "bodied confidence elicitation methods.\n",
      "1. Introduction\n",
      "In complex embodied environments, success depends not\n",
      "only on what an agent knows but also on how well it un-\n",
      "derstands and communicates uncertainty. Whether navi-\n",
      "gating a cluttered space, interacting with objects, or plan-\n",
      "ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # í•˜ë‚˜ì˜ ì²­í¬ í¬ê¸°ë¥¼ 1000ìë¡œ ì„¤ì •\n",
    "    chunk_overlap=200,     # ì²­í¬ ê°„ 200ì ê²¹ì¹˜ê²Œ ì„¤ì • (ë¬¸ë§¥ ìœ ì§€ ëª©ì )\n",
    "    length_function=len,   # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (len ì‚¬ìš©)\n",
    "    is_separator_regex=False  # separatorë¥¼ ì •ê·œì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ì²˜ë¦¬\n",
    ")\n",
    "\n",
    "# PDFì—ì„œ ë¡œë“œí•œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• \n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\")# ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°\n",
    "print(texts[0].page_content)# ì²« ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš© \n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\")# ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°\n",
    "print(texts[1].page_content)# ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# \"bge-m3\" ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
    "embeddings_model=OllamaEmbeddings(model=\"bge-m3\")\n",
    "# ì²­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "embeddings = embeddings_model.embed_documents([i.page_content for i in texts])\n",
    "\n",
    "len(embeddings[0])# ìƒì„±ëœ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISSë¡œ ì„ë² ë”© ë²¡í„° ì €ì¥í•˜ê¸°\n",
    "\n",
    "FAISS(Facebook AI Similarity Search)\n",
    "*  ëŒ€ëŸ‰ì˜ ë²¡í„° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "* ë¬¸ì„œ ê²€ìƒ‰, ì¶”ì²œ ì‹œìŠ¤í…œ, ì´ë¯¸ì§€ ê²€ìƒ‰ ë“±ì—ì„œ í™œìš©ë¨.\n",
    "* ë²¡í„°ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•\n",
    "* `FAISS.from_documents()` â†’ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ FAISSì— ì €ì¥.\n",
    "* `embedding=embeddings_model` â†’ Ollamaì˜ `bge-m3` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±.\n",
    "\n",
    "FAISS ê²€ìƒ‰ ë°©ì‹\n",
    "* `similarity_search(query, k=1)` â†’ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
    "* `similarity_search(query, k=10, filter={\"page\": 0})` â†’ íŠ¹ì • ì¡°ê±´(ì˜ˆ: page=0)ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰.\n",
    "* `similarity_search_with_score(query, k=10)` â†’ ê²€ìƒ‰ëœ ë¬¸ì„œì™€ í•¨ê»˜ ìœ ì‚¬ë„ ì ìˆ˜(ê±°ë¦¬) ì¶œë ¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ocling (c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ocling (c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ocling (c:\\Users\\user\\anaconda3\\envs\\test_venv\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "embedchain 0.1.125 requires pypdf<6.0.0,>=5.0.0, but you have pypdf 4.3.1 which is incompatible.\n",
      "embedchain 0.1.125 requires rich<14.0.0,>=13.7.0, but you have rich 13.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²­í¬ì˜ ìˆ˜: 99\n",
      "ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: 99\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#  FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± (OllamaEmbeddingsì„ í™œìš©)\n",
    "# ì•ì„œ ìƒì„±í•œ ì²­í¬(`texts`)ì™€ ì„ë² ë”© ëª¨ë¸(`embeddings_model`)ì„ ì´ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings_model # Ollama ì„ë² ë”© ëª¨ë¸\n",
    ")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ í¬ê¸° í™•ì¸\n",
    "print(f\"ì²­í¬ì˜ ìˆ˜: {len(texts)}\") # ì´ ì²­í¬ëœ ë¬¸ì„œ ê°œìˆ˜\n",
    "print(f\"ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {vector_store.index.ntotal}\")# ì´ ì²­í¬ëœ ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n"
     ]
    }
   ],
   "source": [
    "# 1ê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (ê¸°ë³¸ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agentê°€ ë­ì•¼?\",k=1)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n",
      "tion and decision-making processes (Ren et al., 2023; Liang\n",
      "et al., 2024). For humans, this instinctive ability to express\n",
      "and calibrate uncertainty is fundamental to decision-making\n",
      "and social interaction. As AI systems are increasingly de-\n",
      "ployed in high-stakes contexts such as autonomous driving\n",
      "or healthcare, they must also acquire this crucial skill.\n",
      "*Preprint. Work in progress.\n",
      "Embodied Environment\n",
      "Elicitation Module\n",
      "Are you sure about your\n",
      "next action ?\n",
      "Elicitation Module\n",
      "Are you sure about\n",
      "what you  see?Elicitation\n",
      "Policies\n",
      "Execution\n",
      "PoliciesElicitation\n",
      "Policies\n",
      "Execution\n",
      "Policies\n",
      "Perception StageAction StageFigure 1. Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies andExecution Policies , which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ì • í˜ì´ì§€ì— ëŒ€í•œ í•„í„°ë§ ê²€ìƒ‰ (page=0ì¸ ë¬¸ì„œì—ì„œ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agentê°€ ë­ì•¼?\",k=10,filter={\"page\": 0})\n",
    "\n",
    "# í•„í„°ë§ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "* [ìœ ì‚¬ë„=0.848845] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir  [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.876677] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr [{'source': 'data/arxiv_paper.pdf', 'page': 8}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.892435] tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 pa [{'source': 'data/arxiv_paper.pdf', 'page': 5}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.892554] Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes [{'source': 'data/arxiv_paper.pdf', 'page': 4}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.897213] and other types of language-based guidance. For a given\n",
      "taskT, the agent operates under a policy Ï€:I [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.897255] Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., â€œI am 70%\n",
      " [{'source': 'data/arxiv_paper.pdf', 'page': 1}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.906592] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore [{'source': 'data/arxiv_paper.pdf', 'page': 10}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.930120] Confidence Elicitation in Embodied Agents\n",
      "language understanding. Trained on a vast dataset of 500,0 [{'source': 'data/arxiv_paper.pdf', 'page': 16}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.934772] ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact wit [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.938182] (Â§3.3) refine and expand confidence assessment through scenario reinterpretation, action sampling, a [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (Queryì™€ ë¬¸ì„œ ê°„ ê±°ë¦¬ ê³„ì‚°)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied Agentê°€ ë­ì•¼?\",k=10)\n",
    "\n",
    "print(\"ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì ìˆ˜ ì¶œë ¥\n",
    "for doc, score in results:\n",
    "    print(f\"* [ìœ ì‚¬ë„={score:3f}] {doc.page_content[:100]} [{doc.metadata}]\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma ë²¡í„° DB\n",
    "\n",
    "\n",
    "* ChromaDBëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ, í…ìŠ¤íŠ¸ ê²€ìƒ‰ê³¼ ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì— í™œìš©ë¨.\n",
    "* FAISSëŠ” ë©”ëª¨ë¦¬ ë‚´(in-memory)ì—ì„œ ì‘ë™í•˜ì§€ë§Œ, ChromaëŠ” ì˜êµ¬ ì €ì¥(Persistent Storage) ê°€ëŠ¥.\n",
    "* ChromaëŠ” ì¿¼ë¦¬ ì‹œ ë” ë‹¤ì–‘í•œ í•„í„°ë§ê³¼ ì¡°í•©ì´ ê°€ëŠ¥í•¨.\n",
    "\n",
    "**Chroma ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•**\n",
    "* `Chroma(collection_name=\"test_01\", embedding_function=embeddings_model)`â†’ \"test_01\"ì´ë¼ëŠ” ì»¬ë ‰ì…˜ì„ ìƒì„±í•˜ê³ , Ollama ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥.\n",
    "* `add_documents(documents=texts, ids=ids)`â†’ ì„ë² ë”©ì„ ìƒì„±í•œ í›„ ChromaDBì— ì €ì¥.\n",
    "\n",
    "**ìœ ì‚¬ë„ ê²€ìƒ‰ ë°©ì‹**\n",
    "* `similarity_search(query, k=1)` â†’ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
    "* `similarity_search_with_score(query, k=5)` â†’ ë¬¸ì„œì™€ ì¿¼ë¦¬ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í•¨ê»˜ ë°˜í™˜.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -qU chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e36a3340-2299-435e-bbe0-980b9f425a41',\n",
       " '927df71a-4ec1-4fc1-ad35-591db7f6b35e',\n",
       " 'f7762f29-5604-4668-b6b8-1d2cbd262922',\n",
       " '1a0a1b08-0bc2-4a5d-be57-75e960e59062',\n",
       " '64c232a2-3f9e-4f33-91c6-5df21c6b82c6',\n",
       " '0479c143-2250-4a96-ae1f-05ae58d47566',\n",
       " 'b838def6-ea76-4e22-a5ab-e53b82d778db',\n",
       " '02de8bab-82b0-476c-adcd-afba5eabd2b2',\n",
       " '64ded0df-cd27-4e41-9b89-615ab300fc41',\n",
       " '57f86230-afd3-43e1-ac63-7b84db5ad8ce',\n",
       " '468a59c9-aae2-492e-a1e8-727cb31d2d9e',\n",
       " '47aa8a17-3448-4f3b-abfe-7cb52a7dd9f7',\n",
       " 'faba007d-fafd-4706-b66f-aae4b4ebcb66',\n",
       " '1034752d-086d-4301-809b-948df36c3421',\n",
       " 'ee3a30d2-0c50-4404-b8b4-f0b58899de1f',\n",
       " 'ce8162d5-2885-412d-ac62-1d2604d4f382',\n",
       " '621177aa-c822-4243-a2cf-f6e52f7216e8',\n",
       " '5f1e09fe-aa11-4487-9e77-6aacbbdcdf66',\n",
       " 'cf823ad0-ee7d-42f5-85db-4e668a308d7e',\n",
       " 'c9b8da22-1f75-4f66-b400-4e2afe2af8fd',\n",
       " 'e71d1737-1dc2-47ac-8815-88cb88148ba7',\n",
       " '62ee0eca-8619-4d1f-884c-4b8624039a90',\n",
       " 'e0993fa8-eda7-47ee-8a03-c477848c51a1',\n",
       " 'df6774bf-85ed-467f-a684-af67f2cc8ce2',\n",
       " '818c8370-869c-4232-a5c3-1e3b4ca42e77',\n",
       " '9786afa2-fad9-4b00-9260-8137ea2a8601',\n",
       " '020302a0-3f53-4db5-a735-fdff6b37e0d9',\n",
       " '2693a543-01e3-40f5-a490-f97cf7da79ae',\n",
       " '4fe3a9ec-6b64-4739-8cb8-90055335f9c8',\n",
       " 'b4e8d249-6f92-4fd3-bc2b-7e9f4ddc2c93',\n",
       " '753cc5d7-8db1-44b0-954d-e7f3622b95b6',\n",
       " 'b6084850-8da0-4d82-b7e7-36b83fdfd87e',\n",
       " 'e636516f-4074-4537-b19a-add255cbad57',\n",
       " '74d85891-3847-4657-a1ca-050e7df72157',\n",
       " 'f6c3afa2-79b7-4dd3-8d25-8a0e3d62c5fd',\n",
       " 'd8bd663e-6d58-4109-9cf4-b2764b46c4d3',\n",
       " '31ddfb40-2253-4829-bc28-f997474d7afe',\n",
       " '1cc3d3b2-8345-425a-8af4-b370c047657f',\n",
       " 'd0199790-ff97-474f-847c-df6f3c5a7cd8',\n",
       " '69c0317f-3046-4511-b29d-8c5de9844671',\n",
       " '97facb1b-adc3-4561-8a99-d42fca665800',\n",
       " '64af178c-de08-45a5-b085-20c7b2eac667',\n",
       " '88ca3244-881f-4900-a25f-960a02cfb504',\n",
       " 'bdc62767-5e8b-4b89-b423-dc0ff343d627',\n",
       " '022327b2-956e-45fc-962e-f46cc2cc2a77',\n",
       " '0792c32d-0387-43a5-9d93-74e2a51e9966',\n",
       " '8a6c3fb1-de99-4930-8416-acbba536f640',\n",
       " 'a4f44d2f-ed72-42e5-a78b-43000e602a0a',\n",
       " '715c4537-9759-4384-8831-3d640c6b8bb7',\n",
       " '3e649ccc-5a72-4bb9-a899-1002fc631779',\n",
       " '081ca574-723a-463f-98bc-8b2064fd48c7',\n",
       " '0cb052bb-a4fe-47e3-93f6-b0f7447e47c0',\n",
       " '424af106-5559-4bc9-8727-9e122e440cdb',\n",
       " '727b455b-b407-4f43-b006-7e22707d1b51',\n",
       " 'bec09eac-bfc1-422d-bbb6-fcc86df4b357',\n",
       " '62a19535-735f-427e-9d01-ad61f5786d72',\n",
       " '9e9d739d-ccd4-460c-906e-604c6b1d2e73',\n",
       " 'b9bab06d-4045-4ae1-99f4-59c043b51578',\n",
       " 'b860d153-7071-40c6-9b35-cb39923a2dd2',\n",
       " 'e097c521-7d50-49be-85b5-c853b59401b3',\n",
       " 'a2522779-3800-4637-942f-d2859ccf7e93',\n",
       " '9162f66f-4be7-4ebe-b30c-4ae0545c0e79',\n",
       " 'd7c14ac6-514c-456c-81ee-76498a288b4b',\n",
       " 'd2c3c35b-aefa-4d81-b54e-761f61242209',\n",
       " 'a81e49bd-876f-409b-974c-11f516128e8a',\n",
       " '7407865f-21a7-4b45-8b0a-c77a5357c92a',\n",
       " '78343d1c-c115-4204-be6d-c0ee78b9b162',\n",
       " '4997813b-bc23-4c47-9a3f-bc595ecc0044',\n",
       " '8b31cbbf-6026-4ad5-9c78-79378c5b0f60',\n",
       " '5a9747dc-ce13-4ed3-aabe-9ca99c77982d',\n",
       " 'b97cf381-5bcf-4c36-815a-eb2b78d841d2',\n",
       " '7fc15e79-3ed8-480a-8a92-678235152abe',\n",
       " '64208e8c-d8a7-417e-9dea-3e129153db02',\n",
       " '4e6ce1fe-42b5-4617-be7b-edbd367ff24d',\n",
       " '06b004df-7e4c-4ff2-b554-375acdd5dec6',\n",
       " '3c03a694-4312-41f7-8e4f-a93b75b2afd5',\n",
       " '8425dbdd-391d-44e9-a06e-021a83855f22',\n",
       " '4196964c-a23b-4af9-aa65-7682c0e5f669',\n",
       " 'ef747e8d-d667-4f68-94d0-a2f309e45388',\n",
       " '2b6b4b63-5099-47f3-a018-2a909f379610',\n",
       " '7bead8c8-d2d7-498e-ab1c-9d3f84e5bf23',\n",
       " '051a3c4e-9306-401c-bf03-afdb21fc37c5',\n",
       " '5f51e253-7468-4ffe-b4ab-869588341ee7',\n",
       " '0e30a0e6-39f7-4a7e-a3af-9829413f51df',\n",
       " '21ba1464-f8ef-4072-aee6-7096b61f7c59',\n",
       " '1d90cbb7-2030-4b30-bd34-0b6d2baed8b4',\n",
       " 'e49b227c-a3cd-4fcc-a920-cdce1a01f415',\n",
       " 'ad499c0d-aaf3-4527-847c-7c433f408c6d',\n",
       " '49274f09-26dc-4702-84bb-f2916728e46d',\n",
       " 'c01744d2-ac6b-4c27-b09e-bb5b959725ba',\n",
       " '46fc571a-7e46-4aec-8188-7ff5ef16b26f',\n",
       " 'c98e2b06-0efa-4a47-988a-165efd15b06c',\n",
       " '13afd048-d8f2-4fb0-a278-d665ddae7055',\n",
       " '5659eb91-14b9-417d-9051-441f58ca7e83',\n",
       " 'a9983cd7-6e03-4add-ae61-a9bafc74c536',\n",
       " '21d40e2c-a977-46ed-bd56-c768c9e0a5c6',\n",
       " 'f8012018-3d12-4753-92f4-c8c06b7ac409',\n",
       " 'da4f2b28-a1d0-45ce-9af0-0c2e1fcda0b1',\n",
       " '9e716822-5aae-4aa2-86be-b54c2504c1fb',\n",
       " 'a48d62fe-eb76-4a72-addf-e442f7d0ec06']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from uuid import uuid4\n",
    "\n",
    "#  Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_01\",  # ë°ì´í„°ê°€ ì €ì¥ë  ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    embedding_function=embeddings_model,  # Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "#  UUIDë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œë³„ ê³ ìœ  ID ìƒì„±\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€ (ì„ë² ë”© ìë™ ìƒì„±)\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# 1ê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (ê¸°ë³¸ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agentê°€ ë­ì•¼?\", k=1)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "* [ìœ ì‚¬ë„=0.830] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir  [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.855] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore [{'page': 10, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.857] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr [{'page': 8, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.859] tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 pa [{'page': 5, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.859] sents visual observations and It represents task instructions\n",
      "and other types of language-based guid [{'page': 2, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (Queryì™€ ë¬¸ì„œ ê°„ ê±°ë¦¬ ê³„ì‚°)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agentê°€ ë­ì•¼?\", k=5)\n",
    "\n",
    "print(\"ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì ìˆ˜ ì¶œë ¥\n",
    "for doc, score in results:\n",
    "    print(f\"* [ìœ ì‚¬ë„={score:.3f}] {doc.page_content[:100]} [{doc.metadata}]\")  # ìƒìœ„ 100ìë§Œ ì¶œë ¥\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant ë²¡í„° DB\n",
    "\n",
    "* QdrantëŠ” ë²¡í„° ê²€ìƒ‰(Vector Search) ë° í•„í„°ë§ì„ ì§€ì›í•˜ëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤.\n",
    "* ë©”ëª¨ë¦¬ ë‚´(in-memory) ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ì˜êµ¬ ì €ì¥(persistent storage)ë„ ì§€ì›.\n",
    "* FAISSë‚˜ Chromaì™€ ë‹¤ë¥´ê²Œ, ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ê¸°ëŠ¥ì´ ê°•ë ¥í•¨.\n",
    "\n",
    "**Qdrant ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•**\n",
    "* `QdrantClient(\":memory:\")` â†’ ì¸ë©”ëª¨ë¦¬ ë²¡í„° ì €ì¥ì†Œ ìƒì„±.\n",
    "* `create_collection()` â†’ 1024ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì»¬ë ‰ì…˜ ìƒì„±.\n",
    "* `distance=Distance.COSINE` â†’ ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŒ.\n",
    "  \n",
    "**Qdrant ê²€ìƒ‰ ê¸°ëŠ¥**\n",
    "* `similarity_search(query, k=1)` â†’ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
    "* `similarity_search_with_score(query, k=5)` â†’ ë¬¸ì„œì™€ ì¿¼ë¦¬ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í•¨ê»˜ ë°˜í™˜.\n",
    "* `similarity_search(query, k=1, filter=...)` â†’ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -qU langchain_qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChainì—ì„œ Qdrant ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Qdrant í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë©”ëª¨ë¦¬ ê¸°ë°˜)\n",
    "# \":memory:\" ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ íœ˜ë°œì„±(In-Memory) ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì‹¤í–‰ë¨.\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "#  Qdrant ì»¬ë ‰ì…˜ ìƒì„±\n",
    "client.create_collection(\n",
    "    collection_name=\"test\",  # ì €ì¥ì†Œ ì´ë¦„\n",
    "    vectors_config=VectorParams(\n",
    "        size=1024,  # ë²¡í„° ì°¨ì› ìˆ˜ (ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ì— ë§ì¶°ì•¼ í•¨)\n",
    "        distance=Distance.COSINE  # ë²¡í„° ê°„ ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ (ì½”ì‚¬ì¸ ê±°ë¦¬ ì‚¬ìš©)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Qdrant ë²¡í„° ì €ì¥ì†Œ ê°ì²´ ìƒì„±\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,  # Qdrant í´ë¼ì´ì–¸íŠ¸\n",
    "    collection_name=\"test\",  # ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    embedding=embeddings_model  # Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b840b5af-51c4-4073-b436-ed7e049b0bef',\n",
       " 'e11116e7-534f-4b44-8895-d3cac16c0cc3',\n",
       " 'bb0c0ad4-aa56-4859-a340-b1ba64601af4',\n",
       " '8df32e68-9a66-41c6-9083-ab60fa855898',\n",
       " '8605f401-379a-4099-bfb1-332b497b9a5a',\n",
       " 'f13a3fd5-e5e4-468f-bab1-f4be3bb35cc9',\n",
       " '83088019-cc7c-492b-81c0-60bf2d27180f',\n",
       " 'e41d6a63-a2b6-4e0a-9827-5a9d0f13f1a8',\n",
       " '83710cbf-5a27-42d3-9d5f-aed608eac089',\n",
       " '1be9b921-aeb4-41dc-a59a-e551171b756e',\n",
       " '207e508e-a8b8-495e-bbf4-45aaeac333c3',\n",
       " '018eef8c-7555-4c32-b8ba-32c06d1adcd4',\n",
       " '1f7f02d1-93ba-433f-a8d4-ea7f1df22c52',\n",
       " 'ec5cda78-489f-4939-b945-80495f9e94fc',\n",
       " '6cd32a15-6c56-4e43-b892-df19da068eb0',\n",
       " '8a07c46d-c59f-4358-a1d7-50411fa5c927',\n",
       " 'af88d6e9-da5e-4580-a647-eb2c37ca3cd7',\n",
       " 'c48a8721-12dc-4051-8d0a-f00426c592e5',\n",
       " '68cbc027-852c-4f49-859e-233261e103e4',\n",
       " '644b20fe-1e86-4738-86f6-1696de289a82',\n",
       " '0a9b490f-b184-45a5-8733-297f1e1718c9',\n",
       " 'de7da756-6634-4512-af78-6ec08e9b7b00',\n",
       " 'e15fa4c4-d95d-4fea-ab5a-c022786482b7',\n",
       " 'd95c8f44-a5cd-456a-b408-f8ed3fa9e961',\n",
       " '0434c7bf-fe92-492d-9194-4efbb5cf0d17',\n",
       " '4f480437-58b7-45c9-b895-7098f69a42fd',\n",
       " '9d9897f8-4bac-4881-b41a-60e9e0634f58',\n",
       " '8de7bc5d-7cc7-42e3-b41f-21d2a7cc9ea4',\n",
       " '45ea2c5d-b1d3-485d-82eb-c0f83069bd13',\n",
       " 'a30ff1ac-c925-4571-86af-969e070acb34',\n",
       " '89399203-8556-47f8-9a1a-99bf1a2ca5b4',\n",
       " '49a44f47-db39-4392-b09b-8efae05e9b31',\n",
       " 'a99ba258-88bd-4c3e-91f8-658b9f85c53c',\n",
       " '95454e12-9831-47cf-ac1d-26b295f231b6',\n",
       " 'fc4d2217-222f-4c8d-bb5a-ce4c0f5a1c66',\n",
       " '29a0c4d8-a194-461a-ad99-b04b48ee87a3',\n",
       " '08f65390-f695-4824-b186-86654c203b5b',\n",
       " '8afbc200-034c-434d-8cc5-be2802991b27',\n",
       " '47fb0a27-d039-43f7-9458-e5715b3d9245',\n",
       " '3ac2b721-90ce-4940-8973-e1a2b853e0e1',\n",
       " '12fd32a2-f395-452c-b583-dfd694ca45f3',\n",
       " 'c2d1ac81-7331-4d8f-b0f2-f778db5f8082',\n",
       " 'f4cead39-2c68-445b-ba8a-bf4ad848ba27',\n",
       " 'a3000421-77ee-41dc-ae67-84a806c014e1',\n",
       " '63bb6213-f3bc-484d-aa63-b01e914b1edc',\n",
       " '86fe75e9-2b00-4bdb-bcca-6ddad140f85e',\n",
       " '99664e11-4c12-4d7f-8d28-be62a9c3e82d',\n",
       " 'c978bb34-3c0f-4216-9c29-be9f17551cc2',\n",
       " '39cda4b3-bcbd-4a2a-8b66-088e8db1c887',\n",
       " '889a83d3-3794-4a67-8352-7799555c07f0',\n",
       " '590477e6-6ae7-4f20-8612-652545283a1d',\n",
       " '7b52856d-4468-40f5-8438-96218c46e18c',\n",
       " 'a4f5fb23-72ea-46c1-82fe-a879969e1992',\n",
       " '92137943-00e7-4a5b-85fd-1e27d9bbe6a8',\n",
       " '9081d4b0-35b5-472e-9db3-2e9478d09c19',\n",
       " '44ebf576-c708-4eb3-9fea-b3a1527320d0',\n",
       " '4e48e514-164e-4ada-8024-34c5f58de71f',\n",
       " 'df12dc67-1235-4efa-867d-12b9e52554c8',\n",
       " '9c4d3483-9d3b-4461-a483-bae618bb93ee',\n",
       " '343e1e81-ee1b-438a-b8e4-cc01d22b4c28',\n",
       " '1eca483c-38f0-4c45-9510-6f6a18ef838d',\n",
       " 'e07223a8-5cb9-4492-884a-83f3704d01c0',\n",
       " 'd20a8c4b-b43b-46dc-a70a-d2e49ac31af0',\n",
       " '9c0eb7d0-44ee-4fc0-a092-c62c61026d69',\n",
       " '282af929-0229-41f3-8f86-9d197af41a61',\n",
       " '9bf2b3d5-a61f-4c2f-9160-e2103aade803',\n",
       " 'c06f2738-09b6-4b0f-a102-613206e81916',\n",
       " '9b0bebb0-4eb9-4d8f-ae4e-33c4c540e83b',\n",
       " '5189f3a5-7ad6-4141-accd-5c8358122458',\n",
       " '90a8fa41-31a3-454f-961a-467315692d10',\n",
       " '85c2bd34-4741-4ab9-9bf5-a17c4fd57af4',\n",
       " 'd51abdf4-0344-4370-a7df-3898e14dd146',\n",
       " '13ba530b-12ce-4e20-986b-b0efb80c65b8',\n",
       " '5e41bd69-5daa-4c01-8224-eef4731d7c5f',\n",
       " '521e8ec9-6bd0-446f-bebb-8e3f05dc33c3',\n",
       " '89ddc64a-c0cf-4260-adbe-6359ce37a9c8',\n",
       " 'a728aa1a-8e3a-4b2b-b14e-da027c5d1a23',\n",
       " '7bf78941-be61-4a52-bc7b-4b12ef7e9222',\n",
       " '54511f8f-3eba-47f9-90a7-41eef0c6d249',\n",
       " 'cf914374-1a37-4fc4-9f14-e581789fb86c',\n",
       " 'acb9e0e5-3a48-468a-ba2c-85f1e0d355a8',\n",
       " '91df61ec-aaba-4a7d-9f05-e6998f2d0560',\n",
       " '2d0e4f92-b71d-47f6-ae62-4b711d2f339b',\n",
       " 'c7addf68-fbcd-4359-bbef-fd888456fc10',\n",
       " '8c7cf3bb-82e8-40c7-b612-4d34fd471e33',\n",
       " 'f5c7bdaa-52d2-42f2-9180-563458c283aa',\n",
       " 'f9ea3500-6d39-450f-935d-db5347ac1578',\n",
       " 'fecd04be-59d5-45f9-851d-cce597e260df',\n",
       " '723e906f-c4b4-419e-8e23-150d9fb355ca',\n",
       " '04e9ca63-3aff-475a-a27f-821fb95874c9',\n",
       " 'd8028934-9c68-43b9-b680-3ed900b3e269',\n",
       " '3fe28d7c-31e4-42e6-b48a-586117ed42c8',\n",
       " '3a5c65ff-d9d1-4a0a-a305-c6bee3f9028e',\n",
       " 'fbc10849-9f3a-4453-b5de-d4c99a93f3d5',\n",
       " 'd99ac434-6c9b-472e-8976-9e76a16b50ae',\n",
       " 'e30a734b-a0c8-4ce1-b025-a1c10b363311',\n",
       " 'a94fff2e-1bd8-4a9c-9185-aefa6956e439',\n",
       " 'cc724d3f-6622-411b-9cb0-a85eb856b059',\n",
       " '6a8ee6cf-2d9a-4c9f-9c3f-bdab683d63b4',\n",
       " '3eae0af6-6b38-4982-a5ab-e4959b6fab62']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì„œ IDë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ UUID ì‚¬ìš©\n",
    "from uuid import uuid4\n",
    "\n",
    "# ê° ë¬¸ì„œì— ëŒ€í•´ ê³ ìœ í•œ ID ìƒì„±\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2 }@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': '67fa1f86-e931-43fb-866d-9f273e685a3f', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "# 1ê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (ê¸°ë³¸ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agentê°€ ë­ì•¼?\", k=1)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* which are increasingly prevalent in real-world applications\n",
      "(Achiam et al., 2023; Touvron et al., 2023a). Additionally,\n",
      "their free-form nature of outputs further complicates the\n",
      "application of traditional methods. As a result, alternative\n",
      "approaches have been proposed, including estimating un-\n",
      "certainty by directly querying models for confidence scores\n",
      "after generating responses (Xiong et al., 2024; Kadavath\n",
      "et al., 2022; Lin et al., 2022a; Mielke et al., 2022; Chen &\n",
      "Mueller, 2024). Despite these advancements, existing meth-\n",
      "ods are not designed for embodied tasks, where confidence\n",
      "elicitation must address the challenges of multimodal per-\n",
      "ception, hierarchical reasoning and planning across various\n",
      "open-ended tasks, as well as non-deterministic interactions.\n",
      "LLM-based Embodied Agents. With the advent of lan-\n",
      "guage models, leveraging their reasoning and planning abili-\n",
      "ties to empower embodied agents has become quintessential\n",
      "(Huang et al., 2023; Yao et al., 2023; Chen et al., 2023; [{'source': 'data/arxiv_paper.pdf', 'page': 1, '_id': '207e508e-a8b8-495e-bbf4-45aaeac333c3', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "#  íŠ¹ì • í•„í„°ë§ ì¡°ê±´ì„ ì ìš©í•œ ê²€ìƒ‰\n",
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"thud\",\n",
    "    k=1,\n",
    "    filter=models.Filter(must=[models.FieldCondition(\n",
    "        key=\"metadata.page\",  # íŠ¹ì • í•„ë“œ(ì˜ˆ: page) ê¸°ë°˜ í•„í„°ë§\n",
    "        match=models.MatchValue(value=1),  # page=1ì¸ ë¬¸ì„œë§Œ ê²€ìƒ‰\n",
    "    )])\n",
    ")\n",
    "\n",
    "# í•„í„°ë§ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "* [ìœ ì‚¬ë„=0.585] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': 'b840b5af-51c4-4073-b436-ed7e049b0bef', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.573] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agentsâ€™ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using bayesian binning. In\n",
      "AAAI Conference on Artificial Intelligence, 2015.\n",
      "Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y ., Ha-\n",
      "jishirzi, H., Singh, S., and Fox, R. Do embodied agents\n",
      "dream of pixelated sheep: Embodied decision making\n",
      "using language guided world modelling. In International\n",
      "Conference on Machine Learning, 2023.\n",
      "Okoli, C. Inductive, abductive and deductive theorising.\n",
      "International Journal of Management Concepts and Phi-\n",
      "losophy, 2023. [{'source': 'data/arxiv_paper.pdf', 'page': 10, '_id': '9bf2b3d5-a61f-4c2f-9160-e2103aade803', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.571] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment is critical. However, the\n",
      "reliance on large pre-trained models raises concerns about\n",
      "energy efficiency and ethical considerations in high-stakes\n",
      "applications, which warrant further exploration.\n",
      "References\n",
      "Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,\n",
      "Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-\n",
      "ravi, A., Acharya, U. R., et al. A review of uncertainty\n",
      "quantification in deep learning: Techniques, applications\n",
      "and challenges. Information Fusion, 2021. [{'source': 'data/arxiv_paper.pdf', 'page': 8, '_id': '92137943-00e7-4a5b-85fd-1e27d9bbe6a8', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.571] tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVEâ€™s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence judgments.\n",
      "So, How Effectively Can Embodied Agents Express Con-\n",
      "fidence in Dynamic Embodied Tasks? While embodied\n",
      "agents can convey confidence to some extent, their effec-\n",
      "tiveness depends on how well they integrate reasoning, un-\n",
      "certainty assessment, and environmental interactions. The\n",
      "findings reveal that embodied confidence elicitation remains\n",
      "a challenging problem, requiring a careful balance between\n",
      "general-purpose reasoning and task-specific specialization. [{'source': 'data/arxiv_paper.pdf', 'page': 5, '_id': '47fb0a27-d039-43f7-9458-e5715b3d9245', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.570] sents visual observations and It represents task instructions\n",
      "and other types of language-based guidance. For a given\n",
      "task T , the agent operates under a policy Ï€ : I â†’ Athat\n",
      "maps input I to actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score c âˆˆ [0, 1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.\n",
      "The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks. This requires not only\n",
      "detecting uncertain aspects of the agentâ€™s perception, reason-\n",
      "ing, or actions but also ensuring that confidence estimates\n",
      "are refined and reliable under dynamic multimodal condi-\n",
      "tions. To address this, we propose an embodied confidence\n",
      "estimation framework centered around Elicitation Modules\n",
      "that facilitates confidence elicitation at two critical points [{'source': 'data/arxiv_paper.pdf', 'page': 2, '_id': 'af88d6e9-da5e-4580-a647-eb2c37ca3cd7', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (Queryì™€ ë¬¸ì„œ ê°„ ê±°ë¦¬ ê³„ì‚°)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agentê°€ ë­ì•¼?\", k=5)\n",
    "\n",
    "print(\"ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì ìˆ˜ ì¶œë ¥\n",
    "for doc, score in results:\n",
    "    print(f\"* [ìœ ì‚¬ë„={score:.3f}] {doc.page_content[:1000]} [{doc.metadata}]\")  # ì²« 1000ì ì¶œë ¥\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
