{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "## ğŸ“š ê°•ì˜ ê°œìš” (Overview)\n",
    "\n",
    "ì´ ê°•ì˜ì—ì„œëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ê³ , ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ(Load)í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì—ì„œ ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í• ì§€, ê·¸ë¦¬ê³  ì–´ë–¤ ë²¡í„° ì €ì¥ì†Œë¥¼ í™œìš©í• ì§€ê°€ ê²€ìƒ‰ ë° ì‘ë‹µ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê°•ì˜ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì—¬ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨: \n",
    "* [OpenAI ì„ë² ë”© ëª¨ë¸ í™œìš©í•˜ê¸°](#openai-ì„ë² ë”©-ëª¨ë¸-í™œìš©í•˜ê¸°)\n",
    "* [Ollama Embedding ëª¨ë¸ í™œìš©í•˜ê¸°](#ollama-embedding-ëª¨ë¸-í™œìš©í•˜ê¸°)\n",
    "* [FAISSë¡œ ì„ë² ë”© ë²¡í„° ì €ì¥í•˜ê¸°](#faissë¡œ-ì„ë² ë”©-ë²¡í„°-ì €ì¥í•˜ê¸°)\n",
    "* [Chroma ë²¡í„° DB](#chroma-ë²¡í„°-db)\n",
    "* [Qdrant ë²¡í„° DB](#qdrant-ë²¡í„°-db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •í•˜ê¸° (.env íŒŒì¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ì—¬ê¸°ì— ì…ë ¥í•´ì£¼ì„¸ìš”!)\n",
    "import os\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "os.environ[\"API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI ì„ë² ë”© ëª¨ë¸ í™œìš©í•˜ê¸°\n",
    "\n",
    "`text-embedding-3-small`: OpenAIì—ì„œ ì œê³µí•˜ëŠ” ìµœì‹  ì„ë² ë”© ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ë¹ ë¥´ê³  ê°€ë²¼ìš´ ì„ë² ë”©ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¬¸ì„œ ì„ë² ë”© vs ì§ˆì˜ ì„ë² ë”©\n",
    "* `embed_documents()` â†’ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ í•œ ë²ˆì— ì„ë² ë”© (ë¬¸ì„œ ê²€ìƒ‰ ë“±ì— í™œìš©).\n",
    "* `embed_query()` â†’ ì§ˆì˜(Query)ë¥¼ ì„ë² ë”© (ì§ˆë¬¸-ì‘ë‹µ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰í•  ë•Œ í™œìš©)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://arize.com/wp-content/uploads/2022/06/blog-king-queen-embeddings.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì„œ(ë¬¸ì¥) ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”©)\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ì„ë² ë”©ëœ ë¬¸ì„œ ê°œìˆ˜ì™€ ê°œë³„ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ì¶œë ¥\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.010680568404495716,\n",
       " -0.01018487848341465,\n",
       " -0.0019450020045042038,\n",
       " 0.023096051067113876,\n",
       " -0.02682921662926674]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§ˆì˜(Query) ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5] #ì²˜ìŒ 5ê°œì˜ ê°’ë§Œ ì¶œë ¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])# ê°œë³„ ë¬¸ì¥ì˜ ì„ë² ë”© ì°¨ì› í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Embedding ëª¨ë¸ í™œìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ë³„ë¡œ ì €ì¥í•˜ëŠ” ê³¼ì •\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"data/arxiv_paper.pdf\"\n",
    ")\n",
    "\n",
    "# PDF ë¡œë” ê°ì²´ ìƒì„±\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PDFì˜ ê° í˜ì´ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "pages = []\n",
    "\n",
    "# PDFë¥¼ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ë³„ë¡œ ì €ì¥\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'data/arxiv_paper.pdf', 'page': 0}\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi-\n",
      "ronment, we show that structured reasoning ap-\n",
      "proaches, such as Chain-of-Thoughts, improve\n",
      "confidence calibration. However, our findings\n",
      "also reveal persistent challenges in distinguishing\n",
      "uncertainty, particularly under abductive settings,\n",
      "underscoring the need for more sophisticated em-\n",
      "bodied confidence elicitation methods.\n",
      "1. Introduction\n",
      "In complex embodied environments, success depends not\n",
      "only on what an agent knows but also on how well it un-\n",
      "derstands and communicates uncertainty. Whether navi-\n",
      "gating a cluttered space, interacting with objects, or plan-\n",
      "ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # í•˜ë‚˜ì˜ ì²­í¬ í¬ê¸°ë¥¼ 1000ìë¡œ ì„¤ì •\n",
    "    chunk_overlap=200,     # ì²­í¬ ê°„ 200ì ê²¹ì¹˜ê²Œ ì„¤ì • (ë¬¸ë§¥ ìœ ì§€ ëª©ì )\n",
    "    length_function=len,   # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (len ì‚¬ìš©)\n",
    "    is_separator_regex=False  # separatorë¥¼ ì •ê·œì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ì²˜ë¦¬\n",
    ")\n",
    "\n",
    "# PDFì—ì„œ ë¡œë“œí•œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• \n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"{texts[0].metadata}\")# ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°\n",
    "print(texts[0].page_content)# ì²« ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš© \n",
    "print(\"-\"*100)\n",
    "print(f\"{texts[1].metadata}\")# ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°\n",
    "print(texts[1].page_content)# ë‘ ë²ˆì§¸ ì²­í¬ì˜ ë‚´ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='Uncertainty in Action: Confidence Elicitation in Embodied Agents\\nTianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\\nTal August, Ismini Lourentzou\\nUniversity of Illinois Urbana-Champaign\\n{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\\nhttps://plan-lab.github.io/ece\\nAbstract\\nExpressing confidence is challenging for embod-\\nied agents navigating dynamic multimodal en-\\nvironments, where uncertainty arises from both\\nperception and decision-making processes. We\\npresent the first work investigating embodied con-\\nfidence elicitation in open-ended multimodal en-\\nvironments. We introduce Elicitation Policies,\\nwhich structure confidence assessment across\\ninductive, deductive, and abductive reasoning,\\nalong with Execution Policies, which enhance\\nconfidence calibration through scenario reinter-\\npretation, action sampling, and hypothetical rea-\\nsoning. Evaluating agents in calibration and fail-\\nure prediction tasks within the Minecraft envi-'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='confidence calibration through scenario reinter-\\npretation, action sampling, and hypothetical rea-\\nsoning. Evaluating agents in calibration and fail-\\nure prediction tasks within the Minecraft envi-\\nronment, we show that structured reasoning ap-\\nproaches, such as Chain-of-Thoughts, improve\\nconfidence calibration. However, our findings\\nalso reveal persistent challenges in distinguishing\\nuncertainty, particularly under abductive settings,\\nunderscoring the need for more sophisticated em-\\nbodied confidence elicitation methods.\\n1. Introduction\\nIn complex embodied environments, success depends not\\nonly on what an agent knows but also on how well it un-\\nderstands and communicates uncertainty. Whether navi-\\ngating a cluttered space, interacting with objects, or plan-\\nning long-term strategies, eliciting confidence is pivotal as\\nagents must interpret and interact with dynamic settings\\nin real-time while managing uncertainty from both percep-'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='ning long-term strategies, eliciting confidence is pivotal as\\nagents must interpret and interact with dynamic settings\\nin real-time while managing uncertainty from both percep-\\ntion and decision-making processes (Ren et al., 2023; Liang\\net al., 2024). For humans, this instinctive ability to express\\nand calibrate uncertainty is fundamental to decision-making\\nand social interaction. As AI systems are increasingly de-\\nployed in high-stakes contexts such as autonomous driving\\nor healthcare, they must also acquire this crucial skill.\\n*Preprint. Work in progress.\\nEmbodied Environment\\nElicitation Module\\nAre you sure about yournext action?\\nElicitation Module\\nAre you sure aboutwhat you see?\\nElicitation\\nPolicies\\nExecution\\nPolicies\\nElicitation\\nPolicies\\nExecution\\nPolicies\\nPerception Stage\\nAction Stage\\nFigure 1.Embodied Confidence Estimation Framework consist-\\ning of Elicitation Policies and Execution Policies, which jointly\\nenable an agent to assess and express its confidence. Elicitation'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='Figure 1.Embodied Confidence Estimation Framework consist-\\ning of Elicitation Policies and Execution Policies, which jointly\\nenable an agent to assess and express its confidence. Elicitation\\nModules prompt the agent to evaluate uncertainty in what it sees\\nand does, while Execution Policies refine confidence calibration\\nby expanding the agentâ€™s reasoning space (See Â§3 for details).\\nSpecifically, accurate confidence elicitation from AI systems\\nprovides critical insights for risk assessment, error mitiga-\\ntion, and system reliability in decision-making (Kuleshov\\n& Deshpande, 2022; Clark, 2015; Yildirim et al., 2019).\\nThis is particularly important in open-ended reasoning tasks,\\nwhere models may generate outputs that are semantically\\nplausible but factually incorrect, a phenomenon commonly\\nreferred to as hallucination (Xiao & Wang, 2021). How-\\never, confidence elicitation in embodied AI is particularly\\nchallenging. For instance, in open-ended environments such'),\n",
       " Document(metadata={'source': 'data/arxiv_paper.pdf', 'page': 0}, page_content='referred to as hallucination (Xiao & Wang, 2021). How-\\never, confidence elicitation in embodied AI is particularly\\nchallenging. For instance, in open-ended environments such\\nas Minecraft, an agent may misinterpret visual cues due\\nto limited viewpoints or struggle to determine the correct\\naction sequence to achieve complex goals (e.g., obtaining\\na diamond). These illustrate the broader difficulties in elic-\\niting confidence in embodied environments, where agents\\nmust navigate uncertainty at multiple levels.\\nConfidence elicitation in open-ended embodied environ-\\nments faces several challenges, including: 1) Multimodal\\nunderstanding, where the agent must assess uncertainty from\\ninputs across different interconnected modalities. 2) Granu-\\nlarity of confidence estimation, where the agent evaluates\\nconfidence not only in performing specific actions (e.g., â€œI\\nam 90% confident I can collect some woodâ€) but also in\\n1\\narXiv:2503.10628v1  [cs.AI]  13 Mar 2025')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# \"bge-m3\" ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
    "embeddings_model=OllamaEmbeddings(model=\"bge-m3\")\n",
    "# ì²­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "embeddings = embeddings_model.embed_documents([i.page_content for i in texts])\n",
    "\n",
    "len(embeddings[0])# ìƒì„±ëœ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imghub.insilicogen.com/media/photos/51034_43550_1241.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISSë¡œ ì„ë² ë”© ë²¡í„° ì €ì¥í•˜ê¸°\n",
    "\n",
    "FAISS(Facebook AI Similarity Search)\n",
    "*  ëŒ€ëŸ‰ì˜ ë²¡í„° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "* ë¬¸ì„œ ê²€ìƒ‰, ì¶”ì²œ ì‹œìŠ¤í…œ, ì´ë¯¸ì§€ ê²€ìƒ‰ ë“±ì—ì„œ í™œìš©ë¨.\n",
    "* ë²¡í„°ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•\n",
    "* `FAISS.from_documents()` â†’ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ FAISSì— ì €ì¥.\n",
    "* `embedding=embeddings_model` â†’ Ollamaì˜ `bge-m3` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±.\n",
    "\n",
    "FAISS ê²€ìƒ‰ ë°©ì‹\n",
    "* `similarity_search(query, k=1)` â†’ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
    "* `similarity_search(query, k=10, filter={\"page\": 0})` â†’ íŠ¹ì • ì¡°ê±´(ì˜ˆ: page=0)ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰.\n",
    "* `similarity_search_with_score(query, k=10)` â†’ ê²€ìƒ‰ëœ ë¬¸ì„œì™€ í•¨ê»˜ ìœ ì‚¬ë„ ì ìˆ˜(ê±°ë¦¬) ì¶œë ¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain_community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²­í¬ì˜ ìˆ˜: 100\n",
      "ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: 100\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#  FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± (OllamaEmbeddingsì„ í™œìš©)\n",
    "# ì•ì„œ ìƒì„±í•œ ì²­í¬(`texts`)ì™€ ì„ë² ë”© ëª¨ë¸(`embeddings_model`)ì„ ì´ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings_model # Ollama ì„ë² ë”© ëª¨ë¸\n",
    ")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ í¬ê¸° í™•ì¸\n",
    "print(f\"ì²­í¬ì˜ ìˆ˜: {len(texts)}\") # ì´ ì²­í¬ëœ ë¬¸ì„œ ê°œìˆ˜\n",
    "print(f\"ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {vector_store.index.ntotal}\")# ì´ ì²­í¬ëœ ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment is critical. However, the\n",
      "reliance on large pre-trained models raises concerns about\n",
      "energy efficiency and ethical considerations in high-stakes\n",
      "applications, which warrant further exploration.\n",
      "References\n",
      "Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,\n",
      "Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-\n",
      "ravi, A., Acharya, U. R., et al. A review of uncertainty\n",
      "quantification in deep learning: Techniques, applications\n",
      "and challenges. Information Fusion, 2021. [{'source': 'data/arxiv_paper.pdf', 'page': 8}]\n",
      "* sents visual observations and It represents task instructions\n",
      "and other types of language-based guidance. For a given\n",
      "task T , the agent operates under a policy Ï€ : I â†’ Athat\n",
      "maps input I to actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score c âˆˆ [0, 1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.\n",
      "The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks. This requires not only\n",
      "detecting uncertain aspects of the agentâ€™s perception, reason-\n",
      "ing, or actions but also ensuring that confidence estimates\n",
      "are refined and reliable under dynamic multimodal condi-\n",
      "tions. To address this, we propose an embodied confidence\n",
      "estimation framework centered around Elicitation Modules\n",
      "that facilitates confidence elicitation at two critical points [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n",
      "* tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVEâ€™s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence judgments.\n",
      "So, How Effectively Can Embodied Agents Express Con-\n",
      "fidence in Dynamic Embodied Tasks? While embodied\n",
      "agents can convey confidence to some extent, their effec-\n",
      "tiveness depends on how well they integrate reasoning, un-\n",
      "certainty assessment, and environmental interactions. The\n",
      "findings reveal that embodied confidence elicitation remains\n",
      "a challenging problem, requiring a careful balance between\n",
      "general-purpose reasoning and task-specific specialization. [{'source': 'data/arxiv_paper.pdf', 'page': 5}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "ter quantify uncertainty and anticipate divergent outcomes.\n",
      "To address this, we introduce a set of policies that gener-\n",
      "ate additional observations and diverse action trajectories,\n",
      "promoting robust confidence assessment:\n",
      "âŸ³ Action Sampling: The agent can generate multiple pos-\n",
      "sible actions by sampling from a learned policy distribution\n",
      "over the action space, conditioned on the current state and\n",
      "task objectives. By doing so, the agent can explore multiple\n",
      "actions, evaluate different outcomes, and assess which is\n",
      "most likely to succeed based on its perception.\n",
      "âŸ³ Scenario Reinterpretation: The agent can be prompted\n",
      "to reinterpret the same scenario from different perspec-\n",
      "tives. For example, it could focus on a particular object,\n",
      "re-evaluate environmental obstacles, or re-assess the prox-\n",
      "imity of targets. This enables the agent to propose different\n",
      "courses of action by gathering and redirecting its attention\n",
      "to relevant environmental information. [{'source': 'data/arxiv_paper.pdf', 'page': 4}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., â€œI am 70%\n",
      "confident I craft a wooden tableâ€). 3) Interactive depen-\n",
      "dencies, where the agentâ€™s actions directly influence the\n",
      "environment, which in turn affects subsequent decisions,\n",
      "requiring ongoing adjustments to confidence estimates as\n",
      "tasks progress. 4) Finally, while state-of-the-art embodied\n",
      "agents leverage proprietary Large Language Models (LLMs)\n",
      "and Vision-Language Models (VLMs) for their strong mul-\n",
      "timodal understanding and reasoning capabilities (Wang\n",
      "et al., 2023a; Qin et al., 2024; Zhu et al., 2023), these of-\n",
      "ten lack access to internal token likelihoods or probabilistic\n",
      "outputs, making traditional confidence estimation methods\n",
      "ineffective (Kumar et al., 2023; Chen et al., 2024b).\n",
      "To address these challenges, we present the first system-\n",
      "atic approach that enables LLM/VLM-powered embodied\n",
      "agents to assess and articulate their confidence across multi- [{'source': 'data/arxiv_paper.pdf', 'page': 1}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agentsâ€™ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using bayesian binning. In\n",
      "AAAI Conference on Artificial Intelligence, 2015.\n",
      "Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y ., Ha-\n",
      "jishirzi, H., Singh, S., and Fox, R. Do embodied agents\n",
      "dream of pixelated sheep: Embodied decision making\n",
      "using language guided world modelling. In International\n",
      "Conference on Machine Learning, 2023.\n",
      "Okoli, C. Inductive, abductive and deductive theorising.\n",
      "International Journal of Management Concepts and Phi-\n",
      "losophy, 2023. [{'source': 'data/arxiv_paper.pdf', 'page': 10}]\n",
      "* Figure 1.Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies and Execution Policies, which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation\n",
      "Modules prompt the agent to evaluate uncertainty in what it sees\n",
      "and does, while Execution Policies refine confidence calibration\n",
      "by expanding the agentâ€™s reasoning space (See Â§3 for details).\n",
      "Specifically, accurate confidence elicitation from AI systems\n",
      "provides critical insights for risk assessment, error mitiga-\n",
      "tion, and system reliability in decision-making (Kuleshov\n",
      "& Deshpande, 2022; Clark, 2015; Yildirim et al., 2019).\n",
      "This is particularly important in open-ended reasoning tasks,\n",
      "where models may generate outputs that are semantically\n",
      "plausible but factually incorrect, a phenomenon commonly\n",
      "referred to as hallucination (Xiao & Wang, 2021). How-\n",
      "ever, confidence elicitation in embodied AI is particularly\n",
      "challenging. For instance, in open-ended environments such [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* Confidence Elicitation in Embodied Agents\n",
      "language understanding. Trained on a vast dataset of 500,000 Minecraft-specific image-text instruction pairs, MineLLM\n",
      "can generate detailed insights about the game environment, answer complex queries, and provide contextual guidance for\n",
      "planning and execution. Its integration into MP5 enables the framework to address context- and process-dependent tasks\n",
      "with remarkable success rates, achieving a 91% success rate on context-dependent tasks and demonstrating exceptional\n",
      "adaptability in novel scenarios.\n",
      "STEVE: The STEVE series represents another advancement in language model-driven embodied agents for the Minecraft\n",
      "environment (Zhao et al., 2025). Built upon the foundation of LLaMA-2 (Touvron et al., 2023b), STEVE integrates powerful\n",
      "language capabilities tailored to enhance task reasoning, contextual understanding, and interaction. At its core, the language [{'source': 'data/arxiv_paper.pdf', 'page': 16}]\n",
      "* (Â§3.3) refine and expand confidence assessment through scenario reinterpretation, action sampling, and hypothetical reasoning. Together,\n",
      "they enhance confidence calibration in embodied agents. The orange text represents the vanilla elicitation policy, which incorporates the\n",
      "vanilla confidence prompt (described in Table 1) into the original instruction. The brown arrows\n",
      " denote the Scenario-Reinterpretation\n",
      "execution policy, prompting the agent to generate additional scene insights.\n",
      "to address by designing a unified approach that enhances\n",
      "reliability and robustness in embodied agents.\n",
      "Uncertainty in Embodied Models. Uncertainty estima-\n",
      "tion is well explored in robot learning and reinforcement\n",
      "learning (Wang & Zou, 2021; Ghasemipour et al., 2022; He\n",
      "et al., 2023; Huang et al., 2019; Jin et al., 2023), but remains\n",
      "a challenge for language models (Tian et al., 2023; Groot\n",
      "& Valdenegro Toro, 2024; Zhang et al., 2024b). While\n",
      "recent efforts have sought to quantify and mitigate uncer- [{'source': 'data/arxiv_paper.pdf', 'page': 2}]\n"
     ]
    }
   ],
   "source": [
    "# 1ê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (ê¸°ë³¸ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agentê°€ ë­ì•¼?\",k=10)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* Figure 1.Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies and Execution Policies, which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation\n",
      "Modules prompt the agent to evaluate uncertainty in what it sees\n",
      "and does, while Execution Policies refine confidence calibration\n",
      "by expanding the agentâ€™s reasoning space (See Â§3 for details).\n",
      "Specifically, accurate confidence elicitation from AI systems\n",
      "provides critical insights for risk assessment, error mitiga-\n",
      "tion, and system reliability in decision-making (Kuleshov\n",
      "& Deshpande, 2022; Clark, 2015; Yildirim et al., 2019).\n",
      "This is particularly important in open-ended reasoning tasks,\n",
      "where models may generate outputs that are semantically\n",
      "plausible but factually incorrect, a phenomenon commonly\n",
      "referred to as hallucination (Xiao & Wang, 2021). How-\n",
      "ever, confidence elicitation in embodied AI is particularly\n",
      "challenging. For instance, in open-ended environments such [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n",
      "* ning long-term strategies, eliciting confidence is pivotal as\n",
      "agents must interpret and interact with dynamic settings\n",
      "in real-time while managing uncertainty from both percep-\n",
      "tion and decision-making processes (Ren et al., 2023; Liang\n",
      "et al., 2024). For humans, this instinctive ability to express\n",
      "and calibrate uncertainty is fundamental to decision-making\n",
      "and social interaction. As AI systems are increasingly de-\n",
      "ployed in high-stakes contexts such as autonomous driving\n",
      "or healthcare, they must also acquire this crucial skill.\n",
      "*Preprint. Work in progress.\n",
      "Embodied Environment\n",
      "Elicitation Module\n",
      "Are you sure about yournext action?\n",
      "Elicitation Module\n",
      "Are you sure aboutwhat you see?\n",
      "Elicitation\n",
      "Policies\n",
      "Execution\n",
      "Policies\n",
      "Elicitation\n",
      "Policies\n",
      "Execution\n",
      "Policies\n",
      "Perception Stage\n",
      "Action Stage\n",
      "Figure 1.Embodied Confidence Estimation Framework consist-\n",
      "ing of Elicitation Policies and Execution Policies, which jointly\n",
      "enable an agent to assess and express its confidence. Elicitation [{'source': 'data/arxiv_paper.pdf', 'page': 0}]\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ì • í˜ì´ì§€ì— ëŒ€í•œ í•„í„°ë§ ê²€ìƒ‰ (page=0ì¸ ë¬¸ì„œì—ì„œ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied Agentê°€ ë­ì•¼?\",k=10,filter={\"page\": 0})\n",
    "\n",
    "# í•„í„°ë§ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (Queryì™€ ë¬¸ì„œ ê°„ ê±°ë¦¬ ê³„ì‚°)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied Agentê°€ ë­ì•¼?\",k=10)\n",
    "\n",
    "print(\"ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì ìˆ˜ ì¶œë ¥\n",
    "for doc, score in results:\n",
    "    print(f\"* [ìœ ì‚¬ë„={score:3f}] {doc.page_content[:100]} [{doc.metadata}]\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma ë²¡í„° DB\n",
    "\n",
    "\n",
    "* ChromaDBëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ, í…ìŠ¤íŠ¸ ê²€ìƒ‰ê³¼ ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì— í™œìš©ë¨.\n",
    "* FAISSëŠ” ë©”ëª¨ë¦¬ ë‚´(in-memory)ì—ì„œ ì‘ë™í•˜ì§€ë§Œ, ChromaëŠ” ì˜êµ¬ ì €ì¥(Persistent Storage) ê°€ëŠ¥.\n",
    "* ChromaëŠ” ì¿¼ë¦¬ ì‹œ ë” ë‹¤ì–‘í•œ í•„í„°ë§ê³¼ ì¡°í•©ì´ ê°€ëŠ¥í•¨.\n",
    "\n",
    "**Chroma ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•**\n",
    "* `Chroma(collection_name=\"test_01\", embedding_function=embeddings_model)`â†’ \"test_01\"ì´ë¼ëŠ” ì»¬ë ‰ì…˜ì„ ìƒì„±í•˜ê³ , Ollama ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥.\n",
    "* `add_documents(documents=texts, ids=ids)`â†’ ì„ë² ë”©ì„ ìƒì„±í•œ í›„ ChromaDBì— ì €ì¥.\n",
    "\n",
    "**ìœ ì‚¬ë„ ê²€ìƒ‰ ë°©ì‹**\n",
    "* `similarity_search(query, k=1)` â†’ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
    "* `similarity_search_with_score(query, k=5)` â†’ ë¬¸ì„œì™€ ì¿¼ë¦¬ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í•¨ê»˜ ë°˜í™˜.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08f1e927-7586-4b72-b522-f5a728e162fd',\n",
       " '5fd0ac08-167d-4818-95b9-60863a79ad63',\n",
       " '6f10d2dc-8abd-40e5-97c1-4eb59cf80969',\n",
       " '5027fbb7-788d-4936-9eef-030e04e93900',\n",
       " '4acd9c25-f2b9-4796-8fd5-caec29b55672',\n",
       " '16b1114c-1184-49b1-b08f-8fa6d7ed4715',\n",
       " '7a3d32e9-0121-4770-a125-7b76db933224',\n",
       " 'baaca446-fffb-45fa-96f7-d14e186a5fe7',\n",
       " 'da749c3d-6718-4aa2-a761-0c5714231862',\n",
       " 'fb0ad5d6-460d-4095-afcb-a0e86c2c8029',\n",
       " '9229acab-ad73-4999-910e-ec60e0633167',\n",
       " 'a02d917d-f1c3-4012-a27f-d71335b31756',\n",
       " '32ae1fbf-0b5e-494c-9790-3dc5961ade49',\n",
       " '974b2322-f4fb-49e9-a1d9-08ffa4a7c5a7',\n",
       " '921de3e4-bee4-4cb4-b3e4-b88948cde0ce',\n",
       " 'cd7bc759-b09b-4fd8-b925-f12f16a7521f',\n",
       " '8308a0d8-f9c2-445c-b98f-93b856ebd5ba',\n",
       " 'f8573157-6cc7-4156-bbfe-3650e0acf51e',\n",
       " '25c136b4-f15d-4c9e-8235-177aaaff2619',\n",
       " '2ab6401c-bb7f-4656-b502-ffdc379ebfd7',\n",
       " 'e6713bab-5bd9-465a-956b-1f97607dd9fc',\n",
       " 'fc1fd8c6-5b0d-4dc4-b0d5-92e1c3df28cc',\n",
       " 'a0e8c55f-c72a-4470-8eb6-b1c0369b234f',\n",
       " '47df9438-c328-4be9-bd25-731ba907fa52',\n",
       " '855b459c-70a5-4ed7-934a-a5788146f5f2',\n",
       " '56b6bf99-01e6-481f-b475-a14680d0652e',\n",
       " 'cf5b1023-c078-494d-a557-eb8a91b46e5f',\n",
       " '63763c2e-ec1f-45ee-a0c9-8864ec55e4a6',\n",
       " '7a3f3add-3591-4237-8e24-c37d10e60600',\n",
       " '1ceabec3-e5e4-4c15-958b-daa6aa39247f',\n",
       " '882f2540-5dc0-4386-96be-7c47ce016e88',\n",
       " '3bb5ae2d-1e92-49b5-bc93-0304bc9fe5b5',\n",
       " 'e02b4ea4-a8b0-47cb-a9f2-a5da78abffe2',\n",
       " 'b10efafa-20e9-4807-a01f-48249f9fb2fc',\n",
       " '0cb063db-d194-400c-827c-d4940ffee13e',\n",
       " '47d75903-b60e-4098-a525-4190c64d0d94',\n",
       " 'aa7438bf-2e7c-4347-ac59-b1bbee33e453',\n",
       " '675228f3-0197-4a7f-8e1d-5fecca6e9d85',\n",
       " '648c95e9-e501-479f-854b-43eae19a04ed',\n",
       " '821a5f76-3313-46a0-b832-bab40af0e0b3',\n",
       " 'e2b0401d-454b-42a3-b563-a8103519581c',\n",
       " 'b6abe948-6334-4648-8c7f-2a12d2b7be15',\n",
       " '8de73063-f397-4f3d-83ac-e8b75d5a3168',\n",
       " '7c9ed7b7-5ffa-48f5-9e7c-718ee666bc37',\n",
       " '35cd41ee-dfec-486e-b84c-070cbfb6a044',\n",
       " '549d13d4-a7bf-499d-934f-69d7a259e749',\n",
       " '187b2511-7738-4d29-8e7e-fb99cc7063b8',\n",
       " '7cbea0ec-1ca3-4b9d-9bf2-2d6ac6460edc',\n",
       " '9851e5e8-7885-4d1c-8719-f20960373cc9',\n",
       " '1006717e-e304-476a-8a53-974a23abd019',\n",
       " 'adbc5597-6cf9-43a7-824a-478b6a371b99',\n",
       " '11e8e275-df9f-4bf3-b6f9-6b82ade771f6',\n",
       " 'ecdbcb83-e7c7-41f1-83ce-5156ba4447df',\n",
       " '4f862c88-1607-4608-b35a-59b1e11c0ee0',\n",
       " '221da9ee-1735-4e9c-8797-2ed533759729',\n",
       " '60b7ac23-838a-49a1-9162-2683cfbe09c2',\n",
       " '049c8425-4aa6-43f3-94e2-bb4c5aab6d33',\n",
       " '0cba8c0d-3964-4734-a122-0b7d4090f6fc',\n",
       " '6fbf6fe5-704d-4883-9b35-cbf6f4e0155e',\n",
       " '967f6563-f3b0-4bce-abc4-7272539be0ce',\n",
       " 'aa9f09ff-074d-4965-bc15-b6851257b954',\n",
       " '2501b3ae-d791-4089-b3b1-2701c81ed389',\n",
       " 'aa99e3c0-35d2-4392-a90f-be6f786df469',\n",
       " '568a22dd-98e8-4a69-a731-c498a7add06e',\n",
       " 'ad42cd19-b725-4d95-8dbb-cefde22fb400',\n",
       " '7f49b18c-f7a2-4771-8009-1c4f56ab825e',\n",
       " '962483c6-bf76-4cfa-860f-662104f64b38',\n",
       " 'bd3a14f4-a5ef-4141-b7ae-3e7b9c6230b7',\n",
       " '0c419c52-39e2-48ed-b311-8a4752a0fc6e',\n",
       " '4230ad0d-dd9b-4fab-be5c-c876722d05ec',\n",
       " 'f60b2745-3dc6-4569-ab06-f9dc6ce272b5',\n",
       " 'cb32d89d-68c5-4e21-8941-242f4242d018',\n",
       " '234f9d77-ba02-4c37-a177-32e011f93553',\n",
       " '19be1cbb-e7dc-4e1c-ab63-d3e0e5446fd6',\n",
       " 'e36db39b-7887-41cf-a140-0bf09768b800',\n",
       " '51de813c-6d2f-46dd-919e-0cda8b7d9e8a',\n",
       " '8c0204f9-b691-460c-8f5e-f17ecd4d9e20',\n",
       " 'b23f652d-f019-404b-b309-97c4f8c0b7d5',\n",
       " '1498d13b-5c9e-4629-89f8-295a6bd095f7',\n",
       " '7e97c175-d7f0-4bdb-8ed5-46c5f7a3e117',\n",
       " '64bbfd90-b810-41c1-93e3-b09f9b31c1ef',\n",
       " '9dbd43de-03f7-44ae-90f0-bc82bab1a0e9',\n",
       " '3aadddf2-e3f5-457d-a109-4e869acdfc1c',\n",
       " '6573954a-3d9d-4830-bf41-224f4229491b',\n",
       " 'd618b2a4-c00f-4059-abf8-8d9120b42277',\n",
       " '2d7cbb4e-9525-451e-9450-12935cf6c85a',\n",
       " '04706390-33dc-4b40-9eaa-9521d3528573',\n",
       " '45301729-8a85-4767-9a87-00604fa8f14e',\n",
       " 'f214b942-b1f6-4796-b39b-f281184d274a',\n",
       " 'c9f77bf6-353b-47be-928f-4176b19ea829',\n",
       " '8bf3128e-abdb-4041-b9cd-0cd1258435d8',\n",
       " '98eb6fda-9aea-4c7d-a0e9-636407ae0d8d',\n",
       " '692d6a48-57e5-4317-af1c-b9662195ea9f',\n",
       " 'c761bf63-0db1-4dd5-b262-35c2ee927b3b',\n",
       " 'ce4a8fe3-0ba9-45e0-8957-9b16fed55504',\n",
       " '785c193e-247f-4831-b53e-622b473e9879',\n",
       " '12c21798-a2aa-4b94-b141-1b1d4953a255',\n",
       " '585accd5-97a2-418d-9637-5cbaf1d99628',\n",
       " '37b93e9b-cf5b-471d-83a7-95e394ae1d78',\n",
       " '0e9761cf-d3a5-4045-ad3f-26f400f57f0a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from uuid import uuid4\n",
    "\n",
    "#  Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_02\",  # ë°ì´í„°ê°€ ì €ì¥ë  ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    embedding_function=embeddings_model,  # Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "#  UUIDë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œë³„ ê³ ìœ  ID ìƒì„±\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€ (ì„ë² ë”© ìë™ ìƒì„±)\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# 1ê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (ê¸°ë³¸ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agentê°€ ë­ì•¼?\", k=1)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "* [ìœ ì‚¬ë„=0.832] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir  [{'page': 0, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.855] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore [{'page': 10, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.858] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by intr [{'page': 8, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.859] tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 pa [{'page': 5, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.860] sents visual observations and It represents task instructions\n",
      "and other types of language-based guid [{'page': 2, 'source': 'data/arxiv_paper.pdf'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (Queryì™€ ë¬¸ì„œ ê°„ ê±°ë¦¬ ê³„ì‚°)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agentê°€ ë­ì•¼?\", k=5)\n",
    "\n",
    "print(\"ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì ìˆ˜ ì¶œë ¥\n",
    "for doc, score in results:\n",
    "    print(f\"* [ìœ ì‚¬ë„={score:.3f}] {doc.page_content[:100]} [{doc.metadata}]\")  # ìƒìœ„ 100ìë§Œ ì¶œë ¥\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant ë²¡í„° DB\n",
    "\n",
    "* QdrantëŠ” ë²¡í„° ê²€ìƒ‰(Vector Search) ë° í•„í„°ë§ì„ ì§€ì›í•˜ëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤.\n",
    "* ë©”ëª¨ë¦¬ ë‚´(in-memory) ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ì˜êµ¬ ì €ì¥(persistent storage)ë„ ì§€ì›.\n",
    "* FAISSë‚˜ Chromaì™€ ë‹¤ë¥´ê²Œ, ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ê¸°ëŠ¥ì´ ê°•ë ¥í•¨.\n",
    "\n",
    "**Qdrant ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•**\n",
    "* `QdrantClient(\":memory:\")` â†’ ì¸ë©”ëª¨ë¦¬ ë²¡í„° ì €ì¥ì†Œ ìƒì„±.\n",
    "* `create_collection()` â†’ 1024ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì»¬ë ‰ì…˜ ìƒì„±.\n",
    "* `distance=Distance.COSINE` â†’ ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŒ.\n",
    "  \n",
    "**Qdrant ê²€ìƒ‰ ê¸°ëŠ¥**\n",
    "* `similarity_search(query, k=1)` â†’ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰.\n",
    "* `similarity_search_with_score(query, k=5)` â†’ ë¬¸ì„œì™€ ì¿¼ë¦¬ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í•¨ê»˜ ë°˜í™˜.\n",
    "* `similarity_search(query, k=1, filter=...)` â†’ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain_qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChainì—ì„œ Qdrant ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Qdrant í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë©”ëª¨ë¦¬ ê¸°ë°˜)\n",
    "# \":memory:\" ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ íœ˜ë°œì„±(In-Memory) ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì‹¤í–‰ë¨.\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "#  Qdrant ì»¬ë ‰ì…˜ ìƒì„±\n",
    "client.create_collection(\n",
    "    collection_name=\"test\",  # ì €ì¥ì†Œ ì´ë¦„\n",
    "    vectors_config=VectorParams(\n",
    "        size=1024,  # ë²¡í„° ì°¨ì› ìˆ˜ (ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ì— ë§ì¶°ì•¼ í•¨)\n",
    "        distance=Distance.COSINE  # ë²¡í„° ê°„ ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ (ì½”ì‚¬ì¸ ê±°ë¦¬ ì‚¬ìš©)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Qdrant ë²¡í„° ì €ì¥ì†Œ ê°ì²´ ìƒì„±\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,  # Qdrant í´ë¼ì´ì–¸íŠ¸\n",
    "    collection_name=\"test\",  # ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    embedding=embeddings_model  # Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da36808d-fec5-4bd3-aa32-5fbe8b3b6e3b',\n",
       " '34b26675-c87c-49c4-a57d-db92449800f4',\n",
       " '2978d1a9-7afa-43bb-87d2-39b3a8f49d4f',\n",
       " '01ab23a7-e09c-4271-a4bd-1f9a82c0e462',\n",
       " '1fc0ce8b-65ea-4d1d-a24d-a1d1a8b9bb39',\n",
       " '3f694b1c-1097-4fc8-a8b0-ff2d366d93ef',\n",
       " 'fd6c7712-bd6b-45f7-b1f3-cbd9589bbfcd',\n",
       " 'fb10eb95-5f84-49f4-b6d9-d0b98005901b',\n",
       " '0fa10f84-e13d-49f2-a968-eb8bf0b47516',\n",
       " 'c3a4f21f-be6f-41ba-8f6f-551e34062aed',\n",
       " '66b833f2-6f03-43fa-9f77-663c462a953b',\n",
       " '5b73b63f-270c-4f99-b225-1fbedfc99f6d',\n",
       " '4180743a-77d1-4a3f-9031-cb310326daa0',\n",
       " 'b97b63f2-8802-4899-a9f1-c0a37ab84473',\n",
       " '4b9ffeb3-2873-499c-b177-d54c1206408d',\n",
       " '7aeefedc-2d75-4bed-89d2-2d5bd8c3822d',\n",
       " '2faa4fee-b58b-4c15-9b32-83e7786c9111',\n",
       " '5ad41bb7-5081-4f48-870f-ac27a6e34d52',\n",
       " 'a4cfe741-a355-40d8-a649-63c86cae48e6',\n",
       " '00c768e4-6f1a-4e11-b9c5-74c49bb1b901',\n",
       " 'a973d117-34d1-4911-83c3-40542eba90a3',\n",
       " '3cca1d72-e76c-44a6-9943-88853a7e03c7',\n",
       " 'e30ff073-5f14-4b54-b5a7-6fe09846d0db',\n",
       " '0815a734-e616-4013-b24f-a61ffdf5750f',\n",
       " '71c0c67e-ceff-4266-aefe-ea3f8d75a572',\n",
       " '0e3a7533-575d-47cd-a453-5da5bb1d7dcd',\n",
       " 'c981d292-f88f-4203-b7c9-1c5696a86aed',\n",
       " 'c44b421d-e6d7-452c-b6ce-56ba53f4a381',\n",
       " 'd6336bd6-286d-4ea9-b49d-5f33a1e749e1',\n",
       " 'd904e9f4-8318-4e4e-b0ed-87899f3ebd6f',\n",
       " '4844a70b-c2a1-4521-9b4f-c4b105e89f82',\n",
       " 'e00a8060-3878-44ca-8a27-1600bf0bb732',\n",
       " '2d81606c-1ec1-4d62-8cb8-fcff4af708e5',\n",
       " 'bce7be57-e5df-4869-a0fb-ffc66033888c',\n",
       " 'e2d276cd-1534-432e-9921-d12fc5013d8d',\n",
       " '7409e63d-f908-4cf6-8458-0ab7b856266f',\n",
       " '55928751-1107-460c-9dc2-784f599f9579',\n",
       " '135fd06c-1337-40b6-b195-94d07077023e',\n",
       " '9bb910f4-31fe-4ec8-b2fe-6b04288ddc7b',\n",
       " '97151fbf-0009-422a-b164-5558be215bd5',\n",
       " '0330e93d-ba77-48c6-a3e5-8b87f190dc9b',\n",
       " '4d508d65-c3f4-40a9-8e28-74ac4ee8fa1f',\n",
       " '833546c2-39e1-423e-9b48-020f4cc4f3ff',\n",
       " '2d6b6f85-0417-47cb-81e4-09bc640e6cae',\n",
       " '9ff0cefb-0f06-41e2-9086-0edc1721ed8a',\n",
       " '209eacf4-51db-4852-a666-299006ccee3a',\n",
       " '2814b883-4541-42db-ab58-c799b883c315',\n",
       " 'd5e74e15-204d-45fb-ac69-fc0721c137c6',\n",
       " '96d80b5c-7e22-42a3-af3b-b9c65c193671',\n",
       " '502fd863-9891-4f70-9d24-5d047eec72ee',\n",
       " 'b512a420-20fa-480a-a3e0-35ec95f1929d',\n",
       " 'a2319cf9-df3e-47b3-b78a-6d3a296c102a',\n",
       " '9a65c377-75da-4c69-948d-1cd252fc953d',\n",
       " '5be91ffe-27cd-492d-8132-c3d80842edf2',\n",
       " 'e6ee4081-ba96-49f3-8b5d-5e23fe9df8b8',\n",
       " '060f7df0-a2e1-4b97-ad50-0a5849b69cd3',\n",
       " '00565e0a-a9ef-4faa-892c-afcf2225ba02',\n",
       " 'fe53b18a-35c3-4cb0-acd3-1c24cfa47945',\n",
       " '57fec2c2-760d-49c4-b998-f82f037f9814',\n",
       " '56d4fe6b-edeb-46e8-a2da-9fcb1b1e5a28',\n",
       " '0552302a-34ec-48c4-aef5-4a9fb3ae6a8b',\n",
       " 'd09924a0-c0e6-429f-ad21-9d9e54875aac',\n",
       " 'd4c5d276-a321-4891-b8fc-29feb5198461',\n",
       " '86e4ae1a-de17-43dc-b4cf-c32a5ecf9b44',\n",
       " '6596e0f0-f6f1-4a95-86d7-82b65c234bad',\n",
       " '90579a3d-ca84-4715-b00c-2d66858264eb',\n",
       " '7da0977f-1abe-4fc6-a4c4-d49cc5c2e771',\n",
       " '5e9dee57-ec38-4d41-bce3-c8b53bf57b71',\n",
       " '209c1bb2-0763-42d9-bee4-a92b8781af54',\n",
       " '88af506c-3ea1-4435-b500-da65e5426889',\n",
       " 'fa16bf6a-ca3c-46c1-9f34-35d5a7b28abe',\n",
       " 'ebdfc993-d501-4444-bcb9-4f97b72e4f07',\n",
       " 'd5d0a8bd-6952-40c7-85e5-5f53041bb535',\n",
       " '4d519063-3df9-4b64-ae92-b561fd7c60fe',\n",
       " 'a182b113-a952-43b1-b774-01e9653bdae7',\n",
       " '03267ee4-0551-4634-b860-31fdc5d8477c',\n",
       " '2efb740f-ceb8-4020-965e-67c61fbbb50b',\n",
       " 'b4e83bc7-7179-48b2-b5a0-4b2c4b553e77',\n",
       " '5923b5a9-a7b3-41cd-a048-8e92e7c213d9',\n",
       " 'faeac1ef-d9bb-472d-a9ea-233a5bc8144c',\n",
       " '9178b675-415a-499f-ad2b-1f13524007a9',\n",
       " 'f8365669-81c8-411d-b93c-994a6506df03',\n",
       " '48fe60ba-45ee-45e5-9d39-1b20c7d43ed9',\n",
       " 'dc051752-4162-4f65-a628-74c8a71f2272',\n",
       " '949c7444-f066-454e-b64f-6b6ee8161fb3',\n",
       " 'ccf9988f-07b8-4274-a795-94362602f95b',\n",
       " '585e0673-e77d-4227-b88b-d23a61f833ef',\n",
       " 'a0ba7c0c-7446-41d8-a6f6-4b29cd4e269e',\n",
       " '19fd249b-e363-45e1-8e69-0c52aa8b594f',\n",
       " 'f3626950-ad43-4651-84b3-1d631b9bb95e',\n",
       " '2ebde9ec-d3bf-443a-801c-e5548a9703be',\n",
       " 'b729e381-1eb9-4997-8e77-1cd7fd79034b',\n",
       " '8a305e94-9f4d-4232-ac0d-9a74d4c0b9d5',\n",
       " '875907fd-ed1a-4d9c-b2f6-318e64dcfcdb',\n",
       " 'd8722449-9357-4c06-b7fa-aa5aea9de11f',\n",
       " '44d2c2fa-8f6f-45c3-b8ab-573b1dd0e905',\n",
       " '022ee1ff-5bf9-4f71-9d21-905236604f63',\n",
       " '1e78524b-1932-43a9-b960-f7a9e8adcdf6',\n",
       " '8fe72f72-6401-4167-9b5e-e859d531a3cb',\n",
       " '443f92e0-6a45-4c44-ace4-70f82848399e']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì„œ IDë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ UUID ì‚¬ìš©\n",
    "from uuid import uuid4\n",
    "\n",
    "# ê° ë¬¸ì„œì— ëŒ€í•´ ê³ ìœ í•œ ID ìƒì„±\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€\n",
    "vector_store.add_documents(documents=texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': 'da36808d-fec5-4bd3-aa32-5fbe8b3b6e3b', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "# 1ê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ (ê¸°ë³¸ ê²€ìƒ‰)\n",
    "results = vector_store.similarity_search(query=\"Embodied_agentê°€ ë­ì•¼?\", k=1)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Confidence Elicitation in Embodied Agents\n",
      "understanding high-level tasks or goals ( e.g., â€œI am 70%\n",
      "confident I craft a wooden tableâ€). 3) Interactive depen-\n",
      "dencies, where the agentâ€™s actions directly influence the\n",
      "environment, which in turn affects subsequent decisions,\n",
      "requiring ongoing adjustments to confidence estimates as\n",
      "tasks progress. 4) Finally, while state-of-the-art embodied\n",
      "agents leverage proprietary Large Language Models (LLMs)\n",
      "and Vision-Language Models (VLMs) for their strong mul-\n",
      "timodal understanding and reasoning capabilities (Wang\n",
      "et al., 2023a; Qin et al., 2024; Zhu et al., 2023), these of-\n",
      "ten lack access to internal token likelihoods or probabilistic\n",
      "outputs, making traditional confidence estimation methods\n",
      "ineffective (Kumar et al., 2023; Chen et al., 2024b).\n",
      "To address these challenges, we present the first system-\n",
      "atic approach that enables LLM/VLM-powered embodied\n",
      "agents to assess and articulate their confidence across multi- [{'source': 'data/arxiv_paper.pdf', 'page': 1, '_id': '3f694b1c-1097-4fc8-a8b0-ff2d366d93ef', '_collection_name': 'test'}]\n"
     ]
    }
   ],
   "source": [
    "#  íŠ¹ì • í•„í„°ë§ ì¡°ê±´ì„ ì ìš©í•œ ê²€ìƒ‰\n",
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"Embodied_agentê°€ ë­ì•¼?\",\n",
    "    k=1,\n",
    "    filter=models.Filter(must=[models.FieldCondition(\n",
    "        key=\"metadata.page\",  # íŠ¹ì • í•„ë“œ(ì˜ˆ: page) ê¸°ë°˜ í•„í„°ë§\n",
    "        match=models.MatchValue(value=1),  # page=1ì¸ ë¬¸ì„œë§Œ ê²€ìƒ‰\n",
    "    )])\n",
    ")\n",
    "\n",
    "# í•„í„°ë§ëœ ë¬¸ì„œ ì¶œë ¥\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "* [ìœ ì‚¬ë„=0.584] Uncertainty in Action: Confidence Elicitation in Embodied Agents\n",
      "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar\n",
      "Tal August, Ismini Lourentzou\n",
      "University of Illinois Urbana-Champaign\n",
      "{ty41,vrshah4,mwahed2,kietan2,adheesh2,taugust,lourent2}@illinois.edu\n",
      "https://plan-lab.github.io/ece\n",
      "Abstract\n",
      "Expressing confidence is challenging for embod-\n",
      "ied agents navigating dynamic multimodal en-\n",
      "vironments, where uncertainty arises from both\n",
      "perception and decision-making processes. We\n",
      "present the first work investigating embodied con-\n",
      "fidence elicitation in open-ended multimodal en-\n",
      "vironments. We introduce Elicitation Policies,\n",
      "which structure confidence assessment across\n",
      "inductive, deductive, and abductive reasoning,\n",
      "along with Execution Policies, which enhance\n",
      "confidence calibration through scenario reinter-\n",
      "pretation, action sampling, and hypothetical rea-\n",
      "soning. Evaluating agents in calibration and fail-\n",
      "ure prediction tasks within the Minecraft envi- [{'source': 'data/arxiv_paper.pdf', 'page': 0, '_id': 'da36808d-fec5-4bd3-aa32-5fbe8b3b6e3b', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.573] Confidence Elicitation in Embodied Agents\n",
      "Malinin, A. and Gales, M. Uncertainty estimation in autore-\n",
      "gressive structured prediction. In International Confer-\n",
      "ence on Learning Representations, 2021.\n",
      "Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y .-L.\n",
      "Reducing conversational agentsâ€™ overconfidence through\n",
      "linguistic calibration. Transactions of the Association for\n",
      "Computational Linguistics, 2022.\n",
      "Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining\n",
      "well calibrated probabilities using bayesian binning. In\n",
      "AAAI Conference on Artificial Intelligence, 2015.\n",
      "Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y ., Ha-\n",
      "jishirzi, H., Singh, S., and Fox, R. Do embodied agents\n",
      "dream of pixelated sheep: Embodied decision making\n",
      "using language guided world modelling. In International\n",
      "Conference on Machine Learning, 2023.\n",
      "Okoli, C. Inductive, abductive and deductive theorising.\n",
      "International Journal of Management Concepts and Phi-\n",
      "losophy, 2023. [{'source': 'data/arxiv_paper.pdf', 'page': 10, '_id': '90579a3d-ca84-4715-b00c-2d66858264eb', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.571] Confidence Elicitation in Embodied Agents\n",
      "8. Impact Statement\n",
      "This work advances Embodied AI by introducing confidence\n",
      "elicitation and execution policies tailored to multimodal and\n",
      "dynamic environments. By enabling embodied agents to\n",
      "express uncertainty, our approach enhances their calibra-\n",
      "tion, adaptability, and reliability in complex tasks. This\n",
      "contribution supports safer AI deployment in real-world do-\n",
      "mains like robotics, education, and collaborative systems,\n",
      "where accurate self-assessment is critical. However, the\n",
      "reliance on large pre-trained models raises concerns about\n",
      "energy efficiency and ethical considerations in high-stakes\n",
      "applications, which warrant further exploration.\n",
      "References\n",
      "Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D.,\n",
      "Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khos-\n",
      "ravi, A., Acharya, U. R., et al. A review of uncertainty\n",
      "quantification in deep learning: Techniques, applications\n",
      "and challenges. Information Fusion, 2021. [{'source': 'data/arxiv_paper.pdf', 'page': 8, '_id': '5be91ffe-27cd-492d-8132-c3d80842edf2', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.570] tend to yield improved confidence calibration. For instance,\n",
      "MineLLMâ€™s ECE achieves 0.32 and 0.30 paired with CoT\n",
      "and P&S respectively, outperforming other combinations.\n",
      "Hypothetical Reasoning sometimes degrades performance.\n",
      "For instance, STEVEâ€™s ECE worsens when pairing Hypo-\n",
      "thetical Reasoning with all Elicitation Policies, suggesting\n",
      "that while this execution strategy allows models to reason\n",
      "over multiple possible outcomes, it may introduce uncer-\n",
      "tainty, leading to less calibrated confidence judgments.\n",
      "So, How Effectively Can Embodied Agents Express Con-\n",
      "fidence in Dynamic Embodied Tasks? While embodied\n",
      "agents can convey confidence to some extent, their effec-\n",
      "tiveness depends on how well they integrate reasoning, un-\n",
      "certainty assessment, and environmental interactions. The\n",
      "findings reveal that embodied confidence elicitation remains\n",
      "a challenging problem, requiring a careful balance between\n",
      "general-purpose reasoning and task-specific specialization. [{'source': 'data/arxiv_paper.pdf', 'page': 5, '_id': '9bb910f4-31fe-4ec8-b2fe-6b04288ddc7b', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* [ìœ ì‚¬ë„=0.570] sents visual observations and It represents task instructions\n",
      "and other types of language-based guidance. For a given\n",
      "task T , the agent operates under a policy Ï€ : I â†’ Athat\n",
      "maps input I to actions A. The task of embodied confidence\n",
      "elicitation is to enable agents to estimate and articulate a\n",
      "confidence score c âˆˆ [0, 1], representing their belief in the\n",
      "correctness of their perception and subsequent actions.\n",
      "The challenge lies in systematically identifying, quantify-\n",
      "ing, and articulating uncertainty as the agent interacts with\n",
      "its environment and executes tasks. This requires not only\n",
      "detecting uncertain aspects of the agentâ€™s perception, reason-\n",
      "ing, or actions but also ensuring that confidence estimates\n",
      "are refined and reliable under dynamic multimodal condi-\n",
      "tions. To address this, we propose an embodied confidence\n",
      "estimation framework centered around Elicitation Modules\n",
      "that facilitates confidence elicitation at two critical points [{'source': 'data/arxiv_paper.pdf', 'page': 2, '_id': '2faa4fee-b58b-4c15-9b32-83e7786c9111', '_collection_name': 'test'}]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (Queryì™€ ë¬¸ì„œ ê°„ ê±°ë¦¬ ê³„ì‚°)\n",
    "results = vector_store.similarity_search_with_score(query=\"Embodied_agentê°€ ë­ì•¼?\", k=5)\n",
    "\n",
    "print(\"ì—¬ê¸°ì„œì˜ scoreëŠ” queryì™€ ë¬¸ì„œì˜ ìœ ì‚¬í•œ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì ìˆ˜ ì¶œë ¥\n",
    "for doc, score in results:\n",
    "    print(f\"* [ìœ ì‚¬ë„={score:.3f}] {doc.page_content[:1000]} [{doc.metadata}]\")  # ì²« 1000ì ì¶œë ¥\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-guide-1QRpMmrn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
